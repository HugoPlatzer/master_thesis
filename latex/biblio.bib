@misc{allyouneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@misc{improvinglu,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI},
  url={https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf}
}

@misc{unsupervisedmultitask,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2018},
  publisher={OpenAI},
  url={https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}
}

@misc{fewshotlearners,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}

@online{alammar-transformer,
  author       = {Jay Alammar},
  title        = {The Illustrated Transformer},
  url          = {http://jalammar.github.io/illustrated-transformer/},
  urldate      = {2023-06-06}
}

@online{alammar-gpt2,
  title = {The Illustrated GPT2},
  author = {Jay Alammar},
  url={http://jalammar.github.io/illustrated-gpt2/},
  urldate      = {2023-06-06}
}

@online{jay-visualizing,
  title = {Visualizing a Neural Machine Translation Model (Mechanics of Seq2Seq Models with Attention)},
  author = {Jay Alammar},
  url={https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/},
  urldate      = {2023-06-06}
}

@online{github-hf,
  author = {Hugging Face (GitHub)},
  title = {Transformers},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  url={https://github.com/huggingface/transformers},
  note = {Commit ID: 7631db0fdcfbd95b1f21d8034a0b8df73b9380ff}
}

@misc{ba2016layer,
      title={Layer Normalization}, 
      author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
      year={2016},
      eprint={1607.06450},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1607.06450}, 
}

@misc{openai_chatgpt_2022,
  title={Introducing ChatGPT},
  author={{OpenAI}},
  year={2022},
  month={11},
  url={https://openai.com/blog/chatgpt/},
  urldate={2023-12-19}
}

@inproceedings{Savelka_2023, series={ICER 2023},
   title={Thrilled by Your Progress! Large Language Models (GPT-4) No Longer Struggle to Pass Assessments in Higher Education Programming Courses},
   url={http://dx.doi.org/10.1145/3568813.3600142},
   DOI={10.1145/3568813.3600142},
   booktitle={Proceedings of the 2023 ACM Conference on International Computing Education Research V.1},
   publisher={ACM},
   author={Savelka, Jaromir and Agarwal, Arav and An, Marshall and Bogart, Chris and Sakr, Majd},
   year={2023},
   month=aug, pages={78–92},
   collection={ICER 2023} }

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI et al.},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@online{OpenGenus2023GPTComparison,
  author = {OpenGenus},
  title = {GPT-2 vs GPT-3 vs GPT-3.5 vs GPT-4: A Comprehensive Comparison of OpenAI LLMs},
  year = {2023},
  url={https://iq.opengenus.org/gpt2-vs-gpt3-vs-gpt35-vs-gpt4/},
  urldate = {2023-12-21}
}



@misc{xue2022byt5,
      title={ByT5: Towards a token-free future with pre-trained byte-to-byte models}, 
      author={Linting Xue and Aditya Barua and Noah Constant and Rami Al-Rfou and Sharan Narang and Mihir Kale and Adam Roberts and Colin Raffel},
      year={2022},
      eprint={2105.13626},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2105.13626}, 
}

@misc{tokenizerchoice,
      title={Tokenizer Choice For LLM Training: Negligible or Crucial?}, 
      author={Mehdi Ali and Michael Fromm and Klaudia Thellmann and Richard Rutmann and Max Lübbering and Johannes Leveling and Katrin Klug and Jan Ebert and Niclas Doll and Jasper Schulze Buschhoff and Charvi Jain and Alexander Arno Weber and Lena Jurkschat and Hammam Abdelwahab and Chelsea John and Pedro Ortiz Suarez and Malte Ostendorff and Samuel Weinbach and Rafet Sifa and Stefan Kesselheim and Nicolas Flores-Herr},
      year={2024},
      eprint={2310.08754},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.08754}, 
}

@misc{subwordunits,
      title={Neural Machine Translation of Rare Words with Subword Units}, 
      author={Rico Sennrich and Barry Haddow and Alexandra Birch},
      year={2016},
      eprint={1508.07909},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1508.07909}, 
}

@misc{subwordreg,
      title={Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates}, 
      author={Taku Kudo},
      year={2018},
      eprint={1804.10959},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1804.10959}, 
}


@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}



@online{HuggingFaceGPT2,
  author = {Hugging Face},
  title = {OpenAI GPT2 - Hugging Face Transformers Documentation},
  year = {2024},
  url={https://huggingface.co/docs/transformers/model_doc/gpt2},
  urldate={2024-01-04}
}

@online{HuggingFaceTokenizers,
  author = {Hugging Face},
  title = {Summary of the tokenizers},
  year = {2024},
  url={https://huggingface.co/docs/transformers/tokenizer_summary},
  urldate={2024-01-04}
}

@misc{su2023roformer,
      title={RoFormer: Enhanced Transformer with Rotary Position Embedding}, 
      author={Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},
      year={2023},
      eprint={2104.09864},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2104.09864}, 
}

@misc{khandelwal2019sample,
      title={Sample Efficient Text Summarization Using a Single Pre-Trained Transformer}, 
      author={Urvashi Khandelwal and Kevin Clark and Dan Jurafsky and Lukasz Kaiser},
      year={2019},
      eprint={1905.08836},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1905.08836}, 
}

@online{Mao2021Autoregressive,
  author = {Mao, Lei},
  title = {Autoregressive Model and Autoregressive Decoding for Sequence to Sequence Tasks},
  year = {2021},
  url={https://leimao.github.io/blog/Autoregressive-Model-Autoregressive-Decoding/},
  urldate={2024-01-04}
}

@misc{ippolito2019comparison,
      title={Comparison of Diverse Decoding Methods from Conditional Language Models}, 
      author={Daphne Ippolito and Reno Kriz and Maria Kustikova and João Sedoc and Chris Callison-Burch},
      year={2019},
      eprint={1906.06362},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1906.06362}, 
}

@misc{OpenAI2019BetterLM,
  title={Better Language Models},
  author={OpenAI},
  year={2019},
  url={https://openai.com/research/better-language-models},
  urldate={2024-01-08}
}

@misc{yi2019coherent,
      title={Towards Coherent and Engaging Spoken Dialog Response Generation Using Automatic Conversation Evaluators}, 
      author={Sanghyun Yi and Rahul Goel and Chandra Khatri and Alessandra Cervone and Tagyoung Chung and Behnam Hedayatnia and Anu Venkatesh and Raefer Gabriel and Dilek Hakkani-Tur},
      year={2019},
      eprint={1904.13015},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1904.13015}, 
}

@misc{ouyang2022training,
      title={Training language models to follow instructions with human feedback}, 
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.02155}, 
}

@article{choi2023unleashing,
  title={Unleashing the Potential of PIM: Accelerating Large Batched Inference of Transformer-Based Generative Models},
  author={Choi, Jaewan and Park, Jaehyun and Kyung, Kwanhee and Kim, Nam Sung and Ahn, Jung Ho},
  journal={IEEE Computer Architecture Letters},
  year={2023},
  publisher={IEEE}
}

@inproceedings{yu2022orca,
  title={Orca: A distributed serving system for $\{$Transformer-Based$\}$ generative models},
  author={Yu, Gyeong-In and Jeong, Joo Seong and Kim, Geon-Woo and Kim, Soojeong and Chun, Byung-Gon},
  booktitle={16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
  pages={521--538},
  year={2022}
}

@online{hfpretrained,
  author = {Hugging Face},
  title = {Pretrained models},
  url = {https://huggingface.co/transformers/v2.2.0/pretrained_models.html},
  urldate = {2024-01-11}
}

@online{hf-howtogeneratetext,
  author = {HuggingFace},
  title = {How to generate text: using different decoding methods for language generation with Transformers},
  year = {2020},
  url={https://huggingface.co/blog/how-to-generate},
  urldate = {2024-01-11}
}

@misc{residual,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1512.03385}, 
}

@misc{gelu,
      title={Gaussian Error Linear Units (GELUs)}, 
      author={Dan Hendrycks and Kevin Gimpel},
      year={2023},
      eprint={1606.08415},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1606.08415}, 
}

@misc{
adambeatssgd,
title={Why {\{}ADAM{\}} Beats {\{}SGD{\}} for Attention Models	},
author={Jingzhao Zhang and Sai Praneeth Karimireddy and Andreas Veit and Seungyeon Kim and Sashank J Reddi and Sanjiv Kumar and Suvrit Sra},
year={2020},
url={https://openreview.net/forum?id=SJx37TEtDH}
}

@misc{reasonoverscenegraphs,
      title={Learning to Reason over Scene Graphs: A Case Study of Finetuning GPT-2 into a Robot Language Model for Grounded Task Planning}, 
      author={Georgia Chalvatzaki and Ali Younes and Daljeet Nandha and An Le and Leonardo F. R. Ribeiro and Iryna Gurevych},
      year={2023},
      eprint={2305.07716},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2305.07716}, 
}

@misc{worth16words,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2021},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2010.11929}, 
}


@misc{cogview,
      title={CogView: Mastering Text-to-Image Generation via Transformers}, 
      author={Ming Ding and Zhuoyi Yang and Wenyi Hong and Wendi Zheng and Chang Zhou and Da Yin and Junyang Lin and Xu Zou and Zhou Shao and Hongxia Yang and Jie Tang},
      year={2021},
      eprint={2105.13290},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2105.13290}, 
}


@misc{decisiontransformer,
      title={Decision Transformer: Reinforcement Learning via Sequence Modeling}, 
      author={Lili Chen and Kevin Lu and Aravind Rajeswaran and Kimin Lee and Aditya Grover and Michael Laskin and Pieter Abbeel and Aravind Srinivas and Igor Mordatch},
      year={2021},
      eprint={2106.01345},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2106.01345}, 
}

@misc{noever2020chess,
      title={The Chess Transformer: Mastering Play using Generative Language Models}, 
      author={David Noever and Matt Ciolino and Josh Kalin},
      year={2020},
      eprint={2008.04057},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2008.04057}, 
}

@misc{grandmasterlevelchess,
      title={Amortized Planning with Large-Scale Transformers: A Case Study on Chess}, 
      author={Anian Ruoss and Grégoire Delétang and Sourabh Medapati and Jordi Grau-Moya and Li Kevin Wenliang and Elliot Catt and John Reid and Cannada A. Lewis and Joel Veness and Tim Genewein},
      year={2024},
      eprint={2402.04494},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.04494}, 
}

@misc{gotransformer,
      title={The Go Transformer: Natural Language Modeling for Game Play}, 
      author={Matthew Ciolino and David Noever and Josh Kalin},
      year={2020},
      eprint={2007.03500},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2007.03500}, 
}

@incollection{transferlearning,
  title={Transfer learning},
  author={Torrey, Lisa and Shavlik, Jude},
  booktitle={Handbook of research on machine learning applications and trends: algorithms, methods, and techniques},
  pages={242--264},
  year={2010},
  publisher={IGI global}
}

@misc{teaching,
      title={Teaching Arithmetic to Small Transformers}, 
      author={Nayoung Lee and Kartik Sreenivasan and Jason D. Lee and Kangwook Lee and Dimitris Papailiopoulos},
      year={2023},
      eprint={2307.03381},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.03381}, 
}

@misc{visual,
      title={Visual Learning of Arithmetic Operations}, 
      author={Yedid Hoshen and Shmuel Peleg},
      year={2015},
      eprint={1506.02264},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1506.02264}, 
}

@article{solving,
title = {Solving arithmetic problems using feed-forward neural networks},
journal = {Neurocomputing},
volume = {18},
number = {1},
pages = {61-79},
year = {1998},
issn = {0925-2312},
doi = {https://doi.org/10.1016/S0925-2312(97)00069-6},
url = {https://www.sciencedirect.com/science/article/pii/S0925231297000696},
author = {Leonardo Franco and Sergio A. Cannas},
keywords = {Neural networks, Arithmetic operations, Shifter circuit},
abstract = {We design new feed-forward multi-layered neural networks which perform different elementary arithmetic operations, such as bit shifting, addition of N p-bit numbers, and multiplication of two n-bit numbers. All the structures are optimal in depth and are polynomially bounded in the number of neurons and in the number of synapses. The whole set of synaptic couplings and thresholds are obtained exactly.}
}

@article{memtocomp,
   title={Arithmetic with language models: From memorization to computation},
   volume={179},
   ISSN={0893-6080},
   url={http://dx.doi.org/10.1016/j.neunet.2024.106550},
   DOI={10.1016/j.neunet.2024.106550},
   journal={Neural Networks},
   publisher={Elsevier BV},
   author={Maltoni, Davide and Ferrara, Matteo},
   year={2024},
   month=nov, pages={106550} }


@misc{goat,
      title={Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks}, 
      author={Tiedong Liu and Bryan Kian Hsiang Low},
      year={2023},
      eprint={2305.14201},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.14201}, 
}

@misc{rightembeddings,
      title={Transformers Can Do Arithmetic with the Right Embeddings}, 
      author={Sean McLeish and Arpit Bansal and Alex Stein and Neel Jain and John Kirchenbauer and Brian R. Bartoldson and Bhavya Kailkhura and Abhinav Bhatele and Jonas Geiping and Avi Schwarzschild and Tom Goldstein},
      year={2024},
      eprint={2405.17399},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.17399}, 
}

@misc{nocalculator,
      title={GPT Can Solve Mathematical Problems Without a Calculator}, 
      author={Zhen Yang and Ming Ding and Qingsong Lv and Zhihuan Jiang and Zehai He and Yuyi Guo and Jinfeng Bai and Jie Tang},
      year={2023},
      eprint={2309.03241},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.03241}, 
}

@misc{implicit,
      title={From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step}, 
      author={Yuntian Deng and Yejin Choi and Stuart Shieber},
      year={2024},
      eprint={2405.14838},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.14838}, 
}

@misc{positionmatters,
      title={Positional Description Matters for Transformers Arithmetic}, 
      author={Ruoqi Shen and Sébastien Bubeck and Ronen Eldan and Yin Tat Lee and Yuanzhi Li and Yi Zhang},
      year={2023},
      eprint={2311.14737},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.14737}, 
}

@misc{dissecting,
      title={Dissecting Multiplication in Transformers: Insights into LLMs}, 
      author={Luyu Qiu and Jianing Li and Chi Su and Chen Jason Zhang and Lei Chen},
      year={2024},
      eprint={2407.15360},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.15360}, 
}

@misc{lengthandcount,
      title={Arithmetic Transformers Can Length-Generalize in Both Operand Length and Count}, 
      author={Hanseul Cho and Jaeyoung Cha and Srinadh Bhojanapalli and Chulhee Yun},
      year={2025},
      eprint={2410.15787},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.15787}, 
}

@misc{lengthgen,
      title={Length Generalization in Arithmetic Transformers}, 
      author={Samy Jelassi and Stéphane d'Ascoli and Carles Domingo-Enrich and Yuhuai Wu and Yuanzhi Li and François Charton},
      year={2023},
      eprint={2306.15400},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2306.15400}, 
}

@misc{intertoextra,
      title={From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers}, 
      author={Shaoxiong Duan and Yining Shi and Wei Xu},
      year={2024},
      eprint={2310.11984},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.11984}, 
}

@misc{explained,
      title={Arithmetic in Transformers Explained}, 
      author={Philip Quirke and Clement Neo and Fazl Barez},
      year={2025},
      eprint={2402.02619},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.02619}, 
}

@misc{impact,
      title={The Impact of Positional Encoding on Length Generalization in Transformers}, 
      author={Amirhossein Kazemnejad and Inkit Padhi and Karthikeyan Natesan Ramamurthy and Payel Das and Siva Reddy},
      year={2023},
      eprint={2305.19466},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.19466}, 
}


@misc{seq2seq,
	title={Sequence to Sequence Learning with Neural Networks}, 
	author={Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
	year={2014},
	eprint={1409.3215},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/1409.3215}, 
}

@misc{phrasereps,
	title={Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}, 
	author={Kyunghyun Cho and Bart van Merrienboer and Caglar Gulcehre and Dzmitry Bahdanau and Fethi Bougares and Holger Schwenk and Yoshua Bengio},
	year={2014},
	eprint={1406.1078},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/1406.1078}, 
}

@misc{convseq,
	title={Convolutional Sequence to Sequence Learning}, 
	author={Jonas Gehring and Michael Auli and David Grangier and Denis Yarats and Yann N. Dauphin},
	year={2017},
	eprint={1705.03122},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/1705.03122}, 
}

@online{nanogpt,
	author       = {Karpathy, Andrej},
	title        = {nanoGPT},
	date         = {2023},
	url          = {https://github.com/karpathy/nanoGPT},
	urldate      = {2025-10-19},
	note         = {GitHub repository}
}

@misc{weightstying,
	title={Using the Output Embedding to Improve Language Models}, 
	author={Ofir Press and Lior Wolf},
	year={2017},
	eprint={1608.05859},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/1608.05859}, 
}

@misc{selfsupervised,
	title={A Cookbook of Self-Supervised Learning}, 
	author={Randall Balestriero and Mark Ibrahim and Vlad Sobal and Ari Morcos and Shashank Shekhar and Tom Goldstein and Florian Bordes and Adrien Bardes and Gregoire Mialon and Yuandong Tian and Avi Schwarzschild and Andrew Gordon Wilson and Jonas Geiping and Quentin Garrido and Pierre Fernandez and Amir Bar and Hamed Pirsiavash and Yann LeCun and Micah Goldblum},
	year={2023},
	eprint={2304.12210},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2304.12210}, 
}

@misc{selfsupervisedvisual,
	title={Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey}, 
	author={Longlong Jing and Yingli Tian},
	year={2019},
	eprint={1902.06162},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/1902.06162}, 
}
