@inproceedings{allyouneed,
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Attention is All you Need},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
	volume = {30},
	year = {2017}
}

@misc{improvinglu,
	title={Improving language understanding by generative pre-training},
	author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
	year={2018},
	publisher={OpenAI},
	url={https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf}
}

@misc{unsupervisedmultitask,
	title={Language Models are Unsupervised Multitask Learners},
	author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	year={2018},
	publisher={OpenAI},
	url={https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}
}

@inproceedings{fewsholtearners,
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {1877--1901},
	publisher = {Curran Associates, Inc.},
	title = {Language Models are Few-Shot Learners},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
	volume = {33},
	year = {2020}
}


@online{alammar-transformer,
	author       = {Jay Alammar},
	title        = {The Illustrated Transformer},
	url          = {http://jalammar.github.io/illustrated-transformer/},
	urldate      = {2023-06-06}
}

@online{alammar-gpt2,
	title = {The Illustrated GPT2},
	author = {Jay Alammar},
	url={http://jalammar.github.io/illustrated-gpt2/},
	urldate      = {2023-06-06}
}

@online{jay-visualizing,
	title = {Visualizing a Neural Machine Translation Model (Mechanics of Seq2Seq Models with Attention)},
	author = {Jay Alammar},
	url={https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/},
	urldate      = {2023-06-06}
}

@online{github-hf,
	author = {Hugging Face (GitHub)},
	title = {Transformers},
	year = {2018},
	publisher = {GitHub},
	journal = {GitHub repository},
	url={https://github.com/huggingface/transformers},
	note = {Commit ID: 7631db0fdcfbd95b1f21d8034a0b8df73b9380ff}
}

@online{openai_chatgpt_2022,
	title={Introducing ChatGPT},
	author={{OpenAI}},
	year={2022},
	month={11},
	url={https://openai.com/blog/chatgpt/},
	urldate={2023-12-19}
}

@inproceedings{Savelka_2023, series={ICER 2023},
	title={Thrilled by Your Progress! Large Language Models (GPT-4) No Longer Struggle to Pass Assessments in Higher Education Programming Courses},
	url={http://dx.doi.org/10.1145/3568813.3600142},
	DOI={10.1145/3568813.3600142},
	booktitle={Proceedings of the 2023 ACM Conference on International Computing Education Research V.1},
	publisher={ACM},
	author={Savelka, Jaromir and Agarwal, Arav and An, Marshall and Bogart, Chris and Sakr, Majd},
	year={2023},
	month=aug, pages={78–92},
	collection={ICER 2023}
}

@misc{openai2023gpt4,
	title={GPT-4 Technical Report}, 
	author={OpenAI},
	year={2024},
	eprint={2303.08774},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2303.08774}, 
}

@online{OpenGenus2023GPTComparison,
	author = {OpenGenus},
	title = {GPT-2 vs GPT-3 vs GPT-3.5 vs GPT-4: A Comprehensive Comparison of OpenAI LLMs},
	year = {2023},
	url={https://iq.opengenus.org/gpt2-vs-gpt3-vs-gpt35-vs-gpt4/},
	urldate = {2023-12-21}
}

@online{HuggingFaceTokenizers,
	author = {Hugging Face},
	title = {Summary of the tokenizers},
	year = {2024},
	url={https://huggingface.co/docs/transformers/tokenizer_summary},
	urldate={2024-01-04}
}

@online{HuggingFaceGPT2,
	author = {Hugging Face},
	title = {OpenAI GPT2 - Hugging Face Transformers Documentation},
	year = {2024},
	url={https://huggingface.co/docs/transformers/model_doc/gpt2},
	urldate={2024-01-04}
}

@online{Mao2021Autoregressive,
	author = {Mao, Lei},
	title = {Autoregressive Model and Autoregressive Decoding for Sequence to Sequence Tasks},
	year = {2021},
	url={https://leimao.github.io/blog/Autoregressive-Model-Autoregressive-Decoding/},
	urldate={2024-01-04}
}

@online{OpenAI2019BetterLM,
	title={Better language models and their implications},
	author={OpenAI},
	year={2019},
	url={https://openai.com/research/better-language-models},
	urldate={2024-01-08}
}

@online{hfpretrained,
	author = {Hugging Face},
	title = {Pretrained models},
	url = {https://huggingface.co/transformers/v2.2.0/pretrained_models.html},
	urldate = {2024-01-11}
}

@online{hf-howtogeneratetext,
	author = {HuggingFace},
	title = {How to generate text: using different decoding methods for language generation with Transformers},
	year = {2020},
	url={https://huggingface.co/blog/how-to-generate},
	urldate = {2024-01-11}
}

@article{solving,
	title = {Solving arithmetic problems using feed-forward neural networks},
	journal = {Neurocomputing},
	volume = {18},
	number = {1},
	pages = {61-79},
	year = {1998},
	issn = {0925-2312},
	doi = {https://doi.org/10.1016/S0925-2312(97)00069-6},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231297000696},
	author = {Leonardo Franco and Sergio A. Cannas},
	keywords = {Neural networks, Arithmetic operations, Shifter circuit},
	abstract = {We design new feed-forward multi-layered neural networks which perform different elementary arithmetic operations, such as bit shifting, addition of N p-bit numbers, and multiplication of two n-bit numbers. All the structures are optimal in depth and are polynomially bounded in the number of neurons and in the number of synapses. The whole set of synaptic couplings and thresholds are obtained exactly.}
}

@online{nanogpt,
	author       = {Karpathy, Andrej},
	title        = {nanoGPT},
	date         = {2023},
	url          = {https://github.com/karpathy/nanoGPT},
	urldate      = {2025-10-19},
	note         = {GitHub repository}
}


@article{theoreticallimits,
	title={Theoretical Limitations of Self-Attention in Neural Sequence Models},
	volume={8},
	ISSN={2307-387X},
	url={http://dx.doi.org/10.1162/tacl_a_00306},
	DOI={10.1162/tacl_a_00306},
	journal={Transactions of the Association for Computational Linguistics},
	publisher={MIT Press - Journals},
	author={Hahn, Michael},
	year={2020},
	month=dec, pages={156–171}
}

@online{tippingpoint,
	author   = {Mollick, Ethan},
	title    = {ChatGPT Is a Tipping Point for AI},
	date     = {2022-12-14},
	url      = {https://hbr.org/2022/12/chatgpt-is-a-tipping-point-for-ai},
	organization = {Harvard Business Review},
	urldate  = {2025-10-30}
}

@misc{ba2016layer,
	title={Layer Normalization}, 
	author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
	year={2016},
	eprint={1607.06450},
	archivePrefix={arXiv},
	primaryClass={stat.ML},
	url={https://arxiv.org/abs/1607.06450}, 
}

@inproceedings{tokenizerchoice,
	title = "Tokenizer Choice For {LLM} Training: Negligible or Crucial?",
	author = {Ali, Mehdi  and
	Fromm, Michael  and
	Thellmann, Klaudia  and
	Rutmann, Richard  and
	L{\"u}bbering, Max  and
	Leveling, Johannes  and
	Klug, Katrin  and
	Ebert, Jan  and
	Doll, Niclas  and
	Buschhoff, Jasper  and
	Jain, Charvi  and
	Weber, Alexander  and
	Jurkschat, Lena  and
	Abdelwahab, Hammam  and
	John, Chelsea  and
	Ortiz Suarez, Pedro  and
	Ostendorff, Malte  and
	Weinbach, Samuel  and
	Sifa, Rafet  and
	Kesselheim, Stefan  and
	Flores-Herr, Nicolas},
	editor = "Duh, Kevin  and
	Gomez, Helena  and
	Bethard, Steven",
	booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
	month = jun,
	year = "2024",
	address = "Mexico City, Mexico",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-naacl.247/",
	doi = "10.18653/v1/2024.findings-naacl.247",
	pages = "3907--3924",
}


@article{xue2022byt5,
	author = {Xue, Linting and Barua, Aditya and Constant, Noah and Al-Rfou, Rami and Narang, Sharan and Kale, Mihir and Roberts, Adam and Raffel, Colin},
	title = {ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {10},
	pages = {291-306},
	year = {2022},
	month = {03},
	issn = {2307-387X},
	doi = {10.1162/tacl_a_00461},
	url = {https://doi.org/10.1162/tacl_a_00461},
	eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00461/2004058/tacl_a_00461.pdf},
}

@inproceedings{subwordunits,
	title = "Neural Machine Translation of Rare Words with Subword Units",
	author = "Sennrich, Rico  and
	Haddow, Barry  and
	Birch, Alexandra",
	editor = "Erk, Katrin  and
	Smith, Noah A.",
	booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = aug,
	year = "2016",
	address = "Berlin, Germany",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/P16-1162/",
	doi = "10.18653/v1/P16-1162",
	pages = "1715--1725"
}

@inproceedings{subwordreg,
	title = "Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates",
	author = "Kudo, Taku",
	editor = "Gurevych, Iryna  and
	Miyao, Yusuke",
	booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = jul,
	year = "2018",
	address = "Melbourne, Australia",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/P18-1007/",
	doi = "10.18653/v1/P18-1007",
	pages = "66--75",
	abstract = "Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT). While sentences are usually converted into unique subword sequences, subword segmentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary. The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robustness of NMT. We present a simple regularization method, subword regularization, which trains the model with multiple subword segmentations probabilistically sampled during training. In addition, for better subword sampling, we propose a new subword segmentation algorithm based on a unigram language model. We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings."
}

@inproceedings{devlin2019bert,
	title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
	author = "Devlin, Jacob  and
	Chang, Ming-Wei  and
	Lee, Kenton  and
	Toutanova, Kristina",
	editor = "Burstein, Jill  and
	Doran, Christy  and
	Solorio, Thamar",
	booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
	month = jun,
	year = "2019",
	address = "Minneapolis, Minnesota",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/N19-1423/",
	doi = "10.18653/v1/N19-1423",
	pages = "4171--4186",
	abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
}

@article{su2023roformer,
	title = {RoFormer: Enhanced transformer with Rotary Position Embedding},
	journal = {Neurocomputing},
	volume = {568},
	pages = {127063},
	year = {2024},
	issn = {0925-2312},
	doi = {https://doi.org/10.1016/j.neucom.2023.127063},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231223011864},
	author = {Jianlin Su and Murtadha Ahmed and Yu Lu and Shengfeng Pan and Wen Bo and Yunfeng Liu},
	keywords = {Pre-trained language models, Position information encoding, Pre-training, Natural language processing}
}

@misc{khandelwal2019sample,
	title={Sample Efficient Text Summarization Using a Single Pre-Trained Transformer}, 
	author={Urvashi Khandelwal and Kevin Clark and Dan Jurafsky and Lukasz Kaiser},
	year={2019},
	eprint={1905.08836},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/1905.08836}, 
}

@inproceedings{ippolito2019comparison,
	title = "Comparison of Diverse Decoding Methods from Conditional Language Models",
	author = "Ippolito, Daphne  and
	Kriz, Reno  and
	Sedoc, Jo{\~a}o  and
	Kustikova, Maria  and
	Callison-Burch, Chris",
	editor = "Korhonen, Anna  and
	Traum, David  and
	Marquez, Lluis",
	booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
	month = jul,
	year = "2019",
	address = "Florence, Italy",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/P19-1365/",
	doi = "10.18653/v1/P19-1365",
	pages = "3752--3762",
}

@inproceedings{yi2019coherent,
	title = "Towards Coherent and Engaging Spoken Dialog Response Generation Using Automatic Conversation Evaluators",
	author = "Yi, Sanghyun  and
	Goel, Rahul  and
	Khatri, Chandra  and
	Cervone, Alessandra  and
	Chung, Tagyoung  and
	Hedayatnia, Behnam  and
	Venkatesh, Anu  and
	Gabriel, Raefer  and
	Hakkani-Tur, Dilek",
	editor = "van Deemter, Kees  and
	Lin, Chenghua  and
	Takamura, Hiroya",
	booktitle = "Proceedings of the 12th International Conference on Natural Language Generation",
	month = 10,
	year = "2019",
	address = "Tokyo, Japan",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/W19-8608/",
	doi = "10.18653/v1/W19-8608",
	pages = "65--75",
}

@inproceedings{ouyang2022training,
	author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F and Leike, Jan and Lowe, Ryan},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
	pages = {27730--27744},
	publisher = {Curran Associates, Inc.},
	title = {Training language models to follow instructions with human feedback},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf},
	volume = {35},
	year = {2022}
}



@article{choi2023unleashing,
	title = "Unleashing the Potential of PIM: Accelerating Large Batched Inference of Transformer-Based Generative Models",
	keywords = "Transformer-based generative model, attention, processing-in-memory",
	author = "Jaewan Choi and Jaehyun Park and Kwanhee Kyung and Kim, \{Nam Sung\} and Ahn, \{Jung Ho\}",
	note = "This work was supported in part by Institute of IITP grant funded by the Korea government (MSIT) under Grants 2021-0-00863 and IITP-2023-RS- 2023-00256081 and in part by PRISM, one of the seven centers in JUMP 2.0, an SRC program sponsored by DARPA",
	year = "2023",
	month = jul,
	day = "1",
	doi = "10.1109/LCA.2023.3305386",
	language = "English (US)",
	volume = "22",
	pages = "113--116",
	journal = "IEEE Computer Architecture Letters",
	issn = "1556-6056",
	publisher = "Institute of Electrical and Electronics Engineers Inc.",
	number = "2",
}

@inproceedings {yu2022orca,
	author = {Gyeong-In Yu and Joo Seong Jeong and Geon-Woo Kim and Soojeong Kim and Byung-Gon Chun},
	title = {Orca: A Distributed Serving System for {Transformer-Based} Generative Models},
	booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
	year = {2022},
	isbn = {978-1-939133-28-1},
	address = {Carlsbad, CA},
	pages = {521--538},
	url = {https://www.usenix.org/conference/osdi22/presentation/yu},
	publisher = {USENIX Association},
	month = jul
}

@inproceedings{residual,
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	title = {Deep Residual Learning for Image Recognition},
	booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	month = {6},
	year = {2016}
}

@misc{gelu,
	title={Gaussian Error Linear Units (GELUs)}, 
	author={Dan Hendrycks and Kevin Gimpel},
	year={2023},
	eprint={1606.08415},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/1606.08415}, 
}

@inproceedings{adambeatssgd,
	author = {Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank and Kumar, Sanjiv and Sra, Suvrit},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {15383--15393},
	publisher = {Curran Associates, Inc.},
	title = {Why are Adaptive Methods Good for Attention Models?},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/b05b57f6add810d3b7490866d74c0053-Paper.pdf},
	volume = {33},
	year = {2020}
}

@article{10.3389/frobt.2023.1221739,
	AUTHOR={Chalvatzaki, Georgia  and Younes, Ali  and Nandha, Daljeet  and Le, An Thai  and Ribeiro, Leonardo F. R.  and Gurevych, Iryna },
	TITLE={Learning to reason over scene graphs: a case study of finetuning GPT-2 into a robot language model for grounded task planning},
	JOURNAL={Frontiers in Robotics and AI},
	VOLUME={Volume 10 - 2023},
	YEAR={2023},
	URL={https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2023.1221739},
	DOI={10.3389/frobt.2023.1221739},
	ISSN={2296-9144},
}

@inproceedings{worth16words,
	title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
	author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
	booktitle={International Conference on Learning Representations},
	year={2021},
	url={https://openreview.net/forum?id=YicbFdNTTy}
}

@inproceedings{cogview,
	author = {Ding, Ming and Yang, Zhuoyi and Hong, Wenyi and Zheng, Wendi and Zhou, Chang and Yin, Da and Lin, Junyang and Zou, Xu and Shao, Zhou and Yang, Hongxia and Tang, Jie},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages = {19822--19835},
	publisher = {Curran Associates, Inc.},
	title = {CogView: Mastering Text-to-Image Generation via Transformers},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/a4d92e2cd541fca87e4620aba658316d-Paper.pdf},
	volume = {34},
	year = {2021}
}

@inproceedings{decisiontransformer,
	author = {Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Misha and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages = {15084--15097},
	publisher = {Curran Associates, Inc.},
	title = {Decision Transformer: Reinforcement Learning via Sequence Modeling},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/7f489f642a0ddb10272b5c31057f0663-Paper.pdf},
	volume = {34},
	year = {2021}
}

@inproceedings{grandmasterlevelchess,
	author = {Ruoss, Anian and Del\'{e}tang, Gr\'{e}goire and Medapati, Sourabh and Grau-Moya, Jordi and Wenliang, Li Kevin and Catt, Elliot and Reid, John and Lewis, Cannada A. and Veness, Joel and Genewein, Tim},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
	pages = {65765--65790},
	publisher = {Curran Associates, Inc.},
	title = {Amortized Planning with Large-Scale Transformers: A Case Study on Chess},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/78f0db30c39c850de728c769f42fc903-Paper-Conference.pdf},
	volume = {37},
	year = {2024}
}

@misc{noever2020chess,
	title={The Chess Transformer: Mastering Play using Generative Language Models}, 
	author={David Noever and Matt Ciolino and Josh Kalin},
	year={2020},
	eprint={2008.04057},
	archivePrefix={arXiv},
	primaryClass={cs.AI},
	url={https://arxiv.org/abs/2008.04057}, 
}

@inproceedings{gotransformer,
	author       = {Matthew Ciolino and
	Josh Kalin and
	David Noever},
	title        = {The Go Transformer: Natural Language Modeling for Game Play},
	booktitle    = {Third International Conference on Artificial Intelligence for Industries,
	{AI4I} 2020, Irvine, CA, USA, September 21-23, 2020},
	pages        = {23--26},
	publisher    = {{IEEE}},
	year         = {2020},
	url          = {https://doi.org/10.1109/AI4I49448.2020.00012},
	doi          = {10.1109/AI4I49448.2020.00012},
	timestamp    = {Fri, 20 Nov 2020 12:16:48 +0100},
	biburl       = {https://dblp.org/rec/conf/ai4i/CiolinoKN20.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@incollection{transferlearning,
	title={Transfer learning},
	author={Torrey, Lisa and Shavlik, Jude},
	booktitle={Handbook of research on machine learning applications and trends: algorithms, methods, and techniques},
	pages={242--264},
	year={2010},
	publisher={IGI global}
}

@inproceedings{teaching,
	author = {Lee, Nayoung and Sreenivasan, Kartik and Lee, Jason and Lee, Kangwook and Papailiopoulos, Dimitris},
	booktitle = {International Conference on Representation Learning},
	editor = {B. Kim and Y. Yue and S. Chaudhuri and K. Fragkiadaki and M. Khan and Y. Sun},
	pages = {25001--25054},
	title = {Teaching Arithmetic to Small Transformers},
	url = {https://proceedings.iclr.cc/paper_files/paper/2024/file/6bf82fdcbd92b6a7793b3894422d2437-Paper-Conference.pdf},
	volume = {2024},
	year = {2024}
}

@inproceedings{visual,
	author       = {Yedid Hoshen and
	Shmuel Peleg},
	editor       = {Dale Schuurmans and
	Michael P. Wellman},
	title        = {Visual Learning of Arithmetic Operation},
	booktitle    = {Proceedings of the Thirtieth {AAAI} Conference on Artificial Intelligence,
	February 12-17, 2016, Phoenix, Arizona, {USA}},
	pages        = {3733--3739},
	publisher    = {{AAAI} Press},
	year         = {2016},
	url          = {https://doi.org/10.1609/aaai.v30i1.9882},
	doi          = {10.1609/AAAI.V30I1.9882},
	timestamp    = {Mon, 04 Sep 2023 16:50:24 +0200},
	biburl       = {https://dblp.org/rec/conf/aaai/HoshenP16.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{memtocomp,
	title = {Arithmetic with language models: From memorization to computation},
	journal = {Neural Networks},
	volume = {179},
	pages = {106550},
	year = {2024},
	issn = {0893-6080},
	doi = {https://doi.org/10.1016/j.neunet.2024.106550},
	url = {https://www.sciencedirect.com/science/article/pii/S089360802400474X},
	author = {Davide Maltoni and Matteo Ferrara},
	keywords = {Language models, AI explainability, Probing, Interpretability, Arithmetic},
}

@inproceedings{rightembeddings,
	author = {McLeish, Sean and Bansal, Arpit and Stein, Alex and Jain, Neel and Kirchenbauer, John and Bartoldson, Brian R. and Kailkhura, Bhavya and Bhatele, Abhinav and Geiping, Jonas and Schwarzschild, Avi and Goldstein, Tom},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
	pages = {108012--108041},
	publisher = {Curran Associates, Inc.},
	title = {Transformers Can Do Arithmetic with the Right Embeddings},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/c35986bc1ee29b31c1011481b77fe540-Paper-Conference.pdf},
	volume = {37},
	year = {2024}
}

@misc{goat,
	title={Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks}, 
	author={Tiedong Liu and Bryan Kian Hsiang Low},
	year={2023},
	eprint={2305.14201},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2305.14201}, 
}

@misc{nocalculator,
	title={GPT Can Solve Mathematical Problems Without a Calculator}, 
	author={Zhen Yang and Ming Ding and Qingsong Lv and Zhihuan Jiang and Zehai He and Yuyi Guo and Jinfeng Bai and Jie Tang},
	year={2023},
	eprint={2309.03241},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2309.03241}, 
}

@misc{implicit,
	title={From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step}, 
	author={Yuntian Deng and Yejin Choi and Stuart Shieber},
	year={2024},
	eprint={2405.14838},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2405.14838}, 
}

@misc{positionmatters,
	title={Positional Description Matters for Transformers Arithmetic}, 
	author={Ruoqi Shen and Sébastien Bubeck and Ronen Eldan and Yin Tat Lee and Yuanzhi Li and Yi Zhang},
	year={2023},
	eprint={2311.14737},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2311.14737}, 
}

@misc{dissecting,
	title={Dissecting Multiplication in Transformers: Insights into LLMs}, 
	author={Luyu Qiu and Jianing Li and Chi Su and Chen Jason Zhang and Lei Chen},
	year={2024},
	eprint={2407.15360},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2407.15360}, 
}

@inproceedings{lengthandcount,
	author = {Cho, Hanseul and Cha, Jaeyoung and Bhojanapalli, Srinadh and Yun, Chulhee},
	booktitle = {International Conference on Representation Learning},
	editor = {Y. Yue and A. Garg and N. Peng and F. Sha and R. Yu},
	pages = {79987--80029},
	title = {Arithmetic Transformers Can Length-Generalize in Both Operand Length and Count},
	url = {https://proceedings.iclr.cc/paper_files/paper/2025/file/c6c29e590e3c62e37e6b39cdd6baf2e8-Paper-Conference.pdf},
	volume = {2025},
	year = {2025}
}

@misc{lengthgen,
	title={Length Generalization in Arithmetic Transformers}, 
	author={Samy Jelassi and Stéphane d'Ascoli and Carles Domingo-Enrich and Yuhuai Wu and Yuanzhi Li and François Charton},
	year={2023},
	eprint={2306.15400},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2306.15400}, 
}

@misc{intertoextra,
	title={From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers}, 
	author={Shaoxiong Duan and Yining Shi and Wei Xu},
	year={2024},
	eprint={2310.11984},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2310.11984}, 
}

@misc{explained,
	title={Arithmetic in Transformers Explained}, 
	author={Philip Quirke and Clement Neo and Fazl Barez},
	year={2025},
	eprint={2402.02619},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2402.02619}, 
}

@inproceedings{impact,
	author       = {Amirhossein Kazemnejad and
	Inkit Padhi and
	Karthikeyan Natesan Ramamurthy and
	Payel Das and
	Siva Reddy},
	editor       = {Alice Oh and
	Tristan Naumann and
	Amir Globerson and
	Kate Saenko and
	Moritz Hardt and
	Sergey Levine},
	title        = {The Impact of Positional Encoding on Length Generalization in Transformers},
	booktitle    = {Advances in Neural Information Processing Systems 36: Annual Conference
	on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
	LA, USA, December 10 - 16, 2023},
	year         = {2023},
	url          = {http://papers.nips.cc/paper\_files/paper/2023/hash/4e85362c02172c0c6567ce593122d31c-Abstract-Conference.html},
	timestamp    = {Fri, 01 Mar 2024 16:26:20 +0100},
	biburl       = {https://dblp.org/rec/conf/nips/KazemnejadPRDR23.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{phrasereps,
	author       = {Kyunghyun Cho and
	Bart van Merrienboer and
	{\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
	Dzmitry Bahdanau and
	Fethi Bougares and
	Holger Schwenk and
	Yoshua Bengio},
	editor       = {Alessandro Moschitti and
	Bo Pang and
	Walter Daelemans},
	title        = {Learning Phrase Representations using {RNN} Encoder-Decoder for Statistical
	Machine Translation},
	booktitle    = {Proceedings of the 2014 Conference on Empirical Methods in Natural
	Language Processing, {EMNLP} 2014, October 25-29, 2014, Doha, Qatar,
	{A} meeting of SIGDAT, a Special Interest Group of the {ACL}},
	pages        = {1724--1734},
	publisher    = {{ACL}},
	year         = {2014},
	url          = {https://doi.org/10.3115/v1/d14-1179},
	doi          = {10.3115/V1/D14-1179},
	timestamp    = {Sun, 06 Oct 2024 21:00:49 +0200},
	biburl       = {https://dblp.org/rec/conf/emnlp/ChoMGBBSB14.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{convseq,
	author       = {Jonas Gehring and
	Michael Auli and
	David Grangier and
	Denis Yarats and
	Yann N. Dauphin},
	editor       = {Doina Precup and
	Yee Whye Teh},
	title        = {Convolutional Sequence to Sequence Learning},
	booktitle    = {Proceedings of the 34th International Conference on Machine Learning,
	{ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017},
	series       = {Proceedings of Machine Learning Research},
	volume       = {70},
	pages        = {1243--1252},
	publisher    = {{PMLR}},
	year         = {2017},
	url          = {http://proceedings.mlr.press/v70/gehring17a.html},
	timestamp    = {Wed, 29 May 2019 08:41:45 +0200},
	biburl       = {https://dblp.org/rec/conf/icml/GehringAGYD17.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{seq2seq,
	author       = {Ilya Sutskever and
	Oriol Vinyals and
	Quoc V. Le},
	editor       = {Zoubin Ghahramani and
	Max Welling and
	Corinna Cortes and
	Neil D. Lawrence and
	Kilian Q. Weinberger},
	title        = {Sequence to Sequence Learning with Neural Networks},
	booktitle    = {Advances in Neural Information Processing Systems 27: Annual Conference
	on Neural Information Processing Systems 2014, December 8-13 2014,
	Montreal, Quebec, Canada},
	pages        = {3104--3112},
	year         = {2014},
	url          = {https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html},
	timestamp    = {Mon, 16 May 2022 15:41:51 +0200},
	biburl       = {https://dblp.org/rec/conf/nips/SutskeverVL14.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{weightstying,
	author       = {Ofir Press and
	Lior Wolf},
	editor       = {Mirella Lapata and
	Phil Blunsom and
	Alexander Koller},
	title        = {Using the Output Embedding to Improve Language Models},
	booktitle    = {Proceedings of the 15th Conference of the European Chapter of the
	Association for Computational Linguistics, {EACL} 2017, Valencia,
	Spain, April 3-7, 2017, Volume 2: Short Papers},
	pages        = {157--163},
	publisher    = {Association for Computational Linguistics},
	year         = {2017},
	url          = {https://doi.org/10.18653/v1/e17-2025},
	doi          = {10.18653/V1/E17-2025},
	timestamp    = {Fri, 06 Aug 2021 00:40:45 +0200},
	biburl       = {https://dblp.org/rec/conf/eacl/PressW17.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{selfsupervisedvisual,
	author       = {Longlong Jing and
	Yingli Tian},
	title        = {Self-Supervised Visual Feature Learning With Deep Neural Networks:
	{A} Survey},
	journal      = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	volume       = {43},
	number       = {11},
	pages        = {4037--4058},
	year         = {2021},
	url          = {https://doi.org/10.1109/TPAMI.2020.2992393},
	doi          = {10.1109/TPAMI.2020.2992393},
	timestamp    = {Wed, 16 Mar 2022 23:54:55 +0100},
	biburl       = {https://dblp.org/rec/journals/pami/JingT21.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{pytorch,
	author       = {Adam Paszke and
	Sam Gross and
	Francisco Massa and
	Adam Lerer and
	James Bradbury and
	Gregory Chanan and
	Trevor Killeen and
	Zeming Lin and
	Natalia Gimelshein and
	Luca Antiga and
	Alban Desmaison and
	Andreas K{\"{o}}pf and
	Edward Z. Yang and
	Zachary DeVito and
	Martin Raison and
	Alykhan Tejani and
	Sasank Chilamkurthy and
	Benoit Steiner and
	Lu Fang and
	Junjie Bai and
	Soumith Chintala},
	editor       = {Hanna M. Wallach and
	Hugo Larochelle and
	Alina Beygelzimer and
	Florence d'Alch{\'{e}}{-}Buc and
	Emily B. Fox and
	Roman Garnett},
	title        = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
	booktitle    = {Advances in Neural Information Processing Systems 32: Annual Conference
	on Neural Information Processing Systems 2019, NeurIPS 2019, December
	8-14, 2019, Vancouver, BC, Canada},
	pages        = {8024--8035},
	year         = {2019},
	url          = {https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html},
	timestamp    = {Mon, 16 May 2022 15:41:51 +0200},
	biburl       = {https://dblp.org/rec/conf/nips/PaszkeGMLBCKLGA19.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{analyzingheads,
	author       = {Elena Voita and
	David Talbot and
	Fedor Moiseev and
	Rico Sennrich and
	Ivan Titov},
	editor       = {Anna Korhonen and
	David R. Traum and
	Lluis Marquez},
	title        = {Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy
	Lifting, the Rest Can Be Pruned},
	booktitle    = {Proceedings of the 57th Conference of the Association for Computational
	Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
	Volume 1: Long Papers},
	pages        = {5797--5808},
	publisher    = {Association for Computational Linguistics},
	year         = {2019},
	url          = {https://doi.org/10.18653/v1/p19-1580},
	doi          = {10.18653/V1/P19-1580},
	timestamp    = {Thu, 07 Aug 2025 09:17:37 +0200},
	biburl       = {https://dblp.org/rec/conf/acl/VoitaTMST19.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{bertsecrets,
	author       = {Olga Kovaleva and
	Alexey Romanov and
	Anna Rogers and
	Anna Rumshisky},
	editor       = {Kentaro Inui and
	Jing Jiang and
	Vincent Ng and
	Xiaojun Wan},
	title        = {Revealing the Dark Secrets of {BERT}},
	booktitle    = {Proceedings of the 2019 Conference on Empirical Methods in Natural
	Language Processing and the 9th International Joint Conference on
	Natural Language Processing, {EMNLP-IJCNLP} 2019, Hong Kong, China,
	November 3-7, 2019},
	pages        = {4364--4373},
	publisher    = {Association for Computational Linguistics},
	year         = {2019},
	url          = {https://doi.org/10.18653/v1/D19-1445},
	doi          = {10.18653/V1/D19-1445},
	timestamp    = {Thu, 07 Apr 2022 09:14:07 +0200},
	biburl       = {https://dblp.org/rec/conf/emnlp/KovalevaRRR19.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{selfsupervised,
	title={A Cookbook of Self-Supervised Learning}, 
	author={Randall Balestriero and Mark Ibrahim and Vlad Sobal and Ari Morcos and Shashank Shekhar and Tom Goldstein and Florian Bordes and Adrien Bardes and Gregoire Mialon and Yuandong Tian and Avi Schwarzschild and Andrew Gordon Wilson and Jonas Geiping and Quentin Garrido and Pierre Fernandez and Amir Bar and Hamed Pirsiavash and Yann LeCun and Micah Goldblum},
	year={2023},
	eprint={2304.12210},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2304.12210}, 
}
