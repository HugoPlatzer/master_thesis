@misc{allyouneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{improvinglu,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}

@misc{unsupervisedmultitask,
  added-at = {2019-02-27T03:35:25.000+0100},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  biburl = {https://www.bibsonomy.org/bibtex/2b30710316a8cfbae687672ea1f85c193/kirk86},
  description = {Language Models are Unsupervised Multitask Learners},
  interhash = {ce8168300081d74707849ed488e2a458},
  intrahash = {b30710316a8cfbae687672ea1f85c193},
  keywords = {learning multitask},
  timestamp = {2019-02-27T03:35:25.000+0100},
  title = {Language Models are Unsupervised Multitask Learners},
  url = {https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf},
  year = 2018
}

@article{fewshotlearners,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@misc{alammar-transformer,
  title = {The Illustrated Transformer},
  author = {Jay Alammar},
  howpublished = {\url{http://jalammar.github.io/illustrated-transformer/}},
  note = {Accessed: 2023-06-06}
}

@misc{alammar-gpt2,
  title = {The Illustrated GPT2},
  author = {Jay Alammar},
  howpublished = {\url{http://jalammar.github.io/illustrated-gpt2/}},
  note = {Accessed: 2023-06-06}
}

@misc{jay-visualizing,
  title = {Visualizing a Neural Machine Translation Model (Mechanics of Seq2Seq Models with Attention)},
  author = {Jay Alammar},
  howpublished = {\url{https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/}},
  note = {Accessed: 2023-06-06}
}

@misc{github-hf,
  author = {The Hugging Face team},
  title = {transformers},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/huggingface/transformers}},
  commit = {7631db0fdcfbd95b1f21d8034a0b8df73b9380ff}
}

@misc{ba2016layer,
      title={Layer Normalization}, 
      author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
      year={2016},
      eprint={1607.06450},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{tf-image,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2021},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{openai_chatgpt_2022,
  title={Introducing ChatGPT},
  author={{OpenAI}},
  year={2022},
  month={Nov},
  howpublished={\url{https://openai.com/blog/chatgpt/}},
  note={Accessed on: 2023-12-19}
}

@inproceedings{Savelka_2023, series={ICER 2023},
   title={Thrilled by Your Progress! Large Language Models (GPT-4) No Longer Struggle to Pass Assessments in Higher Education Programming Courses},
   url={http://dx.doi.org/10.1145/3568813.3600142},
   DOI={10.1145/3568813.3600142},
   booktitle={Proceedings of the 2023 ACM Conference on International Computing Education Research V.1},
   publisher={ACM},
   author={Savelka, Jaromir and Agarwal, Arav and An, Marshall and Bogart, Chris and Sakr, Majd},
   year={2023},
   month=aug, collection={ICER 2023} }


@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{OpenGenus2023GPTComparison,
  author = {OpenGenus},
  title = {GPT-2 vs GPT-3 vs GPT-3.5 vs GPT-4: A Comprehensive Comparison of OpenAI LLMs},
  year = {2023},
  howpublished = {\url{https://iq.opengenus.org/gpt2-vs-gpt3-vs-gpt35-vs-gpt4/}},
  note = {Accessed: 2023-12-21}
}

@misc{yang2023gpt,
      title={GPT Can Solve Mathematical Problems Without a Calculator}, 
      author={Zhen Yang and Ming Ding and Qingsong Lv and Zhihuan Jiang and Zehai He and Yuyi Guo and Jinfeng Bai and Jie Tang},
      year={2023},
      eprint={2309.03241},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{radford2018improving,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  journal={Papers With Code},
  url={https://paperswithcode.com/paper/improving-language-understanding-by},
  year={2018}
}



@misc{xue2022byt5,
      title={ByT5: Towards a token-free future with pre-trained byte-to-byte models}, 
      author={Linting Xue and Aditya Barua and Noah Constant and Rami Al-Rfou and Sharan Narang and Mihir Kale and Adam Roberts and Colin Raffel},
      year={2022},
      eprint={2105.13626},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{tokenizerchoice,
  title={Tokenizer Choice For LLM Training: Negligible or Crucial?},
  author={Ali, Mehdi and Fromm, Michael and Thellmann, Klaudia and Rutmann, Richard and L{\"u}bbering, Max and Leveling, Johannes and Klug, Katrin and Ebert, Jan and Doll, Niclas and Buschhoff, Jasper Schulze and others},
  journal={arXiv preprint arXiv:2310.08754},
  year={2023}
}

@misc{subwordunits,
      title={Neural Machine Translation of Rare Words with Subword Units}, 
      author={Rico Sennrich and Barry Haddow and Alexandra Birch},
      year={2016},
      eprint={1508.07909},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{subwordreg,
  title={Subword regularization: Improving neural network translation models with multiple subword candidates},
  author={Kudo, Taku},
  journal={arXiv preprint arXiv:1804.10959},
  year={2018}
}





@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}



@misc{HuggingFaceGPT2,
  author = {Hugging Face},
  title = {OpenAI GPT2 - Hugging Face Transformers Documentation},
  year = {2024},
  howpublished = {\url{https://huggingface.co/docs/transformers/model_doc/gpt2}}
}

@misc{HuggingFaceTokenizers,
  author = {Hugging Face},
  title = {Summary oof the tokenizers},
  year = {2024},
  howpublished = {\url{https://huggingface.co/docs/transformers/tokenizer_summary}}
}

@misc{su2023roformer,
      title={RoFormer: Enhanced Transformer with Rotary Position Embedding}, 
      author={Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},
      year={2023},
      eprint={2104.09864},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{khandelwal2019sample,
      title={Sample Efficient Text Summarization Using a Single Pre-Trained Transformer}, 
      author={Urvashi Khandelwal and Kevin Clark and Dan Jurafsky and Lukasz Kaiser},
      year={2019},
      eprint={1905.08836},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{Mao2021Autoregressive,
  author = {Mao, Lei},
  title = {Autoregressive Model and Autoregressive Decoding for Sequence to Sequence Tasks},
  year = {2021},
  howpublished = {\url{https://leimao.github.io/blog/Autoregressive-Model-Autoregressive-Decoding/}},
  note = {Accessed: Jan 04 2024}
}

@misc{ippolito2019comparison,
      title={Comparison of Diverse Decoding Methods from Conditional Language Models}, 
      author={Daphne Ippolito and Reno Kriz and Maria Kustikova and Jo√£o Sedoc and Chris Callison-Burch},
      year={2019},
      eprint={1906.06362},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{OpenAI2019BetterLM,
  title={Better Language Models},
  author={OpenAI},
  year={2019},
  howpublished={\url{https://openai.com/research/better-language-models}},
  note={Accessed: 2024-01-08}
}

@misc{yi2019coherent,
      title={Towards Coherent and Engaging Spoken Dialog Response Generation Using Automatic Conversation Evaluators}, 
      author={Sanghyun Yi and Rahul Goel and Chandra Khatri and Alessandra Cervone and Tagyoung Chung and Behnam Hedayatnia and Anu Venkatesh and Raefer Gabriel and Dilek Hakkani-Tur},
      year={2019},
      eprint={1904.13015},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ouyang2022training,
      title={Training language models to follow instructions with human feedback}, 
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{choi2023unleashing,
  title={Unleashing the Potential of PIM: Accelerating Large Batched Inference of Transformer-Based Generative Models},
  author={Choi, Jaewan and Park, Jaehyun and Kyung, Kwanhee and Kim, Nam Sung and Ahn, Jung Ho},
  journal={IEEE Computer Architecture Letters},
  year={2023},
  publisher={IEEE}
}

@inproceedings{yu2022orca,
  title={Orca: A distributed serving system for $\{$Transformer-Based$\}$ generative models},
  author={Yu, Gyeong-In and Jeong, Joo Seong and Kim, Geon-Woo and Kim, Soojeong and Chun, Byung-Gon},
  booktitle={16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
  pages={521--538},
  year={2022}
}

@misc{hfpretrained,
  title = {Pretrained models},
  url = {https://huggingface.co/transformers/v2.2.0/pretrained_models.html},
  publisher = {Hugging Face},
  key={hfpretrained},
  note = {Accessed: 2024-01-11}
}

@misc{hf-howtogeneratetext,
  author = {Patrick von Platen, HuggingFace},
  title = {How to generate text: using different decoding methods for language generation with Transformers},
  year = {2020},
  howpublished = {\url{https://huggingface.co/blog/how-to-generate}},
  note = {Accessed: 2024-01-11}
}

@inproceedings{residual,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{gelu,
  title={Gaussian error linear units (gelus)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}

@misc{adambeatsgd,
  title={Why adam beats sgd for attention models},
  author={Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank J and Kumar, Sanjiv and Sra, Suvrit},
  year={2019}
}

@article{reasonoverscenegraphs,
  title={Learning to reason over scene graphs: a case study of finetuning GPT-2 into a robot language model for grounded task planning},
  author={Chalvatzaki, Georgia and Younes, Ali and Nandha, Daljeet and Le, An Thai and Ribeiro, Leonardo FR and Gurevych, Iryna},
  journal={Frontiers in Robotics and AI},
  volume={10},
  year={2023},
  publisher={Frontiers Media SA}
}


@misc{worth16words,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2021},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@article{cogview,
  title={Cogview: Mastering text-to-image generation via transformers},
  author={Ding, Ming and Yang, Zhuoyi and Hong, Wenyi and Zheng, Wendi and Zhou, Chang and Yin, Da and Lin, Junyang and Zou, Xu and Shao, Zhou and Yang, Hongxia and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={19822--19835},
  year={2021}
}


@inproceedings{decisiontransformer,
 author = {Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Misha and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {15084--15097},
 publisher = {Curran Associates, Inc.},
 title = {Decision Transformer: Reinforcement Learning via Sequence Modeling},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/7f489f642a0ddb10272b5c31057f0663-Paper.pdf},
 volume = {34},
 year = {2021}
}

@misc{noever2020chess,
      title={The Chess Transformer: Mastering Play using Generative Language Models}, 
      author={David Noever and Matt Ciolino and Josh Kalin},
      year={2020},
      eprint={2008.04057},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{grandmasterlevelchess,
      title={Grandmaster-Level Chess Without Search}, 
      author={Anian Ruoss and Gr√©goire Del√©tang and Sourabh Medapati and Jordi Grau-Moya and Li Kevin Wenliang and Elliot Catt and John Reid and Tim Genewein},
      year={2024},
      eprint={2402.04494},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@INPROCEEDINGS{gotransformer,
  author={Ciolino, Matthew and Kalin, Josh and Noever, David},
  booktitle={2020 Third International Conference on Artificial Intelligence for Industries (AI4I)}, 
  title={The Go Transformer: Natural Language Modeling for Game Play}, 
  year={2020},
  volume={},
  number={},
  pages={23-26},
  keywords={Sequential analysis;Training data;Games;Learning (artificial intelligence);Data models;Natural language processing;Task analysis;Natural Language Processing (NLP);Transformers;Game Play;Deep Learning},
  doi={10.1109/AI4I49448.2020.00012}}

@incollection{transferlearning,
  title={Transfer learning},
  author={Torrey, Lisa and Shavlik, Jude},
  booktitle={Handbook of research on machine learning applications and trends: algorithms, methods, and techniques},
  pages={242--264},
  year={2010},
  publisher={IGI global}
}