\section{Experimental setup}
\label{setup:main}

Based on the methods outlined in \cref{methods}, we set up a training pipeline that first creates a dataset for the given arithmetic operation, operand digits and sampling method, then initializes a GPT-2-style model (\cref{methods:model}) with the given model configuration, and then trains the model on the dataset, monitoring loss and accuracy during training.
The details of our training pipeline are as follows:

\begin{itemize}
\item \textbf{Libraries/Frameworks:}
We use the HuggingFace transformers library \cite{github-hf} with PyTorch backend \cite{pytorch} which takes care of implementing transformer networks and the training process. Based on this, we wrote some Python code that implements generating training samples, running model training, evaluating model performance, logging results and generating plots. The generation of plots from the result logs utilized the matplotlib Python library.

\item \textbf{Hardware:}
The experiments were run on Linux machines with NVIDIA GPUs that were rented and accessed via a GPU renting website. We ensured each system had a reasonably recent NVIDIA GPU with at least 16GB VRAM to ensure our experiment computations fit inside GPU memory.

\item \textbf{Model implementation:}
We use the GPT2LMHeadModel class of the HuggingFace transformers library which implements a GPT-2-style decoder-only transformer model.
We adjust the model configuration so it matches nanoGPT \cite{nanogpt}, a basic lightweight GPT-2 implementation which was also used for the experiments in \cite{teaching}. 
The model uses $\dHidden=384$, $\nBlocks=6$, $\nHeads=6$.

\item \textbf{Weight initialization:}
The model weights are initialized using the default distributions as defined in the GPT2LMHeadModel class of the HuggingFace transformers framework.

\item \textbf{Optimizer:} Training uses the AdamW optimizer with default parameters as defined by the HuggingFace transformers Trainer / TrainingArguments class.


\item \textbf{Batch size:}
For the baseline experiments, we used a fixed batch size of 256. We picked this batch size as it was also used in \cite{teaching} and it fits the memory of our training GPU. For some experiments (\cref{setup:scratchpad}) we needed to reduce batch size to fit batches into GPU memory.


\item \textbf{Learning rate scheduling:}
The learning rate is adjusted using a linear schedule with warmup steps, with default parameters as defined by the HuggingFace transformers framework.
For computing the learning rate, we assume a maximum of 200 epochs for training.

\item \textbf{Early stopping:}
We use an early stopping strategy where training is aborted if for 5 epochs there is no new minimum in loss on the validation dataset (minimum improvement $\epsilon=10^{-4}$).

\item \textbf{Text generation:}
To generate answers to a prompt string like "1+2=", we use auto-regressive generation as is standard practice with transformer decoder models (\cref{transformer:textgen}). We use greedy decoding, always selecting the token with the highest probability. Generation stops when token ID 0 (stop token) is generated or the model maximum sequence length is reached.
\end{itemize}


\subsection{Datasets}

Based on the sampling and formatting methods described above, for each experiment we create three separate dataset:

\begin{itemize}
	\item \textbf{Training dataset:} This dataset is used to optimize the model weights during training. Its size varies from 1k to 100k samples as part of the experiment.
	\item \textbf{Validation dataset:} A separate dataset with 1k samples. Loss on this dataset is used to decide when to stop training (early stopping). Accuracy on this dataset is also tracked.
	For some experiments (\cref{setup:scratchpad}) we had to lower the size of the validation dataset to avoid accuracy tracking slowing down training too much.
	\item \textbf{Test dataset:} Another separate dataset with 1k samples. This dataset is used at the end of each training run to compute the final accuracy numbers.
\end{itemize}

All datasets are sampled using the \textbf{basic sampling} method illustrated in \cref{methods:sampling}. This applies to all experiments except those studying different sampling methods (\cref{setup:sampling}).
For addition and multiplication, the operands $a$ and $b$ of each sample are encoded into a string based on their decimal digit representation, the same applies to the sum / product $c$. 
To avoid uneven string/sequence length, $c$ is encoded with leading zeroes up to the maximum possible sum / product length based on the operand size. For example, for 3-digit addition, the sum is padded to 4 digits:

\begin{lstlisting}
	"123+456=0579"
\end{lstlisting}

For the square root operation, we encode a sample like this:

\begin{lstlisting}
	"500000:707"
\end{lstlisting}

For even $n$, the integer square root of a $n$-digit decimal number always has $\frac{n}{2}$ decimal digits, thus no padding is needed.

\subsection{Evaluation}
\label{setup:eval}

During the training process, multiple metrics are tracked:

\begin{itemize}
	\item \textbf{Average batch loss:}
	For each training sample, the model outputs a token probability distribution at each position of the sample. This is compared with the correct next token at this position to compute a cross-entropy loss number. This loss is averaged among all positions of a sample and all samples in the current batch. To reduce statistical noise, the HuggingFace trainer class also averages these batch loss numbers among multiple consecutive training steps.
	
	\item \textbf{Validation loss:}
	At the end of each epoch, the average loss over all samples in the validation dataset is computed. This number is used to track training progress and stop training if no more progress is made.
	
	\item \textbf{Validation accuracy:}
	For each sample in the validation dataset, the string is split at the "=" character, yielding a prompt and answer string (e.g. prompt "1+2=" and corresponding answer "3"). For the prompt string, the model's answer is generated as described in \cref{gpt2:textgen}. This answer is then compared to the correct answer. The ratio of samples where the model's answer matches the correct answer gives the accuracy.
	
	\item \textbf{Test accuracy:}
	It is evaluated just like validation dataset accuracy, but on the test dataset. This gives the final accuracy numbers reported in the tables below.
\end{itemize}


\subsection{Error analysis}
\label{setup:error}

We wanted to analyze not just the performance of the final models in terms of the percentage of correctly solved tasks, but also take a look at the types of errors the model makes.
For this, we picked the first few tasks from the test dataset that the model answered incorrectly. If there were no errors in the test dataset, we created an additional error analysis dataset with 100k samples to look for rare errors.

If there were errors, we then created a table with a few of the error samples for the experiment, with columns for the input (e.g., "123+456"), expected output (e.g., "0579"), model output (e.g., "0679") and additional remarks for some experiments. For longer output sequences (e.g., scratchpad experiments) the erroneous part of the model output was highlighted.
We then tried to identify and discuss observable patterns in the small sample of errors.

\subsection{Baseline experiments}
\label{setup:baseline}

To test the ability of transformer networks to learn arithmetic operations, and to see whether we can replicate the results of \cite{teaching}, we first run some baseline experiments. The training pipeline is as described above and the training samples for the studied operations were created as described in \cref{methods:ops}.



We run a total of 3*3*3=27 baseline experiments, comparing 3 operations, 3 operand sizes per operation, and 3 training dataset sizes:
 
\begin{itemize}
	\item \textbf{Addition:} 3, 5, or 10 operand digits, 1k, 10k, or 100k training dataset.
	\item \textbf{Multiplication:} 3, 5, or 10 operand digits, 1k, 10k, or 100k training dataset.
	\item \textbf{Integer square root:} 6, 10, or 20 operand digits, 1k, 10k, or 100k training dataset.
\end{itemize}


Because integer square root is a unary operation, we double the number of operand digits compared to addition or multiplication.



\subsection{Model size experiments}
\label{setup:modelsize}

To see whether changing the number of parameters of the model improves accuracy, we tried each of the following changes to model dimensions:

\begin{itemize}
	\item Change the hidden vector size $\dHidden$ from 384 to either 192 or 768. This causes a substantial change to the number of learnable model parameters: The model with $\dHidden=192$ has about 2.7M parameters, the baseline model with $\dHidden=384$ about 11M parameters, and the model with $\dHidden=768$ about 43M parameters. Thus doubling $\dHidden$ roughly quadruples the number of parameters (due to matrices for transformations on vectors twice the size having 4 times as many parameters).
	\item Change the number of GPT-2 blocks $\nBlocks$ from 6 to either 3 or 12. This also changes the number of parameters, but in a linear fashion: The model with 3 blocks has 5.4M parameters, and the model with 12 blocks has 21M parameters. 
	\item Change the number of attention heads $\nHeads$ from 6 to either 3 or 12. This has no effect on the number of model parameters, it only affects the "partitioning" of the parameters inside the model.
\end{itemize}

We did not try to combine these changes, each experiment only had one parameter of the model architecture changed.
Due to time/hardware constraints, we limited experiments to the following tasks:

\begin{itemize}
	\item \textbf{Addition:} 10 operand digits, 100k training dataset.
	\item \textbf{Multiplication:} 5 operand digits, 100k training dataset.
\end{itemize}

We picked these two tasks from the baseline experiments because addition with 10 digits could be learned well with the 100k dataset, while still being harder than 3 or 5 digit addition, allowing more chances to see a difference in model performance in terms of how many optimization steps it takes to converge.
Similarly, multiplication with 5 digits and 100k training dataset was chosen because the baseline model could not learn it and achieved zero accuracy. At the same time, learning 5 digit multiplication seemed somewhat realistic with a more capable model architecture, considering that 3 digit multiplication could be learned at a low but not zero (22\%) accuracy.

In summary, we run these 12 experiments with 6 different model changes attempted on two operations and fixed operand digits and dataset size per operation:

\begin{itemize}
	\item \textbf{Addition:} 10 operand digits, 100k training dataset, baseline model but with one of these changes: $\dHidden=192, \dHidden=768, \nBlocks=3, \nBlocks=12, \nHeads=3$ or $\nHeads=12$.
	\item \textbf{Multiplication:} 5 operand digits, 100k training dataset, baseline model but with one of these changes: $\dHidden=192, \dHidden=768, \nBlocks=3, \nBlocks=12, \nHeads=3$ or $\nHeads=12$.
\end{itemize}


\subsection{Sampling strategy experiments}
\label{setup:sampling}

Changing the sampling method / distribution of training samples does not change the length of the training sequences. We can thus run the same 27 baseline experiments (\cref{setup:baseline}) with each of the four sampling methods outlined in \cref{setup:sampling} except "Basic sampling" which was used for the baseline experiments. Thus we run 27*3=81 additional experiments to evaluate sampling strategies:

\begin{itemize}
	\item \textbf{Addition:} 3, 5, or 10 operand digits, 1k, 10k, or 100k training dataset, from-zero, uniform-digits or uniform-bits sampling.
	\item \textbf{Multiplication:} 3, 5, or 10 operand digits, 1k, 10k, or 100k training dataset, from-zero, uniform-digits or uniform-bits sampling.
	\item \textbf{Integer square root:} 6, 10, or 20 operand digits, 1k, 10k, or 100k training dataset, from-zero, uniform-digits or uniform-bits sampling.
\end{itemize}



\subsection{Result reversal experiments}
\label{setup:reversal}

We wanted to find out whether reversing the digits of the result of the arithmetic tasks in the training samples can improve model performance. For this we augmented the training samples with the reversed digits of the result, as described in \cref{methods:reversal}.


This results in increased sample length (\cref{tbl:reverse_sample_lengths}) which means we have to increase the number of positions in the model configuration ($\nPos$) to handle these longer sequences. This does lead to more model parameters due to each position having a learnable positional embedding vector (\cref{gpt2:preproc}). The increase however is just a minute fraction of the total parameters (e.g., 10.710 million parameters for the 10-digit addition baseline requiring 34 positions, and 10.715 million parameters for 10-digit addition with result reversal requiring 47 positions). This minute increase does not necessitate other changes to the training regime such as reduced batch sizes or fewer epochs - we can keep the default batch size of 256 and epoch limit of 200 (\cref{setup:main}).

For the result reversal experiments, to evaluate accuracy as described in \cref{setup:eval} we do not consider the entire generated output of the model (the reversed result in brackets and the final result), but instead strip the reversed part before only comparing the actual result.

\includeTable
{
	task; lenNormal; lenReverse\\
	Addition (3 digits); 13; 19 \\
	Addition (5 digits); 19; 27 \\
	Addition (10 digits); 34; 47 \\
	Multiplication (3 digits); 15; 23 \\
	Multiplication (5 digits); 23; 35 \\
	Multiplication (10 digits); 43; 65 \\
	Square root (6 digits); 11; 16 \\
	Square root (10 digits); 17; 24 \\
	Square root (20 digits); 32; 44 \\
}
{tbl:reverse_sample_lengths}
{
	Sequence lengths for training samples (prompt string + response string + end token) for various tasks and operand sizes.
}
{%
	columns/task/.style={column name={Task}},
	columns/lenNormal/.style={column name={\begin{tabular}{c} Sequence length \\ (baseline) \\ \end{tabular}}},
	columns/lenReverse/.style={column name={\begin{tabular}{c} Sequence length \\ (result reversal) \\ \end{tabular}}}
}



We can thus repeat the panel of 27 baseline experiments at the same settings (\cref{setup:baseline}), with the training samples augmented with result reversal:


\begin{itemize}
	\item \textbf{Addition:} 3, 5, or 10 operand digits, 1k, 10k, or 100k training dataset, using result reversal.
	\item \textbf{Multiplication:} 3, 5, or 10 operand digits, 1k, 10k, or 100k training dataset, using result reversal.
	\item \textbf{Integer square root:} 6, 10, or 20 operand digits, 1k, 10k, or 100k training dataset, using result reversal.
\end{itemize}

\subsection{Scratchpad experiments}
\label{setup:scratchpad}

Since scratchpads (including intermediate steps of the computation in the training sample) did show promise in \cite{teaching} and \cite{goat}, we also wanted to test this for our panel of operations and operand sizes.

Augmenting the training samples with a scratchpad that details the steps of the arithmetic operation to be performed results in a large increase in sequence length (\cref{tbl:scratchpad_sample_lengths}). This increase is especially noticeable for the multiplication operation (which is composed of several single-digit multiplications followed by a large addition) and the square root operation (which is done by binary search with squaring at every step).

\includeTable
{
	task; lenNormal; lenScratch\\
	Addition (3 digits); 13; 32 \\
	Addition (5 digits); 19; 50 \\
	Addition (10 digits); 34; 95 \\
	Multiplication (3 digits); 15; 159 \\
	Multiplication (5 digits); 23; 356 \\
	Multiplication (10 digits); 43; 1181 \\
	Square root (6 digits); 11; 602 \\
	Square root (10 digits); 17; 1447 \\
	Square root (20 digits); 32; 5083 \\
}
{tbl:scratchpad_sample_lengths}
{
	Sequence lengths for training samples (prompt string + response string + end token) for various tasks and operand sizes.
}
{%
	columns/task/.style={column name={Task}},
	columns/lenNormal/.style={column name={\begin{tabular}{c} Sequence length \\ (baseline) \\ \end{tabular}}},
	columns/lenScratch/.style={column name={\begin{tabular}{c} Sequence length \\ (scratchpad) \\ \end{tabular}}}
}

These large sequences of sometimes well beyond 1000 characters necessitate some changes to the training regime. Based on the available hardware, we need to make training batches and the intermediate results during forward/backward pass fit into GPU memory. The smaller batches in turn mean more optimization steps per epoch, thus taking more time to do training for a given number of epochs. At the same time, a training batch with longer sequences results in larger matrices during optimization, thus taking more time per batch than the same batch size with shorter sequences.
Time for evaluating the model (generating answers for the samples in the test/validation dataset to test accuracy) also rises proportionally to the lengths of the sequences being generated, meaning it can become a significant part of training time when sequences are long due to an elaborate scratchpad being trained on.

We want to keep the duration for a single experiment (e.g., square root, using scratchpad, 20 digits, 100k dataset) below 24 hours on the hardware available to us.
We still want to run all experiments, even those with the largest dataset size of 100k, thus we made the compromise of reducing the maximum number of training epochs for the experiments that would take too much time otherwise. This does have the downside of the models being undertrained (unless early stopping would have halted training before the epoch limit) but it seems like the best choice given the time and hardware constraints.


For the addition operation (\cref{tbl:scratchpad_training_add}) the only change that needed to be made was reducing the maximum epochs from 200 to 20 for experiments with the large 100k training dataset.


\includeTable
{
	digits; batch; batchScratch; epochs; epochsScratch\\
	3 digits,  1k;  256; 256; 200; 200\\
	3 digits,  10k;  256; 256; 200; 200\\
	3 digits,  100k;  256; 256; 200; 20\\
	5 digits,  1k;  256; 256; 200; 200\\
	5 digits,  10k;  256; 256; 200; 200\\
	5 digits,  100k;  256; 256; 200; 20\\
	10 digits,  1k;  256; 256; 200; 200\\
	10 digits,  10k;  256; 256; 200; 200\\
	10 digits,  100k;  256; 256; 200; 20\\
}
{tbl:scratchpad_training_add}
{
	Training regime changes for the \textbf{addition} operation.
}
{%
	columns/digits/.style={column name={\begin{tabular}{c}
				Operand digits, \\
				dataset size
	\end{tabular}}},
	columns/dataset/.style={column name={\begin{tabular}{c}
				Dataset \\
				size
	\end{tabular}}},
	columns/batch/.style={column name={\begin{tabular}{c}
				Batch size \\
				(baseline)
	\end{tabular}}},
	columns/batchScratch/.style={column name={\begin{tabular}{c}
				Batch size \\
				(scratchpad)
	\end{tabular}}},
	columns/epochs/.style={column name={\begin{tabular}{c}
				Max. \\ epochs \\
				(baseline)
	\end{tabular}}},
	columns/epochsScratch/.style={column name={\begin{tabular}{c}
				Max. \\ epochs \\
				(scratchpad)
	\end{tabular}}}
}



For the multiplication operation (\cref{tbl:scratchpad_training_mul}) the batch sizes needed to be reduced depending on the sequence length. Also the number of epochs needed to be reduced for the experiments with 10k and 100k datasets.

\includeTable
{
	digits; batch; batchScratch; epochs; epochsScratch\\
	3 digits,  1k;  256; 128; 200; 200\\
	3 digits,  10k;  256; 128; 200; 50\\
	3 digits,  100k;  256; 128; 200; 5\\
	5 digits,  1k;  256; 64; 200; 200\\
	5 digits,  10k;  256; 64; 200; 50\\
	5 digits,  100k;  256; 64; 200; 5\\
	10 digits,  1k;  256; 16; 200; 200\\
	10 digits,  10k;  256; 16; 200; 50\\
	10 digits,  100k;  256; 16; 200; 5\\
}
{tbl:scratchpad_training_mul}
{
	Training regime changes for the \textbf{multiplication} operation.
}
{%
	columns/digits/.style={column name={\begin{tabular}{c}
				Operand digits, \\
				dataset size
	\end{tabular}}},
	columns/dataset/.style={column name={\begin{tabular}{c}
				Dataset \\
				size
	\end{tabular}}},
	columns/batch/.style={column name={\begin{tabular}{c}
				Batch size \\
				(baseline)
	\end{tabular}}},
	columns/batchScratch/.style={column name={\begin{tabular}{c}
				Batch size \\
				(scratchpad)
	\end{tabular}}},
	columns/epochs/.style={column name={\begin{tabular}{c}
				Max. \\ epochs \\
				(baseline)
	\end{tabular}}},
	columns/epochsScratch/.style={column name={\begin{tabular}{c}
				Max. \\ epochs \\
				(scratchpad)
	\end{tabular}}}
}



For the square root operation (\cref{tbl:scratchpad_training_sqrt}), due to time and memory constraints (see \cref{training_time_growth}), the batch sizes and epoch counts needed to be cut even more, even down to a batch size of 1 and epoch count of 1 for the task with 20 operand digits and 100k dataset. Even though this results in much less training than the comparable baseline task, we still think it is better trying to at least get a glimpse of training results for this task instead of not including it at all.

\includeTable
{
	digits; batch; batchScratch; epochs; epochsScratch\\
	6 digits,  1k;  256; 32; 200; 200\\
	6 digits,  10k;  256; 32; 200; 50\\
	6 digits,  100k;  256; 32; 200; 5\\
	10 digits,  1k;  256; 8; 200; 200\\
	10 digits,  10k;  256; 8; 200; 50\\
	10 digits,  100k;  256; 8; 200; 5\\
	20 digits,  1k;  256; 1; 200; 100\\
	20 digits,  10k;  256; 1; 200; 10\\
	20 digits,  100k;  256; 1; 200; 1\\
}
{tbl:scratchpad_training_sqrt}
{
	Training regime changes for the \textbf{square root} operation.
}
{%
	columns/digits/.style={column name={\begin{tabular}{c}
				Operand digits, \\
				dataset size
	\end{tabular}}},
	columns/dataset/.style={column name={\begin{tabular}{c}
				Dataset \\
				size
	\end{tabular}}},
	columns/batch/.style={column name={\begin{tabular}{c}
				Batch size \\
				(baseline)
	\end{tabular}}},
	columns/batchScratch/.style={column name={\begin{tabular}{c}
				Batch size \\
				(scratchpad)
	\end{tabular}}},
	columns/epochs/.style={column name={\begin{tabular}{c}
				Max. \\ epochs \\
				(baseline)
	\end{tabular}}},
	columns/epochsScratch/.style={column name={\begin{tabular}{c}
				Max. \\ epochs \\
				(scratchpad)
	\end{tabular}}}
}


In addition to the experiment-specific changes mentioned above, we also reduced the size of the validation dataset from 1000 to 10 elements and perform accuracy evaluations not once every 100 steps during logging but only at the end of each epoch. This gives a less detailed report of accuracy development during training but saves significant evaluation time. Accuracy on the test dataset, which is unchanged at 1000 elements, is performed only at the end of the experiment, same as for the baseline, thus final accuracy numbers in the results table do not suffer in precision.

Similar to the result reversal experiments,  we do not consider the entire generated output of the model (the scratchpad and the final result) for evaluating accuracy, but instead strip the scratchpad before only comparing the result.

We thus run the following experiments to test model performance when using scratchpads:

\begin{itemize}
	\item \textbf{Addition:} 3, 5, or 10 operand digits, 1k, 10k, or 100k training dataset, using scratchpad, with modifications to training regime described in \cref{tbl:scratchpad_training_add}.
	\item \textbf{Multiplication:} 3, 5, or 10 operand digits, 1k, 10k, or 100k training dataset, using scratchpad, with modifications to training regime described in \cref{tbl:scratchpad_training_mul}.
	\item \textbf{Integer square root:} 6, 10, or 20 operand digits, 1k, 10k, or 100k training dataset, using scratchpad, with modifications to training regime described in \cref{tbl:scratchpad_training_sqrt}.
\end{itemize}


\newpage
\subsection{Attention heatmaps}
\label{setup:heatmap}

We generate diagrams illustrating attention activation between positions (attention heatmap diagrams) based on the methods outlined in \cref{methods:heatmap}. All diagrams consider only the first GPT-2 block and are averaged across all attention heads, except for the baseline "123+456" example, where a detailed diagram across all blocks and attention heads is also provided. Diagrams were generated for the following models handling the following example inputs:

\begin{itemize}
	\item \textbf{Addition:} 
	\begin{itemize}
		\item Baseline model trained on 3 digits, 100k training dataset on these examples: "123+456" (both detailed heatmap and averaged heatmap) and "999+001"
		\item Model trained using result reversal on 3 digits, 100k training dataset on this example (this includes the model generating the result reversal part): "123+456" 
		\item Model trained using scratchpad on 3 digits, 100k training dataset on this example (this includes the model generating the scratchpad part): "123+456"
		\item Baseline model trained on 5 digits, 100k training dataset on this example: "12345+67890"
		
	\end{itemize}
	
	\item \textbf{Multiplication:}  Baseline model trained on 3 digits, 100k training dataset on this example: "123*456"
	\item \textbf{Integer square root:}  Baseline model trained on 6 digits, 100k training dataset on this example: "123456:"
\end{itemize}