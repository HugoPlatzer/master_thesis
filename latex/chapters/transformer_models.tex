\section{Transformer models}

Machine translation, which is the automated translation from one natural language to another, was an important challenge since the beginning for the field of artificial intelligence and machine learning.
The input and output texts are usually modeled as sequences of integers from a limited range, with each integer representing a phrase, word, or part of a word.
Classical dense/feed-forward neural networks are a poor fit for such tasks, as the sequence lengths of even a short text combined with the many possible choices for each position make for a dense network with far too many parameters.

Thus researchers started using approaches like recurrent neural networks (RNN) \cite{phrasereps}, convolutional neural networks (CNN) \cite{convseq} and long short-term memory (LSTM) \cite{seq2seq}.
While these approaches did make the parameter count more manageable due to their sparse architecture, they still had issues with training on long sequences and learning long-distance correlations within a sequence.

Transformer models were introduced in the influential paper "Attention as all you need" \cite{allyouneed} in the context of machine translation to help address the aforementioned issues. Later on, they proved useful for many other tasks. 

Due to their parallelized architecture which will be explained in detail later, transformers allow for training on long sequences in a single optimization step, greatly increasing training throughput \citepage{1}{allyouneed}. They also have full connection between any two points in the sequence in each layer of the network, mitigating the problem of long-range dependencies being only considered across multiple model steps/layers in CNNs or RNNs which makes information loss more likely \citepage{6}{allyouneed}.

\subsection{Types of transformer models}

There are a variety of different albeit similar structures that have been used to build effective transformer networks. In particular, decoder, encoder-decoder, and encoder models need to be differentiated.

Encoder models such as BERT \cite{devlin2019bert} transform any given text into a vector of fixed size. This vector in the ideal case captures the essence of the text's meaning and can thus be used for tasks like text classification or sentiment analysis.

Encoder-decoder models like the original transformer architecture as described in \cite{allyouneed} use the vector generated by the encoder to start generating new text. This can be used for example by translation models or for answering questions.

Decoder models like the GPT models \cite{improvinglu} generate new text to extend some given starting text. This can be used for example to write a story when given a first paragraph. But with the right training, they can be used for the same tasks as encoder and encoder-decoder models as can be seen in the next section.

\subsection{Transformer decoder models}

The purpose of a decoder transformer model is to have it complete a sequence of tokens. These tokens may represent a letter of some alphabet, fragments of words, entire words, keywords of a programming language, even decimal digits, byte values or the binary values 0 and 1.
They could even represent color values or grid coordinates. Natural language modeling, however, is the most common scenario.

The model can be trained on token sequences generated from natural language taken from books, news articles, online communication platforms or other sources. This would make the model useful for writing stories, filling gaps in text and so on.
Or it could be trained on pieces of source code, which would allow the model to act as a code completion assistant.
The model would be given a piece of text that starts a story the user wants to have completed, or a piece of code that only has a function defined with comments. A well-trained model would then complete the code or story in a useful way so the rest of the story matches the theme set in the starting paragraph, or the code generated fits the comments and function signature written before.

The training texts could also all be in a structured question-answer pair form, allowing the model to answer questions. A few examples:
\label{transformer:examples}

\begin{itemize}
\item Translation tasks: "en: The house is big.  de: Das Haus ist gro√ü."
\item Classification tasks: "text:  what a cool product, would buy again  sentiment: happy"
\item Mathematical tasks: "21*2=42" or "123: odd number" / "124: even number" \citepage{3}{nocalculator}
\end{itemize}

When the models is well-trained on these question-answer pair texts, it can be asked a question by simply leaving out the answer part at the end of the text, like "21*3=", as demonstrated in \cite{alammar-transformer}. A good model would be able to generate the correct answer for similar questions by inference from the many examples it has seen, even when the exact question posed is not in its training data.

\subsection{Beyond text}

Apart from text-based tasks, transformer models are also capable in domains like image classification.
A 2D image can be flattened into a sequence of tokens / vectors which can then be processed in a normal transformer model. The resulting image classifier achieves similar results to state-of-the-art CNN-based classifiers \citepage{3,5}{worth16words}.

Transformers can even be successful at generating images from text prompts. The approach presented in \cite{cogview} involves using a regular tokenizer for the text prompts and a separate discrete autoencoder to transform the training images into tokens and the generated tokens back to images.

Board games such as chess and go can also easily be tackled by transformers, as the board state and moves made can be transformed into text or represented by specialized tokenization schemes.
In \cite{grandmasterlevelchess} a transformer decoder model is trained to generate best moves or score board positions based on a large dataset of human chess games. The resulting chess playing engine manages grandmaster-level performance in blitz chess against human opponents \citepage{5}{grandmasterlevelchess}.

There have been minor modifications to the model architecture, data representation, and training regime among the diverse tasks outlined above. Also the performance of the resulting models is generally good but not quite state-of-the art. Nonetheless, having one model architecture that can achieve strong performance across a diverse set of domains seems promising as a step towards artificial general intelligence.


\subsection{Tokenization}
\label{transformer:tokenization}

Most transformer models preprocess input text using a so-called tokenizer, which is a model that was trained to group text into meaningful chunks out of a limited vocabulary called tokens to provide for an efficient representation of language \cite{subwordunits}. The tokenizer transforms the text into a sequence of integers, which is then processed by the transformer model which outputs token IDs which are then converted back into text.

Compared to just using raw bytes as model inputs, a good tokenizer allows for shorter sequence lengths (as a token typically represents multiple characters/bytes), meaning faster generation and greater maximum length \cite{tokenizerchoice}. Nonetheless, there are capable models such as Google ByT5 \cite{xue2022byt5} that do not need a tokenizer as they operate directly on byte values.
Compared to manually specifying a vocabulary (for example contents of a dictionary) tokenization is more flexible, having no issues with unknown words or large composite words (any possible byte sequence can still be represented) and is adaptable to the text corpus the model should be used on.

To train a tokenizer, the byte-pair-encoding (BPE) algorithm \cite{subwordunits} is often used, which also has applications in data compression. It works as follows:

\begin{enumerate}
	\item At the start, the set of token strings is just the possible characters in the input text, e.g., all ASCII characters. At this point, each character needs one token (string) in the representation of the tokenizer.
	\item The input text is modeled as a sequence of token strings from the token strings set, preferring longer strings wherever possible over shorter ones. In this sequence, the most common adjacent pair of two token strings is identified.
	\item A new token string is created from the concatenation of this pair, e.g., if "A" followed by "B" is the most common token string pair in the text, a new token string "AB" is added to the set.
	\item Steps 2 and 3 are repeated over and over. This causes the token vocabulary to grow and the average number of tokens needed per character to decrease. This is done until a good balance between token vocabulary size and tokens per character is achieved.
\end{enumerate}

Apart from the regular token vocabulary, most tokenizers also contain some special token values that will not be found in the input text. These are used for example to represent the start and end of each training sample, delimit individual pieces of text when they are fed to the model, or separate parts of a question/answer pair \cite{HuggingFaceTokenizers}.


\subsection{Input / output}
\label{transformer:input}

Formally, the input to the model consists of a sequence of integers (token IDs) $\modIn_1 \ldots \modIn_n$, where $n$ is the length of the token sequence.
Generally, transformer models place a limit on the length of the sequence $n$ due to their internal structure, we denote this limit as $\nPos$. However, models with no such limitation have also been proposed \cite{su2023roformer}.
Each integer $\modIn_i$ references one of the possible tokens of the model $(1 \leq \modIn_i \leq \dVocab)$. $\dVocab$ is the size of the token vocabulary, which depends on the tokenization process used for the model \citepage{2}{allyouneed}.

\label{transformer:output}

For each position in the input sequence, the model outputs a probability distribution, predicting the next token based on the input tokens up to this position. 
The output of the model is a sequence of probability distributions $\modOut_i$ $(1 \leq i \leq n)$. $\modOut_i \in \mathbb{R}^\dVocab$, $0 \leq (\modOut_i)_j \leq 1$ \citepage{5}{allyouneed} \cite{HuggingFaceGPT2}.
$\modOut_i$ should model which token most likely follows after all the tokens $\modIn_1 \ldots \modIn_n$.

During training, all output positions are useful as they allow to optimize the model to correctly predict $\modIn_2$ based on $\modIn_1$,
$\modIn_3$ based on $\modIn_1$ and $\modIn_2$, $\modIn_n$ based on $\modIn_1 \ldots \modIn_{n-1}$, all in a single training step. This greatly reduces training time for longer sequences \cite{alammar-gpt2}.

\subsection{Text generation}
\label{transformer:textgen}

When using the trained model to generate text, only the last output distribution, $\modOut_n$, is of interest, because
it tells the next token after the given input sequence. \cite{Mao2021Autoregressive}

This is used to generate one new token at a time (autoregressive decoding). The new token is picked from the distribution either by choosing the most likely token (greedy decoding), random sampling according to the probability distribution, or more sophisticated methods such as top-p sampling (choosing among the most likely tokens whose probability mass equals for example $p=0.95$) and top-k sampling (selecting from the for example $k=10$ most likely tokens) \cite{ippolito2019comparison}.

The newly generated token $\modIn_{n+1}$ is appended to the input sequence and the whole process is repeated, with $n$ growing by one after each step, until a special end-of-sequence token is generated, the maximum length $\nPos$ is reached, or some other stop condition is triggered \cite[sect. "Generation"]{HuggingFaceGPT2}. If generation is not terminated, the model could in theory generate a text of infinite length, although transformer models generally only have a limited window of past tokens they can consider when generating the next token. In addition, generation time and memory requirements generally increase quadratically with sequence length for transformer models (\cref{gpt2:attn}).

\subsection{User interaction}

In 2023, OpenAI released their most successful product yet, ChatGPT \cite{openai_chatgpt_2022}. This online service allows users to chat with an artificial intelligence assistant which can help with reciting facts, writing texts and code, giving advice of all sorts and so on.

The principle by which transformer decoder models can act as chat assistants is simple: The model is trained on conversations like this:

\begin{verbatim}
    User: What color is the sky?
    Assistant: The sky is blue.
\end{verbatim}

The model can then become the helpful assistant by filling in the assistant's messages:

\begin{verbatim}
    User: What color is a banana?
    Assistant:
\end{verbatim}

and the model would generate a response like "A banana is yellow" followed by an end-of-sequence token, similar to its training examples. For more details on how generic language models can be tuned to follow instructions or conversation, see \cite{yi2019coherent} and \cite{ouyang2022training}.

\subsection{Generative pre-training}

The GPT2/GPT3/ChatGPT transformer models have demonstrated the effectiveness of a technique used to train language models called generative pre-training (GPT). This technique was first popularized in \cite{improvinglu} and consists of two phases.

In the pre-training stage the model is first trained on a large corpus of text in multiple languages, including websites, books, articles, chats and source code. In this manner, a huge amount of training data can be collected without the need for careful manual selection / preparation of question-answer pairs. The training data is split into appropriate chunks and the model learns to predict (generate) the next word based on the previous ones for each chunk as a form of unsupervised learning \citepage{3}{improvinglu}.

After sufficient pre-training, the model is then fine-tuned on question-answer pairs of the specific task the model should be able to solve, like the examples presented in \cref{transformer:examples} or even human-assistant chats on a wide range of topics.

Due to the well-known phenomenon of transfer learning \cite{transferlearning}, the pre-trained model will have already internalized the structure of natural language, source code and math. 
This allows a relatively small amount of hand picked fine-tuning examples to make it significantly stronger compared to just training on the question-answer pairs on a freshly initialized model \citepage{8}{improvinglu}.

\subsection{Attention mechanism}
\label{transformer:attn}

Before transformers, model architectures such as recurrent neural networks (RNN) and Long Short Term Memory (LSTM) were commonly used to handle textual or other sequential data. They ingest the tokenized text one token at a time, keeping one or multiple fixed vectors of model state. Transformers, in contrast, dropped recurrence and handle the entire sequence in parallel \citepage{1}{allyouneed}.

To draw connections between elements of the token sequence, they use a so-called attention mechanism. It is in fact the only part of the model that connects the different parts of the sequence it is processing, all other components of a transformer model handle each position independently. The concept of an attention mechanism is best explained using a high-level example \cite{alammar-gpt2}: Suppose the model is processing the sentence "A robot must obey orders given to it" and each word is one token. The model should learn that there is a connection from the pronoun "it" to the earlier noun "robot".
The model's attention mechanism would do the following when handling the token "it":
\begin{enumerate}
	\item Generate a query vector $\mathbf{q}_{\mathrm{it}} = f_q("\mathrm{it}")$ for the token "it" using a function $f_q$ that depends on learned parameters of the model (often a linear transformation).
	\item Generate key vectors $\mathbf{k}_t$ for all the tokens $t$ of the sentence, using another learned function $f_k$: $\mathbf{k}_{\mathrm{a}} = f_k(\mathrm{"a"})$, $\mathbf{k}_{\mathrm{robot}} = f_k(\mathrm{"robot"})$, \ldots, $\mathbf{k}_{\mathrm{it}} = f_k(\mathrm{"it")}$.
	\item Compute numeric similarity scores $s$ between the query vector and each key vector, using a measure like dot product: $s_{\mathrm{a}} = \mathbf{\mathbf{q}}_{\mathrm{it}} \cdot \mathbf{k}_{\mathrm{a}}$, $s_{\mathrm{robot}} = \mathbf{q}_{\mathrm{it}} \cdot \mathbf{k}_{\mathrm{robot}}$, \ldots, $s_{\mathrm{it}} = \mathbf{q}_{\mathrm{it}} \cdot \mathbf{k}_{\mathrm{it}}$.
	\item Normalize the similarity scores $s$ using softmax to get the normalized scores $s'_t$ for each token $t$. This ensures that $\sum_t s'_t = 1$.
	\item Generate value vectors $\mathbf{v}_t$ for all the tokens $t$ of the sentence, using another learned function $f_v$: $\mathbf{v}_{\mathrm{a}} = f_v(\mathrm{"a"})$, $\mathbf{v}_{\mathrm{robot}} = f_v(\mathrm{"robot"})$, \ldots, $\mathbf{v}_{\mathrm{it}} = f_v(\mathrm{"it")}$.
	\item Using the normalized similarity scores, blend the value vectors together to get the attention output vector for "it": $\mathbf{a}_{\mathrm{it}} = \sum_t s'_t \mathbf{v}_t$. 
\end{enumerate}

Assuming the model has learned to generate meaningful query and key vectors, the similarity score to the query of "it" should be much higher for the key of "robot" than for the other positions' keys. This would cause the attention output vector of "it" to mostly consist of the value vector of the token "robot", with the other positions' value vectors having little impact. Thus data flows to the position of the token "it" mostly from the position of the token "robot", which models the linguistic connection between these words.

It has to be said however that this explanation has been simplified considerably. Transformer models do not operate on words or tokens directly but first transform them into vectors of fixed size that are then fed through the attention mechanism. There are multiple stacked blocks inside a typical transformer model, each consisting of an attention mechanism among other components. A more precise description of the attention mechanism of a realistic transformer model is given in \cref{gpt2:attn}.

