\section{Summary}

In this work, we investigated the performance of small transformer models on the arithmetic operations of addition, multiplication and integer square root.
To establish a baseline, we trained transformer networks with GPT-2 architecture and about 10 million parameters on each combination of these three operations and various operand lengths and training dataset sizes per operation.

We then wanted to improve performance, in particular for the multiplication and integer square root tasks and thus attempted a number of changes to the model and dataset.
First, we explored whether changing the model size (number of layers, hidden vector sizes etc.) improved model performance (\cref{results:modelsize}).
After that, we tried changing the sampling function for the training dataset to see whether prioritizing certain training samples improved performance (\cref{results:sampling}).
Then, we went on to change the actual training samples themselves. For this, we tried two different strategies:
The first strategy was reversing the digits of the result for each training sample (\cref{results:reversal}).
The second strategy was augmenting each training sample with a scratchpad of intermediate steps of the arithmetic computation of that sample (\cref{results:scratchpad}).
Finally, we created heatmaps of model attention to get some insight into how the model processes input strings (\cref{results:heatmap}).

\subsection{Results}

The best results we could achieve for the arithmetic functions we studied were 100\% accuracy for 3-, 5- and 10-digit addition, 94\% for 3-digit multiplication, 63\% for 5-digit multiplication, 22\% for 10-digit multiplication, 98\% for 6-digit integer square root, 64\% for 10-digit integer square root and 0\% for 20-digit integer square root.
For addition and multiplication, these results were obtained when using scratchpads, for integer square root, they are from the baseline model. The largest training dataset size (100k training samples) was used.

On the one hand, these results could likely be made even better by longer training runs when using scratchpads (we constrained epochs to keep training time manageable with the long scratchpad strings).
On the other hand, using scratchpads also requires knowledge of meaningful intermediate steps - how to compute the output from the input step by step. A key advantage of machine learning over traditional algorithms is that this knowledge of how to compute the result is generally not required: The model is provided only a sequence of input and output pairs with the optimizer handling how to make the model get from input to output. This advantage is lost when having to devise a step-by-step computation for the training samples.

Other strategies we tried for improving performance such as changing the dimensions of the model, dataset sampling strategy, or reversing the digits of the results generally brought only improvements to convergence speed (number of optimization steps until the model learned the task) or improved model accuracy from an already high 98-99\% to 100\% (no detectable errors). These strategies did not give the desired "quantum leap" of making multiplication or square root with large operands learnable.

\subsection{Future work}

An obvious avenue for building on our results would be running training on the scratchpad experiments for longer, until an actual plateau is reached. Training using scratchpad was limited by the resources and budget. Training for more epochs would likely allow to see the actual limitations of scratchpad-aided multiplication on our model better, as we have reason to believe our models were undertrained (\cref{results:scratchpad}).
It is also possible that the stopping condition we used contributes to premature termination of training; analyzing accuracy development curves suggests this might be a possibility. Focusing not only on loss but also accuracy might ensure training does not stop before the model reaches maximum accuracy.

We did not analyze variances between multiple training runs as even a single training run took substantial resources. It would however be interesting to see if a task that normally fails to converge occasionally does converge or vice versa.
It would be worth combining result reversal with uniform-digit sampling to try and replicate the good results for multiplication in \cite{positionmatters}.
It would also be interesting to see whether a second model could learn to predict where the main model made a mistake and try to correct it, especially for long output sequences when using scratchpads.

The good results of basic feed-forward networks in \cite{visual} would make it promising to explore their limits, especially with regards to longer scratchpad sequences. It remains to determine at which point transformers start having an advantage over more basic and established models. Other architectures like one-dimensional CNNs or recurrent networks could be compared too.

Using a simpler, more straightforward scratchpad for multiplication as in \cite{implicit} would reduce sequence length and thus accelerate training. It is worthwhile to compare performance between such a bare-bones scratchpad and the more elaborate one we are using.
Furthermore, a better scratchpad for integer square root should be devised. It must be reasonably short, yet not leave big chunks of computation without intermediate steps, which we identified as a potential problem of our square root scratchpad.

Finally, the inner mechanisms of the model could be explored further. The heatmap diagrams in \cref{results:heatmap} show clear, logical relationships between input and output in the first layer already - but it does not explain the whole "algorithm" learned by the model. Analysis would have to focus on the more opaque patterns in later layers of the model.