\section{Experimental results}

In this section we present the result of running each of the experiments outlined in \cref{setup:baseline} to \cref{setup:heatmap}. Results are presented in the form of colored accuracy tables outlining final model accuracy on the test dataset per experiment.
For some experiments, we also plot curves illustrating the development of accuracy during model training.
We also sample errors of the trained models for some experiments and compare them to expected results side by side.
Finally, this section contains diagrams illustrating attention patterns in the trained models in \cref{results:heatmap}.

\subsection{Baseline experiments}
\label{results:baseline}

For the addition operation, we can see in \cref{tbl:baseline_add} that addition up to 10 digits can be learned, as long as the training dataset has at least 10k samples. Accuracy numbers are high, but short of 100\% for 5 and 10 digit addition, meaning there are quite a few errors left in the validation dataset. Only 3 digit addition achieved perfect accuracy.
Considering the development of accuracy during training, we can see in \cref{fig:baseline_add} that accuracy flatlines until a few hundred to a few thousand optimization steps are performed. More operand digits mean more optimization steps to reach the point of accuracy leaving zero. Interestingly, larger dataset sizes also require slightly more optimization steps to leave zero accuracy.
After leaving zero, accuracy quickly jumps to about 80\%, then slowly keeps climbing to close to 100\%.
Based on these accuracy curves, it does not seem unreasonable that training for longer that early stopping permits (\cref{setup:early_stopping}) would have resulted in even better accuracy. Investigating the type and parameters of our early stopping criterion could be worthwhile.


For the multiplication task, we can see, in \cref{tbl:baseline_mul}, a very different picture to the addition task. Accuracy never gets above zero, except for the 3 digits, 10k dataset experiment, where it reaches about 1\%, and the 3 digits, 100k dataset experiment, where it reaches about 22\%.
This suggests that learning multiplication, all else being equal, is harder than learning addition.
When looking at the accuracy curves in  \cref{fig:baseline_mul}, we can see that for the only experiment where accuracy clearly leaves zero (3 digit multiplication, 100k dataset) accuracy does not jump quickly to a certain value as it did with addition, but slowly keeps climbing until early stopping terminates training.
For the experiments that do not achieve any accuracy, we can see that the number of optimization steps performed until training is aborted depends mostly on training dataset size and not much on operand digits, with the experiments with 1k dataset getting aborted after about 200 steps, and the experiments with 100k dataset getting aborted after about 40000 steps.


For the square root task, we can see, in \cref{tbl:baseline_sqrt}, accuracy numbers that lie somewhere between those for addition and multiplication.
For 6 digit integer square root, there was high but not perfect accuracy when using a 10k or 100k training dataset.
For 10 digit integer square root, accuracy was 63\% for the 100k dataset but remained low otherwise. For 20 digit integer square root, no samples in the validation dataset where predicted correctly.
When looking at the accuracy curves in \cref{fig:baseline_sqrt}, we can see curves that are more similar to those of the multiplication experiment than the addition experiments, with accuracy slowly climbing from zero. Once again, it looks like early stopping might prevent the model from achieving their full accuracy potential.



\includeAccuracyTable{experiment_results/baseline_final_accuracy/add.csv}{tbl:baseline_add}{Final model accuracy on the test dataset for the addition operation for the given training dataset sizes and operand digits.}{}

\includeAccuracyTable{experiment_results/baseline_final_accuracy/mul.csv}{tbl:baseline_mul}{Final model accuracy on the test dataset for the multiplication operation for the given training dataset sizes and operand digits.}{}

\includeAccuracyTable{experiment_results/baseline_final_accuracy/sqrt.csv}{tbl:baseline_sqrt}{Final model accuracy on the test dataset for the square root operation for the given training dataset sizes and operand digits.}{}


\includePDFPlotSmall{experiment_results/baseline_accuracy/add.pdf}{fig:baseline_add}{Development of accuracy on the validation dataset when training for the addition task for different operand digits and dataset sizes. Square markers represent the point training was terminated by early stopping.}

\includePDFPlotSmall{experiment_results/baseline_accuracy/mul.pdf}{fig:baseline_mul}{Development of accuracy on the validation dataset when training for the multiplication task for different operand digits and dataset sizes. Square markers represent the point training was terminated by early stopping.}

\includePDFPlotSmall{experiment_results/baseline_accuracy/sqrt.pdf}{fig:baseline_sqrt}{Development of accuracy on the validation dataset when training for the square root task for different operand digits and dataset sizes. Square markers represent the point training was terminated by early stopping.}



\subsubsection{Error analysis}
\label{error_analysis}

To get a better understanding of the presented accuracy numbers, we took a closer look at samples where the answer of the model differed from the correct one. This was done for tasks where the final model showed perfect or near-perfect accuracy:

\begin{itemize}
	\item Addition (5 digits, 100k training samples)
	\item Addition (10 digits, 100k training samples)
	\item Square root (6 digits, 100k training samples)
\end{itemize}

We also looked at tasks where the final model showed poor performance or did not even manage to go above zero accuracy:

\begin{itemize}
	\item Multiplication (3 digits, 100k training samples)
	\item Multiplication (5 digits, 100k training samples)
	\item Square root (20 digits, 100k training samples)
\end{itemize}

For each of these tasks, a separate error analysis dataset of 100k samples was created and the responses of the best model that was saved during training evaluated on it. A selection of the respective errors per task is analyzed below.








For 5-digit addition, the model achieved near-perfect accuracy (\cref{tbl:baseline_add}).
It can be seen that the errors the model makes for this task (\cref{tbl:errors_add_5digits}) are single digits of the result being off by one.


% Add errors

\includeTable
{
	task;model;correct\\
	91167+53933;145000;145100\\
	20518+99486;110004;120004\\
	68540+11493;070033;080033\\
	96725+23235;129960;119960\\
	58653+91352;140005;150005\\
	88018+94181;182299;182199\\
	73472+66524;149996;139996\\
	56590+43405;109995;099995\\
	68517+85882;154499;154399\\
	72822+97194;160016;170016\\
}
{tbl:errors_add_5digits}
{
	Model errors (addition task, 5 digits, 100k training dataset size).
}
{
	columns/task/.style={column name={Task}},
	columns/model/.style={column name={Model result}},
	columns/correct/.style={column name={Correct result}}
}




A similar pattern emerges for 10-digit addition, which also achieved near-perfect accuracy (\cref{tbl:baseline_add}). The failed tasks in \cref{tbl:errors_add_10digits} show a single digit, typically in the middle, being off by one. 


\includeTable
{
	task;model;correct\\
	1470939445+3694860228;05165899673;05165799673\\
	9438411468+9410288290;18848799758;18848699758\\
	1600337712+5064162365;06664400077;06664500077\\
	8115628227+5759761714;13875399941;13875389941\\
	2578662692+8570333548;11149996240;11148996240\\
	8757570742+5604529187;14362199929;14362099929\\
	4847761430+3799620568;08647382998;08647381998\\
	7560389844+5150090143;12710489987;12710479987\\
	4277581356+7505693643;11783275999;11783274999\\
	7691931213+8322868827;16014700040;16014800040\\
}
{tbl:errors_add_10digits}
{
	Model errors (addition task, 10 digits, 100k training dataset size).
}
{
	columns/task/.style={column name={Task}},
	columns/model/.style={column name={Model result}},
	columns/correct/.style={column name={Correct result}}
}



% Mul errors

For 3-digit multiplication, the model achieved an accuracy of about 20\% (\cref{tbl:baseline_mul}). The errors are affecting only single digits, however some of the digits are off by more than one (\cref{tbl:errors_mul_3digits}).


\includeTable
{
	task;model;correct\\
	896*551;493096;493696\\
	219*107;023933;023433\\
	192*926;177992;177792\\
	696*135;093060;093960\\
	787*809;636083;636683\\
	884*463;409392;409292\\
	131*907;118617;118817\\
	116*454;052464;052664\\
	763*738;563194;563094\\
	595*730;435350;434350\\
}
{tbl:errors_mul_3digits}
{
	Model errors (multiplication task, 3 digits, 100k training dataset size).
}
{
	columns/task/.style={column name={Task}},
	columns/model/.style={column name={Model result}},
	columns/correct/.style={column name={Correct result}}
}




For 5-digit multiplication, for which the model did not achieve above-zero accuracy (\cref{tbl:baseline_mul}), the first and last digits were still correct for failed tasks, but multiple digits in the middle of the product were off (\cref{tbl:errors_mul_5digits}).

\includeTable
{
	task;model;correct\\
	67767*25315;1717767705;1715521605\\
	10981*21880;0231011280;0240264280\\
	86313*14577;1251111101;1258184601\\
	98023*21001;2056666623;2058581023\\
	22900*56482;1295185800;1293437800\\
	41039*12284;0501111176;0504123076\\
	14038*12076;0164444688;0169522888\\
	55362*94976;5251717112;5258061312\\
	91694*73471;6731711774;6736849874\\
	90716*70758;6417716608;6418882728\\
}
{tbl:errors_mul_5digits}
{
	Model errors (multiplication task, 5 digits, 100k training dataset size).
}
{
	columns/task/.style={column name={Task}},
	columns/model/.style={column name={Model result}},
	columns/correct/.style={column name={Correct result}}
}



% Sqrt errors

For 6-digit integer square root, the model achieved about 99\% accuracy (\cref{tbl:baseline_sqrt}). The observed errors show the model answer being off by one from the correct answer (\cref{tbl:errors_sqrt_6digits}). Also, the real square root of the failed tasks was very close to an integer for 9 out of 10 failed tasks, which means the task was close to a square number, which are points from which on the integer square root increases by one. It is reasonable to assume that for such a "close call" it is harder to find the exact integer square root.

\includeTable
{
	task;model;correct;sqrt\\
	178980;422;423;423.06028\\
	458306;677;676;676.98301\\
	213411;462;461;461.96428\\
	139840;374;373;373.95187\\
	101789;318;319;319.04388\\
	128218;357;358;358.07541\\
	911019;953;954;954.47315\\
	451595;671;672;672.00818\\
	777861;882;881;881.96428\\
	254001;504;503;503.98512\\
}
{tbl:errors_sqrt_6digits}
{
	Model errors (square root task, 6 digits, 100k training dataset size).
}
{
	columns/task/.style={column name={Task}},
	columns/model/.style={column name={Model result}},
	columns/correct/.style={column name={Correct result}},
	columns/sqrt/.style={column name={$\sqrt{Task}$}}
}

For 20-digit integer square root, the model did not achieve above-zero accuracy (\cref{tbl:baseline_sqrt}). Analyzing some of the failed tasks shows the model predictions only have the leftmost digits correct (\cref{tbl:errors_sqrt_20digits}), with about 4 out of 10 digits of the result being correct. Also, these tasks are not particularly close to a square number, which stands in contrast to the failed tasks for 6 operand digits.


\includeTable
{
	task;model;correct;sqrt\\
	79694857207412785081;8927155525;8927197612;8927197612.20803\\
	10138193304849869414;3183999089;3184052968;3184052968.28584\\
	20740203138915481021;4554099999;4554141317;4554141317.40721\\
	22785206268196001170;4773433272;4773385200;4773385200.06462\\
	61032565798178795501;7812323248;7812334209;7812334209.32430\\
	10321490749937914057;3212499999;3212707697;3212707697.55636\\
	24550288483602635175;4954848488;4954824768;4954824768.20348\\
	95746296144872950099;9785048434;9785003635;9785003635.40418\\
	76700026808202922573;8757999083;8757855148;8757855148.84797\\
	30130695889482657119;5489032224;5489143456;5489143456.81388\\
}
{tbl:errors_sqrt_20digits}
{
	Model errors (square root task, 20 digits, 100k training dataset size).
}
{
	columns/task/.style={column name={Task}},
	columns/model/.style={column name={Model result}},
	columns/correct/.style={column name={Correct result}},
	columns/sqrt/.style={column name={$\sqrt{Task}$}},
}


To conclude, we can see that the errors our model makes follow clear patterns: If the model is trained successfully (achieves good accuracy) the wrong answer is similar to the correct answer, with either a single digit being wrong in the case of addition and multiplication, or the square root being off by one in the case of integer square root. If the model is trained unsuccessfully (zero accuracy achieved) errors often affect more digits (as in the case of 5-digit multiplication).


\subsection{Model size experiments}
\label{results:modelsize}



For the addition task, we can see in \cref{tbl:model_size} that all of the tried model configurations were able to learn the task successfully, although there are small differences in final accuracy, such as the $n_{embd}=768$ model finishing with perfect accuracy on the test dataset.

When looking at the development of accuracy in \cref{fig:model_size_add}, we can see that there are substantial differences in how quickly the model converges on the addition task when changing the hidden vector size $n_{embd}$. For $n_{embd}=192$, it took about 2500 optimization steps for the model to cross 50\% accuracy, for $n_{embd}=384$ (baseline) it took about 900 steps and for $n_{embd}=768$ only about 500 steps. So in this case the bigger model with more parameters is able to learn faster than smaller models with fewer parameters.

Increasing or decreasing the number of GPT-2 blocks/layers however only made a minor change in convergence speed, with 12 layers performing actually a bit worse than the baseline 6 layers.

Changing the number of attention heads, which does not impact the number of model parameters but only the "partitioning" of data inside the model, also had only a minor impact on convergence speed.

For the multiplication task with 5 operand digits, we can see in \cref{tbl:model_size} and \cref{fig:model_size_mul} that none of the model size changes allowed the model to learn this harder task.


\includeAccuracyTable
{experiment_results/model_size_final_accuracy/model_size.csv}
{tbl:model_size}
{Final model accuracy on the test dataset for the given operation and change in model configuration.}
{%
	columns/name/.append style={
		column name={},
		string replace={baseline}{baseline},
		string replace={n_embd=192}{$n_{embd}=192$},
		string replace={n_embd=768}{$n_{embd}=768$},
		string replace={n_head=3}{$n_{head}=3$},
		string replace={n_head=12}{$n_{head}=12$},
		string replace={n_layer=3}{$n_{layer}=3$},
		string replace={n_layer=12}{$n_{layer}=12$},
	},
	columns/add_10digits_100k/.style={column name={\makecell[r]{add\\(10 digits, 100k)}}},
	columns/mul_5digits_100k/.style={column name={\makecell[r]{mul\\(5 digits, 100k)}}},
}

\includePDFPlotSmall{experiment_results/model_size_accuracy/add.pdf}{fig:model_size_add}{Development of accuracy on the validation dataset when training for the addition task (10 digits, 100k dataset) for different changes in model configuration.}

\includePDFPlotSmall{experiment_results/model_size_accuracy/mul.pdf}{fig:model_size_mul}{Development of accuracy on the validation dataset when training for the multiplication task (5 digits, 100k dataset) for different changes in model configuration.}




\subsection{Sampling strategy experiments}
\label{results:sampling}


For the addition task there were no dramatic differences in achieved model capability for the studied sampling methods  (\cref{tbl:sampling_strategies_add_basic} to \cref{tbl:sampling_strategies_add_uniformbits}). With a 1k training dataset size, none of the methods achieved high accuracy for any operand size, for 10k it was a mix with high accuracy for 3 operand digits and lower accuracy for 10 operand  digits. 100k dataset size allowed for good accuracy for all operand sizes.
There were, however, some less significant but still noticeable differences when comparing to the other sampling methods: Uniform digits and uniform bits sampling performed very similar to one another. Both achieved about 40\% accuracy for 3 digits, 1k dataset size while basic sampling and from-zero sampling remained close to zero. Here, the mix of easier (smaller, often single-digit) and harder (larger) training samples might have allowed for quicker convergence as we intended. On the other hand, they both had somewhat lower accuracy compared to basic sampling for 10k dataset size. From-zero sampling performed very similar to basic sampling,  except for 10 digits, where 10k training samples were not enough to achieve convergence. In that case, it could be that uniform digits and uniform bits sampling did not allow for enough training samples with the full 10 decimal digits in the training dataset if the training dataset size is small and the number of possible bits/digits is large.


As expected, the multiplication task (\cref{tbl:sampling_strategies_mul_basic} to \cref{tbl:sampling_strategies_mul_uniformbits}) is much harder to learn, with most combinations of operand size and dataset size leaving the final model at zero accuracy. This does not change significantly based on the sampling method used, however uniform digits and uniform bits sampling do lead to a  slight increase in accuracy for the 100k-dataset, 3-digits case, which given the limited sample space and large number of samples, is the easiest case as expected.

The integer square root task (\cref{tbl:sampling_strategies_sqrt_basic} to \cref{tbl:sampling_strategies_sqrt_uniformbits}) is somewhat in between addition and multiplication  for learning difficulty. The accuracy tables for basic and from-zero sampling also look very similar for this task.  Uniform digits and uniform bits sampling lead to slightly worse accuracy numbers, but the general characteristic is the same: The accuracy numbers mostly depend on the operation learned (addition versus multiplication versus square root) and only slightly change based on the sampling method.


\begin{table}[!htbp]
	\begin{minipage}[t]{0.47\linewidth}
		\hfill
		\includeAccuracyTableCore{experiment_results/sampling_strategies/basic_add.csv}{}
		\captionof{table}{Addition accuracy, basic sampling.}
		\label{tbl:sampling_strategies_add_basic}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.47\linewidth}
		\hfill
		\includeAccuracyTableCore{experiment_results/sampling_strategies/from_zero_add.csv}{}
		\captionof{table}{Addition accuracy, from-zero sampling.}
		\label{tbl:sampling_strategies_add_fromzero}
	\end{minipage}
	
	\vspace{0.5cm}
	\begin{minipage}[t]{0.47\linewidth}
		\hfill
		\includeAccuracyTableCore{experiment_results/sampling_strategies/uniform_digits_add.csv}{}
		\captionof{table}{Addition accuracy, uniform digits sampling.}
		\label{tbl:sampling_strategies_add_uniformdigits}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.47\linewidth}
		\hfill
		\includeAccuracyTableCore{experiment_results/sampling_strategies/uniform_bits_add.csv}{}
		\captionof{table}{Addition accuracy uniform bits sampling.}
		\label{tbl:sampling_strategies_add_uniformbits}
	\end{minipage}
	
\end{table}


\begin{table}[!htbp]
	\begin{minipage}[t]{0.47\linewidth}
		\hfill
		\includeAccuracyTableCore{experiment_results/sampling_strategies/basic_mul.csv}{}
		\captionof{table}{Multiplication accuracy, basic sampling.}
		\label{tbl:sampling_strategies_mul_basic}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.47\linewidth}
		\hfill
		\includeAccuracyTableCore{experiment_results/sampling_strategies/from_zero_mul.csv}{}
		\captionof{table}{Multiplication accuracy, from-zero sampling.}
		\label{tbl:sampling_strategies_mul_fromzero}
	\end{minipage}
	
	\vspace{0.5cm}
	\begin{minipage}[t]{0.47\linewidth}
		\hfill
		\includeAccuracyTableCore{experiment_results/sampling_strategies/uniform_digits_mul.csv}{}
		\captionof{table}{Multiplication accuracy, uniform digits sampling.}
		\label{tbl:sampling_strategies_mul_uniformdigits}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.47\linewidth}
		\hfill
		\includeAccuracyTableCore{experiment_results/sampling_strategies/uniform_bits_mul.csv}{}
		\captionof{table}{Multiplication accuracy, uniform bits sampling.}
		\label{tbl:sampling_strategies_mul_uniformbits}
	\end{minipage}
\end{table}


\begin{table}[!htbp]
	\begin{minipage}[t]{0.47\linewidth}
		\hfill
		\includeAccuracyTableCore{experiment_results/sampling_strategies/basic_sqrt.csv}{}
		\captionof{table}{Integer square root accuracy, basic sampling.}
		\label{tbl:sampling_strategies_sqrt_basic}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.47\linewidth}
		\hfill
		\includeAccuracyTableCore{experiment_results/sampling_strategies/from_zero_sqrt.csv}{}
		\captionof{table}{Integer square root accuracy, from-zero sampling.}
		\label{tbl:sampling_strategies_sqrt_fromzero}
	\end{minipage}
	
	\vspace{0.5cm}
	\begin{minipage}[t]{0.47\linewidth}
		\hfill
		\includeAccuracyTableCore{experiment_results/sampling_strategies/uniform_digits_sqrt.csv}{}
		\captionof{table}{Integer square root accuracy, uniform digits sampling.}
		\label{tbl:sampling_strategies_sqrt_uniformdigits}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.47\linewidth}
		\hfill
		\includeAccuracyTableCore{experiment_results/sampling_strategies/uniform_bits_sqrt.csv}{}
		\captionof{table}{Integer square root accuracy, uniform bits sampling.}
		\label{tbl:sampling_strategies_sqrt_uniformbits}
	\end{minipage}
\end{table}



\subsection{Result reversal experiments}
\label{results:reversal}


For the addition task (\cref{tbl:reverse_add}), when compared to the baseline experiments (\cref{tbl:reverse_add_baseline}), result reversal led to somewhat successful training (71\% accuracy) for the 3 digits, 1k dataset scenario, which could not be learned in baseline. However, training failed for the 10 digits, 10k dataset scenario.
To us it is not clear whether this is due to a true difference in the learnability between the two training sample structures, or simply due to variance between training runs. Similar differences observed in the sampling strategy experiments (\cref{results:sampling}) might point towards the latter.

It is also worth noting that for result reversal, 5 of the 9 scenarios, including all operand sizes for the 100k dataset size, led to perfect test accuracy (all 1000 samples correct), while for the baseline addition experiments accuracy was not quite perfect for the experiments with 5 and 10 operand digits.

For the multiplication task (\cref{tbl:reverse_mul}), results were in general very similar to those obtained without result reversal (\cref{tbl:reverse_mul_baseline}). This operation thus remained the hardest of the three studied and could not be made easier for the model by first presenting the digit-reversed product in the training samples.

For the integer square root task (\cref{tbl:reverse_sqrt}), the results were also very similar to the baseline task without result reversal (\cref{tbl:reverse_sqrt_baseline}), without significant differences in which scenarios could be learned and  the final accuracy numbers obtained.

\begin{table}[!htbp]
	\begin{minipage}[t]{0.47\linewidth}
		\hfill
		\includeAccuracyTableCore{experiment_results/baseline_final_accuracy/add.csv}{}
		\captionof{table}{Baseline addition accuracy (without result reversal).}
		\label{tbl:reverse_add_baseline}
	\end{minipage}
	\hfill %%%
	\begin{minipage}[t]{0.47\linewidth}
		\hfill
		\includeAccuracyTableCore{experiment_results/intermediate_steps/reverse/add.csv}{}
		\captionof{table}{Addition accuracy when using result reversal.}
		\label{tbl:reverse_add}
	\end{minipage}
\end{table}

\begin{table}[!htbp]
	\begin{minipage}[t]{0.47\linewidth}
		\hfill
		\includeAccuracyTableCore{experiment_results/baseline_final_accuracy/mul.csv}{}
		\captionof{table}{Baseline multiplication accuracy (without result reversal).}
		\label{tbl:reverse_mul_baseline}
	\end{minipage}
	\hfill %%%
	\begin{minipage}[t]{0.47\linewidth}
		\hfill
		\includeAccuracyTableCore{experiment_results/intermediate_steps/reverse/mul.csv}{}
		\captionof{table}{Multiplication accuracy when using result reversal.}
		\label{tbl:reverse_mul}
	\end{minipage}
\end{table}

\begin{table}[!htbp]
	\begin{minipage}[t]{0.47\linewidth}
		\hfill
		\includeAccuracyTableCore{experiment_results/baseline_final_accuracy/sqrt.csv}{}
		\captionof{table}{Baseline integer square root accuracy (without result reversal).}
		\label{tbl:reverse_sqrt_baseline}
	\end{minipage}
	\hfill %%%
	\begin{minipage}[t]{0.47\linewidth}
		\hfill
		\includeAccuracyTableCore{experiment_results/intermediate_steps/reverse/sqrt.csv}{}
		\captionof{table}{Integer square root accuracy when using result reversal.}
		\label{tbl:reverse_sqrt}
	\end{minipage}
\end{table}


\subsubsection{Accuracy development}

We also generated accuracy development plots for the result reversal experiments. These plots were combined for the result reversal and scratchpad experiments and are analyzed in \cref{results:rev_scratch_accuracy}.


\subsubsection{Error analysis}
\label{results:reversal:error}

We wanted to investigate whether there was any difference in the error patterns between the baseline models and the ones trained using result reversal. To do this, we used the same tasks as for the baseline error analysis (\cref{error_analysis}): addition with 5 and 10 digits (100k dataset), multiplication with 3 and 5 digits (100k dataset) and square root with 6 and 20 digits (100k dataset).

For both 5-digit addition and 10-digit addition, when training using result reversal on a 100k training dataset, there were \textbf{no errors} in the entire 100k error analysis dataset, meaning the model performed each addition correctly. This corresponds to the perfect scores seen on the test dataset for these tasks (\cref{tbl:reverse_add}).
This stands in contrast to the baseline experiments, where despite 99\% accuracy, it was not hard to still find numbers where the addition was performed incorrectly (\cref{tbl:errors_add_5digits}, \cref{tbl:errors_add_10digits}).

For 3-digit multiplication (\cref{tbl:reverse_errors_mul_3digits}), the error patterns observed were basically the same as for the baseline experiments  (\cref{tbl:errors_mul_3digits}): Single digits being off by one or, sometimes, more than one.
The mistakes are already present in the reversed result,
which is then reversed correctly.

\includeTable
{
	task;model;correct\\
	896*551=;[692394]493296;[696394]493696\\
	219*107=;[336320]023633;[334320]023433\\
	192*926=;[293771]177392;[297771]177792\\
	696*135=;[060390]093060;[069390]093960\\
	787*809=;[382636]636283;[386636]636683\\
}
{tbl:reverse_errors_mul_3digits}
{
	Model errors (multiplication task, result reversal, 3 digits, 100k training dataset size).
}
{
	columns/task/.style={column name={Task}},
	columns/model/.style={column name={Model result}},
	columns/correct/.style={column name={Correct result}}
}

For 5-digit multiplication (\cref{tbl:reverse_errors_mul_5digits}), we can also observe similar error patterns compared to the corresponding baseline experiment: The first and last digits of the result are correct, while the digits in the middle are wrong.
There also seems to be a pattern of repetition of single digits in the middle of the results, where the model has not learned to output the correct digits.

\includeTable
{
	task;model;correct\\
	67767*25315=;[5062665171]1715662605;[5061255171]1715521605\\
	10981*21880=;[0826666320]0236666280;[0824620420]0240264280\\
	86313*14577=;[1059999421]1249999501;[1064818521]1258184601\\
	98023*21001=;[3211119502]2059111123;[3201858502]2058581023\\
	22900*56482=;[0087774921]1294777800;[0087343921]1293437800\\
}
{tbl:reverse_errors_mul_5digits}
{
	Model errors (multiplication task, result reversal, 5 digits, 100k training dataset size).
}
{
	columns/task/.style={column name={Task}},
	columns/model/.style={column name={Model result}},
	columns/correct/.style={column name={Correct result}}
}

For  6-digit integer square root (\cref{tbl:reverse_errors_sqrt_6digits}), the error patterns are also similar to the baseline (\cref{tbl:errors_sqrt_6digits}): Off-by-one errors on numbers where the operand is close to a square number, and the model "picked the wrong side".

\includeTable
{
	task;model;correct;sqrt\\
	458306:;[776]677;[676]676;676.98301\\
	478802:;[296]692;[196]691;691.95520\\
	988048:;[399]993;[499]994;994.00604\\
	651341:;[608]806;[708]807;807.05700\\
	942826:;[179]971;[079]970;970.99228\\
}
{tbl:reverse_errors_sqrt_6digits}
{
	Model errors (square root task, result reversal, 6 digits, 100k training dataset size).
}
{
	columns/task/.style={column name={Task}},
	columns/model/.style={column name={Model result}},
	columns/correct/.style={column name={Correct result}},
	columns/sqrt/.style={column name={$\sqrt{Task}$}}
}

For 20-digit integer square root (\cref{tbl:reverse_errors_sqrt_20digits}), the error patterns once again more or less match the baseline (\cref{tbl:errors_sqrt_20digits}): the 3 most significant digits are correct, the rest are wrong.
Compared to the baseline, we can see the result reversal model only got 3 out of 10 most significant digits correct for the examples in the table, whereas for the baseline it was 4 or even 5 correct digits for the examples.
Also there is an interesting digit pattern not seen in the baseline errors. A lot of repeated "8"s in the less significant digits where the model is wrong. This corresponds to similar digit repetitions for the multiplication task errors (\cref{tbl:reverse_errors_mul_5digits}).


\includeTable
{
	task;model;correct;sqrt\\
	79694857207412785081:;[8828828298]8928288288;[2167917298]8927197612;8927197612.208\\
	10138193304849869414:;[8888888813]3188888888;[8692504813]3184052968;3184052968.286\\
	20740203138915481021:;[8828828554]4558288288;[7131414554]4554141317;4554141317.407\\
	22785206268196001170:;[8288282774]4772828828;[0025833774]4773385200;4773385200.065\\
	61032565798178795501:;[8818888187]7818888188;[9024332187]7812334209;7812334209.324\\
}
{tbl:reverse_errors_sqrt_20digits}
{
	Model errors (square root task, result reversal, 20 digits, 100k training dataset size).
}
{
	columns/task/.style={column name={Task}},
	columns/model/.style={column name={Model result}},
	columns/correct/.style={column name={Correct result}},
	columns/sqrt/.style={column name={$\sqrt{Task}$}}
}



\subsection{Scratchpad experiments}
\label{results:scratchpad}


For the addition task (\cref{tbl:scratchpad_add}), using scratchpad brings clear improvements over baseline (\cref{tbl:scratchpad_add_baseline}): For the 1k dataset size, training was more or less successful for all operand sizes (good accuracy but not 100\%), whereas there was no accuracy for the baseline experiments at 1k dataset size.
For the 10k and 100k dataset sizes, accuracy was perfect for all operand sizes, whereas in the baseline experiments accuracy was perfect only for 3 digit addition.

For the multiplication task (\cref{tbl:scratchpad_mul}), there were even more significant improvements made by providing a scratchpad: For the baseline experiments (\cref{tbl:scratchpad_mul_baseline}) only 3 digits, 10k dataset and 3 digits, 100k dataset provided above-zero accuracy, and even there it was only about 20\%. When using the scratchpad, accuracy is good for 3 digits, 10k and 100k dataset sizes.
Even for 5 and 10 digit multiplication, there is some degree of accuracy obtained, whereas it was a flat zero in the baseline experiments.

Because the accuracy numbers for 5 and 10 digits are somewhat "in the middle", showing the model is able to do a significant portion of multiplications correctly, but still making some errors, it is not unreasonable to assume training for longer would have made the model able to perform 5 and 10 digit multiplications with high accuracy. In contrast to the baseline, we had to limit training epochs for the multiplication scratchpad experiments to keep training times reasonable (\cref{setup:scratchpad}, \cref{tbl:scratchpad_training_add}).

It is also an interesting observation that accuracy numbers obtained on the 100k training dataset were lower than those on the 10k dataset. Training was limited to 50 epochs for the 10k dataset and 5 epochs for the 100k dataset (\cref{tbl:scratchpad_training_mul}). However, the total amount of training batches / optimization steps is the same for both scenarios, but the variety of training samples is lower in the 10k dataset scenario, increasing the risk of overfitting.
We would thus have expected better scores for the 100k dataset despite the lower epoch count.

For the integer square root task (\cref{tbl:scratchpad_sqrt}) the results were, interestingly, generally worse than baseline (\cref{tbl:scratchpad_sqrt_baseline}). This might be for a number of reasons:
First of all, we had to reduce the amount of training epochs, even more so than for the multiplication tasks, due to the very long scratchpads (\cref{tbl:scratchpad_sample_lengths}). Thus the resulting models are most likely severely undertrained.

Also, there is an important difference in the scratchpad structure between addition/multiplication and integer square root: For the addition and multiplication operations, the task was decomposed down into the smallest of pieces (single-digit addition and multiplication) (\cref{add_scratchpad}, \cref{mul_scratchpad}). Whereas for the square root task, we modeled the steps of the binary search algorithm but just left the squaring operations of large numbers as-is, without any explanation on how to compute the squares (we could not include this information as it would have made the scratchpads too massive) (\cref{sqrt_scratchpad}).



\begin{table}[!htbp]
	\begin{minipage}[t]{0.47\linewidth}
		\hfill
		\includeAccuracyTableCore{experiment_results/baseline_final_accuracy/add.csv}{}
		\captionof{table}{Baseline addition accuracy (without scratchpad).}
		\label{tbl:scratchpad_add_baseline}
	\end{minipage}
	\hfill %%%
	\begin{minipage}[t]{0.47\linewidth}
		\hfill
		\includeAccuracyTableCore{experiment_results/intermediate_steps/scratchpad/add.csv}{}
		\captionof{table}{Addition accuracy when using scratchpad.}
		\label{tbl:scratchpad_add}
	\end{minipage}
\end{table}

\begin{table}[!htbp]
	\begin{minipage}[t]{0.47\linewidth}
		\hfill
		\includeAccuracyTableCore{experiment_results/baseline_final_accuracy/mul.csv}{}
		\captionof{table}{Baseline multiplication accuracy (without scratchpad).}
		\label{tbl:scratchpad_mul_baseline}
	\end{minipage}
	\hfill %%%
	\begin{minipage}[t]{0.47\linewidth}
		\hfill
		\includeAccuracyTableCore{experiment_results/intermediate_steps/scratchpad/mul.csv}{}
		\captionof{table}{Multiplication accuracy when using scratchpad.}
		\label{tbl:scratchpad_mul}
	\end{minipage}
\end{table}

\begin{table}[!htbp]
	\begin{minipage}[t]{0.47\linewidth}
		\hfill
		\includeAccuracyTableCore{experiment_results/baseline_final_accuracy/sqrt.csv}{}
		\captionof{table}{Baseline integer square root accuracy (without scratchpad).}
		\label{tbl:scratchpad_sqrt_baseline}
	\end{minipage}
	\hfill %%%
	\begin{minipage}[t]{0.47\linewidth}
		\hfill
		\includeAccuracyTableCore{experiment_results/intermediate_steps/scratchpad/sqrt.csv}{}
		\captionof{table}{Integer square root accuracy when using scratchpad.}
		\label{tbl:scratchpad_sqrt}
	\end{minipage}
\end{table}

\subsubsection{Accuracy development}
\label{results:rev_scratch_accuracy}


We wanted to investigate how quickly the model achieved good accuracy for the various tasks and operand sizes when using result reversal or scratchpad compared to baseline. For this, we plotted the accuracy on the validation dataset during training.
The diagrams only show accuracy development for the 100k dataset size for brevity.
For scratchpad-based training, we evaluated accuracy only once every epoch and reduced the validation dataset to 10 samples (\cref{setup:scratchpad}). This of course leads to less precise curves, but can still give a rough idea about training progress.

For the addition task (\cref{fig:intermediate_steps_accuracy_add}) we can see that baseline training needs between 2000 (3 digits) and over 20000 (10 digits) training steps to achieve near 100\% accuracy. In contrast, when using result reversal, the model learned somewhat faster (1000-2000 steps for 3 and 5 digits, 10000 steps for 10 digits) and accuracy hits 100\% much more abruptly compared to baseline, where it hits 80\% and then needs some time to get to 100\%.
Using a scratchpad leads to much faster convergence, where 3 digit addition already converged at the first log interval at 400 steps, and 5 and 10 digit addition converged before 1000 steps.

For the multiplication task (\cref{fig:intermediate_steps_accuracy_mul}), we can see that only scratchpad with 3 digit multiplication converged to 100\%. For the same task, baseline and result reversal had similar curves, starting to leave zero at about 3000 steps, and ending at around 20\% accuracy after about 40000 steps.
For 5 and 10 digit multiplication, only scratchpad-based training achieved above-zero accuracy, while still remaining at around 20\% accuracy and taking between 3000 and 20000 steps to start converging.
It might however converge all the way to 100\% given continued training, but resource constraints made us limit the number of training epochs (\cref{tbl:scratchpad_training_mul}).

For the square root task with 6 operand digits (\cref{fig:intermediate_steps_accuracy_sqrt}), baseline and result reversal performed very similar while the scratchpad took longer to start converging but then quickly converged, matching the other two training methods.
For 10 operand digits, scratchpad actually failed to converge, as opposed to baseline and result reversal. This happened despite taking more optimization steps until training was stopped. It would thus be reasonable to assume that the form of scratchpad we used here, which does not further elaborate on the squaring step, is not helpful for model training, as opposed to the successful addition and multiplication scratchpads.



\includePDFPlotSmall{experiment_results/intermediate_steps_accuracy/add.pdf}{fig:intermediate_steps_accuracy_add}{Development of accuracy on the validation dataset, when training the addition task using a 100k dataset using various training methods and operand digits.}

\includePDFPlotSmall{experiment_results/intermediate_steps_accuracy/mul.pdf}{fig:intermediate_steps_accuracy_mul}{Development of accuracy on the validation dataset, when training the multiplication task using a 100k dataset using various training methods and operand digits.}

\includePDFPlotSmall{experiment_results/intermediate_steps_accuracy/sqrt.pdf}{fig:intermediate_steps_accuracy_sqrt}{Development of accuracy on the validation dataset, when training the square root task using a 100k dataset using various training methods and operand digits.}



\subsubsection{Error analysis}

Similar to the baseline and result reversal experiments, we looked for errors in the model output for the addition task with 5 and 10 digits (100k dataset), multiplication task with 3 and 5 digits (100k dataset) and square root task with 6 digits (100k dataset).
We did not analyze errors for the square root task with 20 operand digits, because the scratchpads are so long that presenting them here is impractical.
We only looked at 3 errors per task due to the longer output size when using the scratchpad. The error samples presented are simply the first 3 samples from the error analysis dataset where the model output did not match the correct output (considering both the scratchpad and final result).

% Add errors

For 5-digit and 10-digit addition using a 100k training dataset, there were no errors in the 100k error analysis dataset, similar to training for addition using result reversal (\cref{results:reversal:error}).
Thus both result reversal and scratchpads allow for training a model that has a very low error probability for addition tasks up to 10 digits.

% 3 digit mul errors

For 3-digit multiplication, the three error samples in \cref{tbl:errors_scratchpad_mul3} show that there were no errors in the single-digit multiplications. In the first error sample, the model fetched the wrong digit from the single-digit results during the addition phase. We hypothesize this could be due to there being multiple single-digit products that are zero, thus making it harder to pick the right digits due to the uneven length of the products.
For the second and third error sample, the model did the entire calculation correctly, but surprisingly failed to read the correct digit sequence from the addition scratchpad.

\ErrorAnalysisTableBegin

\begin{lstlisting}
185*200=[
*200{52001,82171,12130}37000,
*0{50000,80000,10000}0,
*0{50000,80000,10000}0
|{
	000,0,0,0;
	0,0,0,0;
	0,0,0,0;
	!3!,0,!3!,0;
	3,0,3,0
}]
3!3!000
\end{lstlisting} &
\begin{lstlisting}
185*200=[
*200{52001,82171,12130}37000,
*0{50000,80000,10000}0,
*0{50000,80000,10000}0
|{
	000,0,0,0;
	0,0,0,0;
	0,0,0,0;
	7,0,7,0;
	3,0,3,0
}]
37000
\end{lstlisting} \\ \ErrorAnalysisTableRule

\begin{lstlisting}
595*730=[
*700{57053,97366,57614}416500,
*30{53051,93182,53271}17850,
*0{50000,90000,50000}0
|{
	000,0,0,0;
	05,0,5,0;
	58,0,3,1;
	67,1,4,1;
	11,1,3,0;
	4,0,4,0
}]
4!5!4350
\end{lstlisting} &
\begin{lstlisting}
595*730=[
*700{57053,97366,57614}416500,
*30{53051,93182,53271}17850,
*0{50000,90000,50000}0
|{
	000,0,0,0;
	05,0,5,0;
	58,0,3,1;
	67,1,4,1;
	11,1,3,0;
	4,0,4,0
}]
434350
\end{lstlisting} \\ \ErrorAnalysisTableRule

\begin{lstlisting}
577*170=[
*100{71070,71070,51050}57700,
*70{77094,77435,57504}40390,
*0{70000,70000,50000}0
|{
	000,0,0,0;
	09,0,9,0;
	73,0,0,1;
	70,1,8,0;
	54,0,9,0
}]
980!8!0
\end{lstlisting} &
\begin{lstlisting}
577*170=[
*100{71070,71070,51050}57700,
*70{77094,77435,57504}40390,
*0{70000,70000,50000}0
|{
	000,0,0,0;
	09,0,9,0;
	73,0,0,1;
	70,1,8,0;
	54,0,9,0
}]
98090
\end{lstlisting} \\

\ErrorAnalysisTableEnd{tbl:errors_scratchpad_mul3}{Error samples for the multiplication task (3 digits, 100k training dataset, using scratchpad, trained for 5 epochs).}





\newpage
% 5 digit mul errors

For 5-digit multiplication, the first error sample (\cref{tbl:errors_scratchpad_mul5_1}) shows the model failing to compute the correct sum based on the correct list of digits and current carry value. This led to some digits in the middle of the product being wrong.





\ErrorAnalysisTableBegin

\begin{lstlisting}
67767*25315=[
*20000{72041,62131,72151,72151,62131}
1355340000,
*5000{75053,65333,75383,75383,65333}
338835000,
*300{73012,63202,73232,73232,63202}
20330100,
*10{71070,61060,71070,71070,61060}
677670,
*5{75053,65333,75383,75383,65333}
338835
|{
	00005,0,5,0;
	00073,0,0,1;
	00168,1,6,1;
	05078,1,1,2;
	43373,2,!3!,2;
	38363,2,!7!,2;
	580,2,5,1;
	532,1,1,1;
	33,1,7,0;
	1,0,1,0
}]
1715!73!1605
\end{lstlisting} &
\begin{lstlisting}
67767*25315=[
*20000{72041,62131,72151,72151,62131}
1355340000,
*5000{75053,65333,75383,75383,65333}
338835000,
*300{73012,63202,73232,73232,63202}
20330100,
*10{71070,61060,71070,71070,61060}
677670,
*5{75053,65333,75383,75383,65333}
338835
|{
	00005,0,5,0;
	00073,0,0,1;
	00168,1,6,1;
	05078,1,1,2;
	43373,2,2,2;
	38363,2,5,2;
	580,2,5,1;
	532,1,1,1;
	33,1,7,0;
	1,0,1,0
}]
1715521605
\end{lstlisting} \\ \ErrorAnalysisTableRule
\ErrorAnalysisTableEnd{tbl:errors_scratchpad_mul5_1}{First error sample for the multiplication task (5 digits, 100k training dataset, using scratchpad, trained for 5 epochs).}


\newpage
The second error sample  (\cref{tbl:errors_scratchpad_mul5_2}) shows the same pattern: wrong sum adding multiple digits during the addition phase - leading to some wrong digits in the middle of the final product.

\ErrorAnalysisTableBegin
\begin{lstlisting}
10981*21880=[
*20000{12020,82061,92191,02110,12020}
219620000,
*1000{11010,81080,91090,01000,11010}
10981000,
*800{18080,88046,98687,08770,18080}
8784800,
*80{18080,88046,98687,08770,18080}
878480,
*0{10000,80000,90000,00000,10000}
0
|{
	00000,0,0,0;
	0008,0,8,0;
	0084,0,2,1;
	0148,1,4,1;
	2887,1,!8!,2;
	6978,2,!0!,3;
	908,3,0,2;
	11,2,4,0;
	2,0,2,0
}]
240!08!4280
\end{lstlisting} &
\begin{lstlisting}
10981*21880=[
*20000{12020,82061,92191,02110,12020}
219620000,
*1000{11010,81080,91090,01000,11010}
10981000,
*800{18080,88046,98687,08770,18080}
8784800,
*80{18080,88046,98687,08770,18080}
878480,
*0{10000,80000,90000,00000,10000}
0
|{
	00000,0,0,0;
	0008,0,8,0;
	0084,0,2,1;
	0148,1,4,1;
	2887,1,6,2;
	6978,2,2,3;
	908,3,0,2;
	11,2,4,0;
	2,0,2,0
}]
240264280
\end{lstlisting} \\
\ErrorAnalysisTableEnd{tbl:errors_scratchpad_mul5_2}{Second error sample for the multiplication task (5 digits, 100k training dataset, using scratchpad, trained for 5 epochs).}


\newpage
The third error sample  (\cref{tbl:errors_scratchpad_mul5_3}) shows a somewhat different pattern: Interestingly, there were mistakes in a single-digit multiplication with factor 1, basically failing to copy the digits. This led to wrong digits in the middle of the final product.
In summary, there was only one error observed in the digit multiplication stage and one error writing the final result, while there were multiple errors in the sum stage, either computing the wrong sum or fetching the wrong digits.

\ErrorAnalysisTableBegin
\begin{lstlisting}
98023*21001=[
*20000{32060,22040,02000,82061,92191}
1960460000,
*1000{31030,21020,01000,81080,91090}
98023000,
*0{30000,20000,00000,80000,90000}
0,
*0{30000,20000,00000,80000,90000}
0,
*1{31030,21020,!2!10!2!0,!0!1000,!8!10!8!0}
!802!23
|{
	00003,0,3,0;
	002,0,2,0;
	00!2!,0,!2!,0;
	03!0!,0,!3!,!0!;
	62!8!,0,!6!,!1!;
	40,1,5,0;
	08,0,8,0;
	69,0,5,1;
	9,1,0,1;
	1,1,2,0
}]
20585!632!23
\end{lstlisting} &
\begin{lstlisting}
98023*21001=[
*20000{32060,22040,02000,82061,92191}
1960460000,
*1000{31030,21020,01000,81080,91090}
98023000,
*0{30000,20000,00000,80000,90000}
0,
*0{30000,20000,00000,80000,90000}
0,
*1{31030,21020,01000,81080,91090}
98023
|{
	00003,0,3,0;
	002,0,2,0;
	000,0,0,0;
	038,0,1,1;
	629,1,8,1;
	40,1,5,0;
	08,0,8,0;
	69,0,5,1;
	9,1,0,1;
	1,1,2,0
}]
2058581023
\end{lstlisting}
\\
\ErrorAnalysisTableEnd{tbl:errors_scratchpad_mul5_3}{Third error sample for the multiplication task (5 digits, 100k training dataset, using scratchpad, trained for 5 epochs).}



% Sqrt errors
\newpage
For the square root task with 6 digits, in the first error sample (\cref{tbl:errors_scratchpad_sqrt6_1}) we can see the model making mistakes in squaring large numbers at the beginning of the algorithm. This, however, did not influence the final result as the algorithm always has to take a few steps in the same direction at the start due do the large search interval $[0, n]$.
The model did make another mistake at the very end of the algorithm, picking the wrong half of the search space and then picking the wrong one of two numbers for the final square root.

\ErrorAnalysisTableBegin
\begin{lstlisting}
915965:[
{0,915965}457982*457982=209!199148!324
{0,457982}228991*228991=52!299787!081
{0,228991}114495*114495=13!074832!025
{0,114495}57247*57247=32!68650!009
{0,57247}28623*28623=81!7133!129
{0,28623}14311*14311=204!268!721
{0,14311}7155*7155=51!060!025
{0,7155}3577*3577=127!61!929
{0,3577}1788*1788=31!88!944
{0,1788}894*894=799236
{894,1788}1341*1341=1798281
{894,1341}1117*1117=1247689
{894,1117}1005*1005=1010025
{894,1005}949*949=900601
{949,1005}977*977=954529
{949,977}963*963=927369
{949,963}956*956=913936
{956,963}959*959=919681
{956,959}957*957=915849
{!956,957!}957*957=915849
{!956,957!}
]
!956!
\end{lstlisting} &
\begin{lstlisting}
915965:[
{0,915965}457982*457982=209747512324
{0,457982}228991*228991=52436878081
{0,228991}114495*114495=13109105025
{0,114495}57247*57247=3277219009
{0,57247}28623*28623=819276129
{0,28623}14311*14311=204804721
{0,14311}7155*7155=51194025
{0,7155}3577*3577=12794929
{0,3577}1788*1788=3196944
{0,1788}894*894=799236
{894,1788}1341*1341=1798281
{894,1341}1117*1117=1247689
{894,1117}1005*1005=1010025
{894,1005}949*949=900601
{949,1005}977*977=954529
{949,977}963*963=927369
{949,963}956*956=913936
{956,963}959*959=919681
{956,959}957*957=915849
{957,959}958*958=917764
{957,958}
]
957
\end{lstlisting} \\
\ErrorAnalysisTableEnd{tbl:errors_scratchpad_sqrt6_1}{First error sample for the square root task (6 digits, 100k training dataset, using scratchpad, trained for 5 epochs).}


\newpage
In the second error sample (\cref{tbl:errors_scratchpad_sqrt6_2}) we can see the model making similar errors computing large squares at the start. It then did fine computing smaller squares and picking the right half of the search space. At the very end, it picked the wrong half of the search space, similarly to the first example.

\ErrorAnalysisTableBegin
\begin{lstlisting}
195046:[
{0,195046}97523*97523=95!0818!5529
{0,97523}48761*48761=2377!097!121
{0,48761}24380*24380=594!24!4400
{0,24380}12190*12190=1485!616!00
{0,12190}6095*6095=3714!0!025
{0,6095}3047*3047=928!1!209
{0,3047}1523*1523=231!8!529
{0,1523}761*761=579121
{0,761}380*380=144400
{380,761}570*570=324900
{380,570}475*475=225625
{380,475}427*427=182329
{427,475}451*451=203401
{427,451}439*439=192721
{439,451}445*445=198025
{439,445}442*442=195364
{439,442}440*440=193600
{440,442}441*441=194481
{!440,441!}
]
!440!
\end{lstlisting} &
\begin{lstlisting}
195046:[
{0,195046}97523*97523=9510735529
{0,97523}48761*48761=2377635121
{0,48761}24380*24380=594384400
{0,24380}12190*12190=148596100
{0,12190}6095*6095=37149025
{0,6095}3047*3047=9284209
{0,3047}1523*1523=2319529
{0,1523}761*761=579121
{0,761}380*380=144400
{380,761}570*570=324900
{380,570}475*475=225625
{380,475}427*427=182329
{427,475}451*451=203401
{427,451}439*439=192721
{439,451}445*445=198025
{439,445}442*442=195364
{439,442}440*440=193600
{440,442}441*441=194481
{441,442}
]
441
\end{lstlisting} \\
\ErrorAnalysisTableEnd{tbl:errors_scratchpad_sqrt6_2}{Second error sample for the square root task (6 digits, 100k training dataset, using scratchpad, trained for 5 epochs).}



\newpage
The third error sample (\cref{tbl:errors_scratchpad_sqrt6_3}) shows the model coming to the right conclusion after making a few mistakes computing larger squares at the start.
In summary, we can see that errors happen when computing larger square numbers at the start of the algorithm and when picking the right half at the end of the algorithm. Interestingly, computing squares towards of the algorithm does not seem a cause of errors.

\ErrorAnalysisTableBegin
\begin{lstlisting}
562141:[
{0,562141}281070*281070=7!817572!4900
{0,281070}140535*140535=19!543931!225
{0,140535}70267*70267=4!885917!289
{0,70267}35133*35133=12!21444!689
{0,35133}17566*17566=30!5343!356
{0,17566}8783*8783=7!6335!089
{0,8783}4391*4391=19!079!881
{0,4391}2195*2195=4!767!025
{0,2195}1097*1097=1!190!409
{0,1097}548*548=!299!304
{548,1097}822*822=675684
{548,822}685*685=469225
{685,822}753*753=567009
{685,753}719*719=516961
{719,753}736*736=541696
{736,753}744*744=553536
{744,753}748*748=559504
{748,753}750*750=562500
{748,750}749*749=561001
{749,750}
]
749
\end{lstlisting} &
\begin{lstlisting}
562141:[
{0,562141}281070*281070=79000344900
{0,281070}140535*140535=19750086225
{0,140535}70267*70267=4937451289
{0,70267}35133*35133=1234327689
{0,35133}17566*17566=308564356
{0,17566}8783*8783=77141089
{0,8783}4391*4391=19280881
{0,4391}2195*2195=4818025
{0,2195}1097*1097=1203409
{0,1097}548*548=300304
{548,1097}822*822=675684
{548,822}685*685=469225
{685,822}753*753=567009
{685,753}719*719=516961
{719,753}736*736=541696
{736,753}744*744=553536
{744,753}748*748=559504
{748,753}750*750=562500
{748,750}749*749=561001
{749,750}
]
749
\end{lstlisting} \\
\\
\ErrorAnalysisTableEnd{tbl:errors_scratchpad_sqrt6_3}{Third error sample for the square root task (6 digits, 100k training dataset, using scratchpad, trained for 5 epochs).}




\subsection{Attention heatmaps}
\label{results:heatmap}

We generated attention heatmap diagrams for some example tasks processed by the corresponding models as outlined in \cref{methods:heatmap} and \cref{setup:heatmap}.
These diagrams are to be interpreted as follows:

\begin{itemize}
	\item Each row refers to the sequence position where a certain token is generated by the model (the position in the token sequence where the model outputs this token based on the previous ones as input).
	
	\item For each row, the columns of that row refer to previous token positions that the model will consider for generating the output at that position. Note that the attention values for the current and following token positions are always zero, as the model has not yet generated these values at that point.
	
	\item Colors show attention intensity and range from dark purple (no attention to this position) to yellow (full attention to this position).
\end{itemize}


\subsubsection{Addition task}

Attention activation for the addition task "123+456=0579" is illustrated in \cref{fig:heatmap_add_full} for each of the 6 GPT-2 blocks and each of the 6 attention heads per block.
When looking for patterns in these activation heatmaps, a clear relationship is visible in Block 1, where the position of the output digit "5" is mostly attending to the positions to the input digits "1" and "4", the position of the output digit "7" to the input digits "2" and "5" and the output digit "9" to the input digits "3" and "6". The first output digit "0" also attends mostly to the input digits "1" and "4". This is consistent with how data would flow from input to output in a standard addition algorithm. The stop token "." position does not attend to any input position in particular.
This pattern is more or less the same across all 6 attention heads.
When looking at the later blocks, there are also some spots of high activation, however these are often not uniform across attention heads and there is no obvious pattern between input and outputs.

Since we found out that the most visible attention patterns are in the first block, to keep the diagrams of reasonable size especially for larger sequences, for the rest of the diagrams only the attention mechanism of the first GPT-2 block is considered, and the attention values of all 6 heads are averaged.
The single-diagram heatmap of \cref{fig:heatmap_add_simple1} for the same addition task shows the same pattern mentioned before, with an output digit being connected to those two input digits being added for this position, with the relationship being clear for the digits "579" and a bit less prominent for the digit "0".

There is little change to the previous diagram in \cref{fig:heatmap_add_simple2} which shows the same addition model working on different numbers, namely the addition "999+001". Only minor changes in attention value are visible when doing addition of different numbers using the same model.




\includePDFPlot{experiment_results/heatmaps/add_full_1.pdf}{fig:heatmap_add_full}{Attention activation heatmap for all blocks and attention heads for the addition task "123+456".}


\includePDFPlot{experiment_results/heatmaps/add_simple_1.pdf}{fig:heatmap_add_simple1}{First block attention heatmap for the addition task "123+456", averaged over all 6 heads.}


\includePDFPlot{experiment_results/heatmaps/add_simple_2.pdf}{fig:heatmap_add_simple2}{First block attention heatmap for the addition task "999+001", averaged over all 6 heads.}




When switching to a model trained for 5-digit addition and the addition task "12345+67890" (\cref{fig:heatmap_add_5digits}) we can still see a similar pattern of output digits referring to those input digits that make sense when thinking about an addition algorithm. However the pattern is somewhat more diffuse for the more significant output digits, which are also attending to less significant digits of the input numbers somewhat.

For 3-digit addition using result reversal (\cref{fig:heatmap_add_rev}), we can see a very clear pattern of the final output digits attending to the corresponding digits of the reversed result. There however is not such a clear pattern for the reversed addition that happens before, with the output digits "9750" not having a strong connection to the matching input digits as was seen before for normal addition.

For the addition task using the scratchpad (\cref{fig:heatmap_add_scratch}), we can also see some patterns: For the individual addition steps, the input digits are fetched as the first two digits of each step. We can also see that the old carry value (third digit of each step) to some degree attends to the fifth digit of the previous step, which is the output carry value of that previous step.


\includePDFPlot{experiment_results/heatmaps/add_5digits_1.pdf}{fig:heatmap_add_5digits}{First block attention heatmap for the addition task "12345+67890", averaged over all 6 heads.}

\includePDFPlot{experiment_results/heatmaps/add_rev_1.pdf}{fig:heatmap_add_rev}{First block attention heatmap for the addition task "123+456", using result reversal, averaged over all 6 heads.}


\includePDFPlot{experiment_results/heatmaps/add_scratch_1.pdf}{fig:heatmap_add_scratch}{First block attention heatmap for the addition task "123+456", using scratchpad, averaged over all 6 heads.}




\subsubsection{Multiplication task}


For the multiplication task with 3 digits (\cref{fig:heatmap_mul}) our model actually computes the wrong result 056188 instead of the correct result 056088 which is not unexpected considering its low accuracy at about 20\% (\cref{tbl:baseline_mul}).

We can see the 3 least significant digits of the product (188) attending primarily to the  least significant digits of the input numbers (3 and 6). The 3rd most significant digit (6) has an attention pattern without clear focus. The 1st and 2nd most significant digits of the product (0 and 5) attend to the most significant digits of the input (1 and 4).

This does not follow a pattern clearly related to a normal multiplication algorithm: For the least significant digit of the product only the least significant digit of the factors should be relevant, for the second least significant digit this should extend to the first and second digit of the factors and so on, which does not match the pattern visible in the diagram.
This corresponds well to the fact that accuracy achieved for multiplication was much poorer that for addition (only 22\% for 3-digit multiplication, see \cref{tbl:baseline_mul}), prompting speculation whether models with high accuracy on an arithmetic task show more meaningful digit relationships in their heatmaps that those with low accuracy.

\includePDFPlot{experiment_results/heatmaps/mul_1.pdf}{fig:heatmap_mul}{First block attention heatmap for the multiplication task "123*456", averaged over all 6 heads.}



\subsubsection{Square root task}

For the square root tasks with 6 digits (\cref{fig:heatmap_sqrt}) we can see the least significant digit of the output "1" attending to the 3 most significant digits of the input (and slightly to previous output digit "3") whereas the more significant output digit 5 attends mostly to the second most significant input digit "2" and the most significant output digit "3" attends mostly to the most significant input digit "1".

This also does not clearly correspond to a normal integer square root algorithm as for example the least significant 3 digits of the input are not considered at all by the model's attention pattern, while for example 123000 has an integer square root of 350 whereas 123999 has a square root of 352. These digits must thus be considered, if they are even considered at all, only at later blocks of the model not visible in this diagram.

Accuracy was 98\% for 6-digit integer square root (\cref{tbl:baseline_sqrt}), but for this operation it is plausible that the model just guesses the square root based on the first few digits, which would of course fail on numbers close to a square number which require precise computation, but such hard cases should be only a small fraction of the test dataset (which is uniformly selected from all 6-digit integers).

\includePDFPlot{experiment_results/heatmaps/sqrt_1.pdf}{fig:heatmap_sqrt}{First block attention heatmap for the integer square root task "123456", averaged over all 6 heads.}