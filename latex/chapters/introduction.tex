\thispagestyle{empty}

\section*{Abstract}

Transformer networks have proven to be successful in a number of domains such as natural language translation, coding, and general question answering. Arithmetic tasks, however, can be challenging for them.
We study the performance of small (around $10^7$ parameters) transformer networks with a similar architecture to OpenAI GPT-2 on addition (3 to 10 decimal digits), multiplication (3 to 10 decimal digits), and integer square root (6 to 20 decimal digits). Training dataset sizes range from $10^3$ to $10^5$ samples, with character-level tokenization based on ASCII character values.
We observe good performance on addition and poor performance on multiplication, with the latter not being learnable at all with five or more operand digits, with integer square root being somewhere in the middle.
A number of strategies are attempted to address the issues with multiplication and integer square root performance: Adjusting the size of the model, different strategies for sampling the operands, reversing the digits of the result, and including a scratchpad with detailed intermediate steps in each training sample.
While most strategies brought no dramatic improvement, scratchpads showed promise in making unlearnable tasks like multiplication learnable.


\clearpage

\tableofcontents
\clearpage

\section{Introduction}

In 2017, the influential paper "Attention Is All You Need" \cite{allyouneed} was publ<ished. It showed that a new language model architecture, which they called the "Transformer", outperformed all previous models on English-French and English-German translation tasks, while also having lower training cost \citepage{8}{allyouneed}. Soon thereafter, variations of the transformer architecture described in this paper became the state of the art in most natural language processing tasks. They were also found to be competitive with established neural network architectures in other domains such as image classification \citepage{6}{worth16words}.

OpenAI's release of the original ChatGPT web service in late 2022 based on GPT3.5 \cite{openai_chatgpt_2022} made Transformer networks a topic of relevance for the general public. This AI chat assistant gave natural and helpful answers, mimicking human dialogue to a degree not seen before; while showing strong skills in a wide array of domains such as computer science, history and creative writing \cite{Savelka_2023} \citepage{5}{openai2023gpt4}.

While the details of the architecture of GPT3.5 have not been released \cite{openai_chatgpt_2022} \cite{openai2023gpt4}, it is assumed that it is based on a transformer decoder architecture very similar to that of GPT-2, which was released by OpenAI in 2019, with the main differences being that GPT3.5 is based on a bigger model with more learnable parameters as well as a larger training dataset \cite{OpenGenus2023GPTComparison}.

Even though transformer models are very strong at learning the patterns of human language, they do have limitations in other areas such as performing arithmetic.
For example, GPT3.5 and GPT4 are unable to correctly multiply numbers with 4 or more decimal digits. It does however seem capable of adding numbers of arbitrary size \cite{openai2023gpt4} \citepage{9}{nocalculator}.  This work seeks to explore the limits on what transformer-style models are able to learn in terms of integer arithmetic.


\subsection{Natural language versus arithmetic}

From a theoretical standpoint, one could argue both that natural language is harder to learn than arithmetic or vice versa:
Natural language does not always follow strict logic but is more based on established conventions of its speakers, it covers a wide range of topics and can have multiple layers of meaning and nuance.
Arithmetic on the other hand has strict rules with often only one correct result. Computational complexity can be high for some arithmetic tasks.

Traditionally, computers excelled at arithmetic (due to the fact that it is easy to program them to handle it) but failed at natural language.
Only recently, after a few generations of transformer networks and roughly at the point in time ChatGPT was launched in late 2022, have computers managed to handle natural language reasonably well \cite{tippingpoint}.
One might think that an advanced system that is competent at handling natural language would be able to handle arithmetic tasks even better. But that is not necessarily the case.

From a theoretical standpoint, Hahn \cite{theoreticallimits} showed that transformer networks cannot handle some fairly basic tasks like PARITY (testing whether the number of 1s in a bitstring is even) no matter their weights or depth. This however assumes handling the language in a general sense, with unbounded string length. For inputs with limited length, they can in fact learn such languages.

In practice, OpenAI's technical report on GPT-4 \cite{openai2023gpt4} as well as Yang et al.  \citepage{9}{nocalculator} showed that GPT-3.5 and GPT-4, which were among the most advanced models for natural language understanding at the time of release, struggle with multiplication of integers with 4 or more decimal digits. They did, however, observe good performance on adding large integers, highlighting that multiplication is not only harder from a computational complexity standpoint but also a greater challenge for transformer models.

\subsection{Related work}

Basic feed-forward networks can do addition of binary numbers (each input / output neuron representing one bit) with one hidden layer and binary multiplication with two hidden layers with perfect accuracy, as shown by Franco and Cannas \cite{solving}. The authors did not train a network, but instead proved its existence by constructing a feed-forward neural network including weight and bias matrices and proving its output always matches the sum or product of the input numbers.

Hoshen and Peleg \cite{visual} trained feed-forward neural networks on integer addition and multiplication, experimenting both with one-hot decimal digit vectors as well as 2D binary images of the written numbers as inputs and outputs. For addition, input integers were limited so their sum is at most seven decimal digits long. For multiplication operands were limited so the product is at most seven digits long, thus allowing operands in the range $[0, 3162]$. Error rates achieved were low for addition ($\approx 2\%$ for both for vector and image inputs), but high for multiplication ($\approx 38\%$ for vector inputs and $\approx 72\%$ for image inputs).

The main inspiration for our work was an experiment by Lee et al. \cite{teaching} to teach arithmetic to small transformer networks ("nanoGPT" with about 10 million parameters). First, the authors train for 3-digit integer addition and compare final model performance when various training dataset sizes were used. This is followed by experiments using techniques like result reversal (reversing the order of output digits so the least significant digit comes first, this matches order of computing digits when adding by hand) and step-by-step thinking (not just having inputs and their sum in each training sample, but also intermediate steps of the computation, like intermediate sums and carry amounts, similar to adding or multiplying by hand) to improve performance when training with larger operands. They also experiment with 2-digit multiplication and approximating the square root of decimal numbers in the range $[0,10]$.

A much larger transformer network with about 7 billion parameters is trained by Liu and Low \cite{goat}. The authors train on addition and multiplication up to 16 digits while using step by step thinking (including intermediate results of each computation into the training sample, similar to doing addition / multiplication by hand). Accuracy was near-perfect on addition but their model failed to learn multiplication.

Training transformer models on binary addition and multiplication was attempted by Maltoni and Ferrara \cite{memtocomp}. This stands in contrast to the base 10 digits used in the papers mentioned before. The authors train on operands of only seven bits and use almost all possible $2^7 \cdot 2^7 = 2^{14}$ addition or multiplication examples in the training set, with the remaining few reserved for the test set. Training eventually achieved perfect accuracy for addition and multiplication, although the operand sizes were small for this study.

Length generalization is the capability of the model to work on operand lengths larger than the ones seen in training. This is the focus of McLeish et al. \cite{rightembeddings}. The authors use a large training dataset of 20 million samples of mixed operand lengths. Using some tweaks to the model's embeddings, they were able to achieve near perfect accuracy for addition and multiplication, as well as some degree of length generalization for addition.

Various transformer models from 10 million to 2 billion parameters are trained on arithmetic tasks by Yang et al. \cite{nocalculator}. Training is divided into two phases, with the first phase containing operations with operands up to 5 digits and the second phase containing operations with operands up to 12 digits. This led to over 99\% accuracy for 3 and 5 digit addition for all model sizes. For 3 digit multiplication, final accuracy is between 78\% (10 million parameters model) and 98\% (2 billion parameters model), with larger models achieving higher accuracy.
For 5 digit multiplication, final accuracy is between 42\% (10 million parameters model) and 90\% (2 billion parameters model), with larger models again achieving higher accuracy.

The idea of training a model to perform arithmetic computations step by step (meaning the training samples also contain intermediate results similar to computing the result by hand) is explored by Deng et al. \cite{implicit}. After training the model on the step by step samples, the model is trained to no longer output the intermediate steps, but only the final result. This then yields a model that simply outputs the correct result for some arithmetic task.

In addition to the input token sequence, transformer networks usually also receive an additional position vector for each token that helps them to know the position of a token in the sequence (since their learnable parameters are independent of token position).
Positional encoding schemes are ways in which positional information is embedded into the token sequence to make the model grasp the order of tokens. This ranges from no positional encoding at all to more sophisticated schemes involving vector rotation.
Various positional encoding schemes are compared for performance by Shen et al. \cite{positionmatters}, with a focus on learning arithmetic and length generalization. 

The topic of length generalization is also explored by Cho et al. \cite{lengthandcount}, in the size of operands as well as the number of operands (e.g., multiple additions or multiplications per prompt).
Length generalization for the operations of addition, multiplication and modular arithmetic is also studied in \cite{lengthgen}. Various positional encoding schemes were tested. Perfect accuracy for addition from 6-20 digits was achieved for all schemes. For multiplication, models where trained on operations where the smaller operand has 3 decimal digits and the larger operand has 5 decimal digits. Then, the performance of the trained models was tracked when the larger operand has no longer 5, but up to 35 digits (length generalization). Multiplication with two large operands of 5 or 10 digits each was not tested.