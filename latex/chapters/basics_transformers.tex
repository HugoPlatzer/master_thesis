\section{Transformer models}

Machine translation, which is the automated translation from one natural language to another, was an important challenge since the beginning for the field of artifical intelligence and machine learning.
The input and output texts are usualy modeled as sequences of integers from a limited range, with each integer representing a phrase, word, or part of a word.
Classical dense/feed-forward neural networks are a poor fit for such tasks, as the sequence lengths of even a short text combined with the many possible choices for each position make for a dense network with far too many parameters.

Thus researchers started using approaches like recurrent neural networks (RNN) \cite{phrasereps}, convolutional neural networks (CNN) \cite{convseq} and long short-term memory (LSTM) \cite{seq2seq}.
While these approaches did make the parameter count more manageable due to their sparse architecture, they still had issues with training on long sequences and learning long-distance correlations within a sequence.

Transformer models were introduced in \cite{allyouneed} in the context of machine translation to help address the aforementioned issues. Later on, they proved useful for many other tasks. 

Due to their parallelized architecture which will be explained in detail later, transformers allow for training on long sequences in a single optimization step, greatly increasing training throughput \citepage{1}{allyouneed}. They also have full connection between any two points in the sequence in each layer of the network, mitigating the problem of long-range dependencies being only considered across multiple model steps/layers in CNNs or RNNs which makes information loss more likely \citepage{6}{allyouneed}.

\subsection{Types of transformer models}

There are a variety of different albeit similar structures that have been used to build effective transformer networks. In particular, decoder, encoder-decoder, and encoder models need to be differentiated.

Encoder models such as BERT \cite{devlin2019bert} transform any given text into a vector of fixed size. This vector in the ideal case captures the essence of the text's meaning and can thus be used for tasks like text classification or sentiment analysis.

Encoder-decoder models like the original transformer architecture as described in \cite{allyouneed} use the vector generated by the encoder to start generating new text. This can be used for example by translation models or for answering questions.

Decoder models like the GPT models \cite{improvinglu} generate new text to extend some given starting text. This can be used for example to write a story when given a first paragraph. But with the right training, they can be used for the same tasks as encoder and encoder-decoder models as can be seen in the next section.

\subsection{Transformer decoder models}

The purpose of a decoder transformer model is to have it complete a sequence of tokens. These tokens may represent a letter of some alphabet, fragments of words, entire words, keywords of a programming language, even decimal digits, byte values or the binary values 0 and 1.
They could even represent color values or grid coordinates. Natural language modeling, however, is the most common scenario.

The model can be trained on token sequences generated from natural language taken from books, news articles, online communication platforms or other sources. This would make the model useful for writing stories, filling gaps in text and so on.
Or it could be trained on pieces of source code, which would allow the model to act as a code completion assistant.
The model would be given a piece of text that starts a story the user wants to have completed, or a piece of code that only has a function defined with comments. A well-trained model would then complete the code or story in a useful way so the rest of the story matches the theme set in the starting paragraph, or the code generated fits the comments and function signature written before.

The training texts could also all be in a structured question-answer pair form, allowing the model to answer questions. A few examples:
\label{transformer:examples}

\begin{itemize}
\item Translation tasks: "en: The house is big.  de: Das Haus ist gro√ü."
\item Classification tasks: "text:  what a cool product, would buy again  sentiment: happy"
\item Mathematical tasks: "21*2=42" or "123: odd number" / "124: even number" \citepage{3}{nocalculator}
\end{itemize}

When the models is well-trained on these question-answer pair texts, it can be asked a question by simply leaving out the answer part at the end of the text, like "21*3=", as demonstrated in \cite{alammar-transformer}. A good model would be able to generate the correct answer for similar questions by inference from the many examples it has seen, even when the exact question posed is not in its training data.

\subsection{Beyond text}

Apart from text-based tasks, transformer models are also capable in domains like image classification.
A 2D image can be flattened into a sequence of tokens / vectors which can then be processed in a normal transformer model. The resulting image classifier achieves similar results to state-of-the-art CNN-based classifiers \citepage{3,5}{worth16words}.

Transformers can even be successful at generating images from text prompts. The approach presented in \cite{cogview} involves using a regular tokenizer for the text prompts and a separate discrete autoencoder to transform the training images into tokens and the generated tokens back to images.

Board games such as chess and go can also easily be tackled by transformers, as the board state and moves made can be transformed into text or represented by specialized tokenization schemes.
In \cite{grandmasterlevelchess} a transformer decoder model is trained to generate best moves or score board positions based on a large dataset of human chess games. The resulting chess playing engine manages grandmaster-level performance in blitz chess against human opponents \citepage{5}{grandmasterlevelchess}.

There have been minor modifications to the model architecture, data representation, and training regime among the diverse tasks outlined above. Also the performance of the resulting models is generally good but not quite state-of-the art. Nonetheless, having one model architecture that can achieve strong performance across a diverse set of domains seems promising as a step towards artificial general intelligence.


\subsection{Tokenization}
\label{transformer:tokenization}

Most transformer models preprocess input text using a so-called tokenizer, which is a model that was trained to group text into meaningful chunks out of a limited vocabulary called tokens to provide for an efficient representation of language \cite{subwordunits}. The tokenizer transforms the text into a sequence of integers, which is then processed by the transformer model which outputs token IDs which are then converted back into text.

Compared to just using raw bytes as model inputs, a good tokenizer allows for shorter sequence lengths (as a token typically represents multiple characters/bytes), meaning faster generation and greater maximum length \cite{tokenizerchoice}. Nonetheless, there are capable models such as Google ByT5 \cite{xue2022byt5} that do not need a tokenizer as they operate directly on byte values.
Compared to manually specifying a vocabulary (for example contents of a dictionary) tokenization is more flexible, having no issues with unknown words or large composite words (any possible byte sequence can still be represented) and is adaptable to the text corpus the model should be used on.

To train a tokenizer, the byte-pair-encoding (BPE) algorithm \cite{subwordunits} is often used, which also has applications in data compression. It works as follows:

\begin{enumerate}
	\item At the start, the set of token strings is just the possible characters in the input text, e.g., all ASCII characters. At this point, each character needs one token (string) in the representation of the tokenizer.
	\item The input text is modeled as a sequence of token strings from the token strings set, preferring longer strings wherever possible over shorter ones. In this sequence, the most common adjacent pair of two token strings is identified.
	\item A new token string is created from the concatenation of this pair, e.g., if "A" followed by "B" is the most common token string pair in the text, a new token string "AB" is added to the set.
	\item Steps 2 and 3 are repeated over and over. This causes the token vocabulary to grow and the average number of tokens needed per character to decrease. This is done until a good balance between token vocabulary size and tokens per character is achieved.
\end{enumerate}

Apart from the regular token vocabulary, most tokenizers also contain some special token values that will not be found in the input text. These are used for example to represent the start and end of each training sample, delimit individual pieces of text when they are fed to the model, or separate parts of a question/answer pair \cite{HuggingFaceTokenizers}.


\subsection{Input / output}
\label{transformer:input}

Formally, the input to the model consists of a sequence of integers (token IDs) $\modIn_1 \ldots \modIn_n$, where $n$ is the length of the token sequence.
Generally, transformer models place a limit on the length of the sequence $n$ due to their internal structure, we denote this limit as $\nPos$. However, models with no such limitation have also been proposed \cite{su2023roformer}.
Each integer $\modIn_i$ references one of the possible tokens of the model $(1 \leq \modIn_i \leq \dVocab)$. $\dVocab$ is the size of the token vocabulary, which depends on the tokenization process used for the model \citepage{2}{allyouneed}.

\label{transformer:output}

For each position in the input sequence, the model outputs a probability distribution, predicting the next token based on the input tokens up to this position. 
The output of the model is a sequence of probability distributions $\modOut_i$ $(1 \leq i \leq n)$. $\modOut_i \in \mathbb{R}^\dVocab$, $0 \leq (\modOut_i)_j \leq 1$ \citepage{5}{allyouneed} \cite{HuggingFaceGPT2}.
$\modOut_i$ should model which token most likely follows after all the tokens $\modIn_1 \ldots \modIn_n$.

During training, all output positions are useful as they allow to optimize the model to correctly predict $\modIn_2$ based on $\modIn_1$,
$\modIn_3$ based on $\modIn_1$ and $\modIn_2$, $\modIn_n$ based on $\modIn_1 \ldots \modIn_{n-1}$, all in a single training step. This greatly reduces training time for longer sequences \cite{alammar-gpt2}.

\subsection{Text generation}
\label{transformer:textgen}

When using the trained model to generate text, only the last output distribution, $\modOut_n$, is of interest, because
it tells the next token after the given input sequence. \cite{Mao2021Autoregressive}

This is used to generate one new token at a time (autoregressive decoding). The new token is picked from the distribution either by choosing the most likely token (greedy decoding), random sampling according to the probability distribution, or more sophisticated methods such as top-p sampling (choosing among the most likely tokens whose probability mass equals for example $p=0.95$) and top-k sampling (selecting from the for example $k=10$ most likely tokens) \cite{ippolito2019comparison}.

The newly generated token $\modIn_{n+1}$ is appended to the input sequence and the whole process is repeated, with $n$ growing by one after each step, until a special end-of-sequence token is generated, the maximum length $\nPos$ is reached, or some other stop condition is triggered \cite[sect. "Generation"]{HuggingFaceGPT2}. If generation is not terminated, the model could in theory generate a text of infinite length, although transformer models generally only have a limited window of past tokens they can consider when generating the next token. In addition, generation time and memory requirements generally increase quadratically with sequence length for transformer models (\cref{gpt2_attn}).

\subsection{User interaction}

In 2023, OpenAI released their most successful product yet, ChatGPT \cite{openai_chatgpt_2022}. This online service allows users to chat with an artificial intelligence assistant which can help with reciting facts, writing texts and code, giving advice of all sorts and so on.

The principle by which transformer decoder models can act as chat assistants is simple: The model is trained on conversations like this:

\begin{verbatim}
    User: What color is the sky?
    Assistant: The sky is blue.
\end{verbatim}

The model can then become the helpful assistant by filling in the assistant's messages:

\begin{verbatim}
    User: What color is a banana?
    Assistant:
\end{verbatim}

and the model would generate a response like "A banana is yellow" followed by an end-of-sequence token, similar to its training examples. For more details on how generic language models can be tuned to follow instructions or conversation, see \cite{yi2019coherent} and \cite{ouyang2022training}.

\subsection{Generative pre-training}

The GPT2/GPT3/ChatGPT transformer models have demonstrated the effectiveness of a technique used to train language models called generative pre-training (GPT). This technique was first popularized in \cite{improvinglu} and consists of two phases.

In the pre-training stage the model is first trained on a large corpus of text in multiple languages, including websites, books, articles, chats and source code. In this manner, a huge amount of training data can be collected without the need for careful manual selection / preparation of question-answer pairs. The training data is split into appropriate chunks and the model learns to predict (generate) the next word based on the previous ones for each chunk as a form of unsupervised learning \citepage{3}{improvinglu}.

After sufficient pre-training, the model is then fine-tuned on question-answer pairs of the specific task the model should be able to solve, like the examples presented in \cref{transformer:examples} or even human-assistant chats on a wide range of topics.

Due to the well-known phenomenon of transfer learning \cite{transferlearning}, the pre-trained model will have already internalized the structure of natural language, source code and maths. 
This allows a relatively small amount of hand picked fine-tuning examples to make it significantly stronger compared to just training on the question-answer pairs on a freshly initalized model \citepage{8}{improvinglu}.

\subsection{OpenAI GPT2}

A good example of a successful decoder model is OpenAI's GPT2 \cite{unsupervisedmultitask} \cite{OpenAI2019BetterLM} \cite[source code]{HuggingFaceGPT2}. It was released in 2019 and demonstrated the ability to write realistic articles based on a given starting prompt. This model is useful for explaining the details of the transformer architecture, because the model weights and code were released, its proven effectiveness in generating meaningful text, and due to the fact that its even more powerful successors have not experienced significant architectural changes \cite{OpenGenus2023GPTComparison}.

\subsubsection{Achitecture}

A general overview of the architecture of GPT2 is presented in \cref{diagrams/gpt2/overview}.
As described in \cref{transformer:input}, the input to the model is an integer sequence, with each integer representing an element of the token vocabulary.
Creating this token sequence from the input text is done by the tokenizer (\cref{transformer:tokenization}, \cref{gpt2:tokenizer}). This tokenizer is considered a component separate from the model and is trained before it. The model is then trained based on the output indices of the chosen tokenizer, therefore the tokenizer cannot be changed for a trained model. The various stages inside the model are as follows:

\begin{enumerate}
	\item The sequence of token indices is then transformed into a sequence of vectors by the preprocessing stage (\cref{gpt2_preproc}). The preprocessing stage converts each token $\modIn_i \in \mathbb{N}$ into a hidden vector $\modHiddenAt{0}_i \in \mathbb{R}^\dHidden$ based on the token value $x_i$ and token position $i$. For this, it uses learnable embedding matrices.
	The size of each of these hidden vectors, $\dHidden$ is a parameter that can be configured when creating a GPT2 model (\cref{gpt2:parameters}).
	
	\item These vectors $\modHiddenAt{0}_i$ are then fed through a number of GPT2 blocks, each being identical except for having its own set of learnable parameters. These GPT2 blocks are indexed by $l$ ($1 \leq l \leq \nBlocks)$, with $\nBlocks$ being the number of GPT2 blocks the model has, which can also be configured at model creation.
	We denote the input vectors to block number $\modLayer$ as $\modHiddenAt{\modLayer-1}_i$ and the output vectors of that block as $\modHiddenAt{\modLayer}_i$. Thus for a model with, e.g., 12 blocks, the inputs to the first block are $\modHiddenAt{0}_i$ and the outputs after the last block are $\modHiddenAt{12}_i$.
	
	Each block outputs a vector sequence of the same dimensions as its input.
	The GPT2 block is described in \cref{gpt2:block} and consists of multiple layers including normalization, a feed-forward network and the attention mechanism. The attention mechanism (\cref{gpt2_attn}) is special to transformer networks and the only point in the model where vectors are not transformed independently, i.e., the output vector at position $i$ depends not just on the input vector at position $i$, but also on the vectors at all previous positions $1 \leq i' < i$.
	The job of the block and the attention mechanism in particular is to allow the model to learn how later elements of the training sequences are influenced by previous elements, e.g., capitalized words mostly appearing after points and spaces (ends of sentences) in English training texts.
	
	\item After the last block there is the postprocessing stage. Its job is to convert each hidden vector $\modHiddenAt{\nBlocks}_i$ into a probability distribution $\modOut_i \in \mathbb{R}^{\dVocab}$. This is also done with a learnable embedding matrix.
	
	During training (\cref{gpt2:training}), the model parameters are optimized so that every $\modOut_i$ correctly predicts the next token in the training sequence, $\modIn_{i+1}$.
	During inference (\cref{gpt2:textgen}), only the last distribution, $\modOut_n$, is used to sample a predicted token $\modIn_{n+1}$, which then is appended to the input sequence for the next generation step.
\end{enumerate}

\includediagram{diagrams/gpt2/overview}{Overview of a GPT2 model. This example uses a model with $\nBlocks=12$ blocks, $\dHidden=768$ hidden vector size. The model is processing a sequence of length 3. Models can also process longer sequences than shown in this example, up to their positional limit $\nPos$ which is configurable at model creation time as is the number of blocks and hidden vector size.}

\subsubsection{Number of parameters}

\label{gpt2:parameters}

To make the model bigger (needing more memory and computation time, but allowing for more accurate generation) or smaller (faster, but lower quality output), two hyperparameters can be adjusted \citepage{4}{unsupervisedmultitask} \citepage{5}{improvinglu}: The vector size for each position can be changed just like the number of transformer blocks. The number of attention heads is usually adapted so that the size of each head remains around 64 \citepage{4}{unsupervisedmultitask} \cite{hfpretrained}.
OpenAI GPT2 was released in four versions with different parameter counts:

\includeTable
{
	name; n_params; n_blocks; d_hidden; n_heads; d_head\\
	gpt2-small; 117M; 12; 768; 12; 64 \\
	gpt2-medium; 345M; 24; 1024; 16; 64 \\
	gpt2-large; 774M; 36; 1280; 20; 64 \\
	gpt2-xl; 1558M; 48; 1600; 25; 64 \\
}
{tab:gpt2-sizesX}
{
	Versions of GPT2 released with different parameter counts \cite{hfpretrained}: $\nBlocks$ is the number of GPT2 blocks, $\dHidden$ is the vector size at each position, $\nHeads$ is the number of attention heads and $\dHead$ is the size of each attention head.
}
{%
	columns/name/.style={column name={Model size}},
	columns/n_params/.style={column name={No. of parameters}},
	columns/n_blocks/.style={column name={$\nBlocks$}},
	columns/d_hidden/.style={column name={$\dHidden$}},
	columns/n_heads/.style={column name={$\nHeads$}},
	columns/d_head/.style={column name={$\dHead$}}
}


\subsubsection{Tokenization}

\label{gpt2:tokenizer}

The GPT2 models as released by OpenAI were trained using a Byte-pair encoding (BPE) tokenizer (\cref{transformer:tokenization}) with a vocabulary of 50257 tokens \citepage{4}{unsupervisedmultitask} \cite{HuggingFaceGPT2}. The tokenizer was itself trained on a natural language corpus separately before training the actual models. The models were then configured with $\dVocab=50257$ and the training texts were processed using this tokenizer before being fed into the model. Thus, when using these models with the weights as released by OpenAI, the same tokenizer must be used for text generation during inference time as well. An example on how the tokenizer that was released with OpenAI GPT2 handles a sentence can be seen in \cref{diagrams/gpt2/tokenizer}.

It is also possible to train a GPT2 model with a different tokenizer with, e.g., a smaller vocabulary. Instead of a proper tokenizer that was trained to efficiently chunk text in some natural language, other GPT2 implementations like nanoGPT \cite{nanogpt} use a simpler approach: Each ASCII character in the training text is mapped to one token, with the token value equal to the ASCII character value.
This makes for a simpler overall implementation that keeps all the processing outside the tokenizer and inside the actual model.
The downside is that token sequences get significantly longer, thus a model with the same $\nPos$ and ASCII tokenization can only handle significantly shorter texts than one with a trained tokenizer that recognizes, e.g., frequent words and groups all their characters into a single token.

\includediagram{diagrams/gpt2/tokenizer}{Operation of the GPT2 tokenizer on an example sentence.}


\subsubsection{Preprocessing}
\label{gpt2_preproc}

Transformer networks do not operate on integer token IDs, but on state vectors from $\mathbb{R}^{\dHidden}$. $\dHidden$, the dimension of the state vector at each position, is a parameter fixed at the time of model construction (\cref{gpt2:parameters}).
This means the token sequence needs to be converted into a sequence of such vectors before feeding it through the model. This happens during the preprocessing stage, which is illustrated in \cref{diagrams/gpt2/preproc}.
For converting the token IDs $\modIn_i$ into initial state vectors $\modHiddenAt{0}_i$, GPT2 has a learnable token embedding matrix $\embTok \in \mathbb{R}^{\dVocab \times \dHidden}$. Each row of this matrix stores the learnable initial state vector for the corresponding token ID.


Because the blocks of the GPT2 have no positional awareness (their learnable parameters do not depend on the sequence position $i$), information about the position of a token within the sequence needs to be included as well \citepage{6}{allyouneed}.
In GPT2, this is done by a learnable embedding matrix $\embPos \in \mathbb{R}^{\dVocab \times \dHidden}$ storing a learnable vector for each position up to the sequence length limit $\nPos$.

The starting vector $\modHiddenAt{0}_i \in \mathbb{R}^\dHidden$ for a token value $\modIn_i$ at position $i$ is then given in \cref{eq:gpt2_preproc} as the sum of the token and positional embedding vectors \citepage{3}{allyouneed}:

\begin{equation}
	\modHiddenAt{0}_i = \embTokAt{x_i, :} + \embPosAt{i, :}
	\label{eq:gpt2_preproc}
\end{equation}

\includediagram{diagrams/gpt2/preproc}{GPT2 preprocessing stage on an example token sequence. Based on the token position and value, embedding vectors are summed to give the starting vector at this position.}


\subsubsection{Postprocessing}
\label{gpt2:postproc}

GPT2 uses a "language modeling head" \cite{HuggingFaceGPT2} to convert the outputs of the last block $\modHiddenAt{\nBlocks}_i$ into token probabilities $\modOut_i$, the steps of which are illustrated in \cref{diagrams/gpt2/postproc}.

First, a final layer normalization step is performed \citepage{4}{unsupervisedmultitask}, which transforms $\modHiddenAt{\nBlocks}_i$ into $\modHiddenAt{\nBlocks, norm}_i$. The details of this layer normalization step are  the same as the layer normalization steps in each GPT2 block, which are described in \cref{gpt2:layernorm}.

GPT2 then re-uses the same matrix $\embTok$ that was used in the preprocessing stage to convert $\modHiddenAt{\nBlocks, norm}_i$ into a token probability vector $\modOut'_i$ of dimension $\dVocab$, using matrix-vector product, as shown in \cref{eq:postproc}.
This tying of the weights in the postprocessing stage to those of the preprocessing stage is done for improving model performance as well as reducing the number of learnable parameters \cite{weightstying}.

Finally, softmax is applied to transform this vector $\modOut'_i$ into a proper probability distribution $\modOut_i$.

\begin{equation}
	\begin{aligned}
		\modOut'_i &= \embPos \modHiddenAt{\nBlocks, norm}_i \\
		(\modOut'_i)_k &= \sum_{j=1}^{\dHidden}{\embPosAt{k,j} (\modHiddenAt{\nBlocks, norm}_i)_j} && 1 \leq k \leq \dVocab
	\end{aligned}
	\label{eq:postproc}
\end{equation}

\includediagram{diagrams/gpt2/postproc}{Detailed example of the GPT2 postprocessing stage. $x_i$, the output of the last GPT2 block, is converted into token probabilities.}


\subsubsection{Text generation}
\label{gpt2:textgen}

Generating output text using a trained GPT2 model follows the general process for transformer networks outlined in \cref{transformer:textgen}.
The generation process is visualized in \cref{diagrams/gpt2/generation}. It always starts with some existing starting text that is transformed into a sequence of $n$ tokens $\modIn_1 \ldots \modIn_n$. These tokens are fed through the model to get the output probability distributions $\modOut_1 \ldots \modOut_n$. The last distribution $\modOut_n$ is sampled using one of the sampling methods outlined in \cref{transformer:textgen}. The token that results from this sampling becomes $\modIn_{n+1}$ and is appended to the sequence $\modIn_1 \ldots \modIn_n$. The process outlined above is then repeated, now sampling $\modOut_{n+1}$ to get the next token $\modIn_{n+2}$ and so on.

Depending on what texts / token sequences the model was trained on, it will at some point cause a special stop token ID to be sampled, which tells the sampling process that the generated text is now complete and sampling should be stopped.
In other cases, a fixed number of tokens defined by the user might be sampled, which is possible as long as the total token sequence length does not exceed $\nPos$.

Due to the limited dimensions of the positional embedding matrix $\embPos$, GPT2 as described here does not support generating texts of unlimited length. Other variants of GPT2 or similar transformer models use different positional embeddings not based on a matrix of fixed dimension. Those models could support unlimited generation, although it is limited in practice by computational complexity and limited coherence in the generated text (\cref{transformer:textgen}).

\includediagram{diagrams/gpt2/generation}{Text generation using the GPT2 model for the example starting text "where is". The rightmost probability distribution is used to pick the next token, after which the data, augmented with this new token, is fed through the model again.}

\subsubsection{Training}
\label{gpt2:training}

GPT2 uses a training objective similar to \citepage{3}{improvinglu} where a large text corpus is used to make the model predict the next token based on the tokens up to this point.
In \cite{improvinglu} this is called "Unsupervised pre-training", however other authors refer to similar automated labeling techniques as "self-supervised learning" \cite{selfsupervised} \cite{selfsupervisedvisual}.
In contrast to classical supervised learning, the training samples in self-supervised learning are not manually labeled, but the inputs and labels are created from each training sample using some mathematical function. An example of this could be erasing some pixels of an image in the input while leaving them untouched in the label.
For training transformer decoder models like GPT2, the training goal typically is to make the model complete the sample texts when only some part at the start of the sample is given.
To achieve this, each sample

Cross-entropy loss is used on each output position $\modOut_i$ to measure the deviation between the output $\modOut_i$ and the ideal $\modTarget_i$,
where $\modTarget_i$ represents the probability distribution of a guaranteed token with ID $\modIn_{i+1}$ \citepage{2}{unsupervisedmultitask} \cite{HuggingFaceGPT2}.
As explained before (\cref{transformer:output}), the model should predict the next input token at each output position. This means, when having a training sample sequence $\modIn_1 \ldots \modIn_n$, the inputs to the model would be $\modIn_1 \ldots \modIn_{n-1}$, whereas the ideal outputs would be $\modIn_2 \cdots \modIn_n$ (the inputs shifted by one position) \citepage{3}{improvinglu} \citepage{7}{reasonoverscenegraphs}.

Standard initialization techniques (for example uniform initialization) can be used for the weight and bias matrices \citepage{5}{improvinglu}.

GPT2 is trained using the Adaptive Moment Optimizer (Adam)  \citepage{5}{improvinglu}.
A naive stochastic gradient optimizer will not achieve good results when training transformer networks like GPT2, thus more advanced optimizers like Adam need to be used to make the model converge to a degree where it becomes useful \cite{adambeatssgd}.






\subsection{GPT2 block structure}

\label{gpt2:block}

\includediagram{diagrams/gpt2/block}{Overview of the GPT2 block components. This example shows 3 positions $x_1 \ldots x_3$, but the actual model is flexible regarding sequence length.}

Each block has an attention sub-block, an MLP(multi-layer perceptron) sub-block as well as layer normalization stages and residual connections \citepage{3}{allyouneed} \citepage{4}{improvinglu}.
The operations of the block are executed for each position $i$, but the learnable parameters are the same independent of the position. Different blocks however have different parameters.

The learnable parameters of each block are \cite{alammar-gpt2}:

\begin{itemize}
\item weight and bias of the linear transformation that generates query, key, value vectors from the input vector
   at the start of the attention mechanism: $W_q, b_q, W_k, b_k, W_v, b_v$
 \item weight and bias of the linear transformation "projection" at the end of the attention mechanism: $W_p$, $b_p$
 \item weight and bias of the two linear transformations within the MLP sub-block
 \item weight and bias for each of the two layer normalizations (at the end of each layer norm there is a linear transform): $W_{ln_1}, b_{ln_1} W_{ln_2}, b_{ln_2}$
\end{itemize}

\subsubsection{Layer normalization}
\label{gpt2:layernorm}

\includediagram{diagrams/gpt2/layernorm}{Detailed view of the layer normalization stage of the GPT2 block.}

$$x_i = x_{i_j} (1 \leq j \leq n_{hidden})$$

$$x'_{i_j} = \frac {x_{i_j} - \mu(x_i)} {\sqrt {\sigma^2(x_i)+\epsilon}} $$

$$y_{i_j} = a_j x'_{i_j} + b_j$$

$\mu$ and $\sigma^2$ are the mean and variance of a vector, $\epsilon$ is a small positive real constant added for numerical stability, and $a$ and $b$ are learnable gain and bias vectors of dimension $n_{hidden}$.

Layer normalization subtracts the mean and divides by the standard deviation among all features of the input vector.
After this normalization, it applies a learned gain and bias value for each component of the vector.
This is in contrast to batch normalization, where each individual feature $x_{i_j}$ is normalized along the batch dimension \citepage{1-2}{ba2016layer}.
Layer normalization has the advantage of not suffering from statistical noise when the batch size is too small.

The normalization happens independently for each vector $x_i$ (i.e. at each position $i$), but the learnable parameters $a$ and $b$ (like all learnable parameters of the model) are the same for each position.

\subsubsection{Residual connection}

\begin{samepage}

$$y_i = x_i + f(x_i)$$

as opposed to $y_i = f(x_i)$ without a residual connection \citepage{3}{allyouneed} \cite{residual}.

A residual connection allows the input to "bypass" the learned function of the model, by adding the input to the output vector.
Such connections are found in almost all models having many steps/layers, because they were found to greatly help when training deep models.
This is because when using residual connections, the input data does not need to flow through all computation steps before reaching a given point, instead it can bypass some of them, allowing later computations to access earlier data \citepage{1-2}{residual}.

\end{samepage}

\subsubsection{Attention mechanism}
\label{gpt2_attn}

\includediagram{diagrams/gpt2/attention_multihead}{Structure of the Multi-head attention mechanism. This example uses two positions $x_1, x_2$ and two attention heads $h_1, h_2$. The actual model can in theory handle arbitrary sequence lengths and also has more attention heads.}



At its core, GPT2, like previous transformer models \citepage{3}{allyouneed}, uses a mechanism called Multi-Head Attention \citepage{4}{allyouneed}.

The general idea of an attention mechanism is to compare the vector at one position $i$ with those at previous positions by generating query/key/value vectors at each position, matching the query at position $i$ with the keys of previous positions, then blending the value vectors accordingly to generate the output at position $i$ \cite{alammar-transformer} \cite{alammar-gpt2}.

This is the only part of the entire model where the output at a given position also depends on other (previous) positions. All other components of the model process each position independently.
Nonetheless, the learnable parameters of the attention mechanism, which are linear transformations to generate query, key, value vectors and project the output, are independent of position.

\includediagram{diagrams/gpt2/attention_dot}{Structure of the Scaled Dot-product attention mechanism. This diagram illustrates the computation of the output position $a_3$. The other output positions $a_i$ are computed in a similar fashion, replacing $q_3$ in the diagram with $q_i$ and considering $k_1 \ldots k_i$ and $v_1 \ldots v_i$, giving the diagram $i$ columns.}

For each position $i$ $(1 \leq i \leq n_{pos})$ the following steps are performed \citepage{4}{allyouneed} \cite{alammar-transformer} \cite{alammar-gpt2}:


\begin{itemize}



\item Three vectors called query, key and value are computed using a linear transformation with bias. Its parameters are learned during model training.
 
 $$q_i = W_q x_i + b_q$$
 $$k_i = W_k x_i + b_k$$
 $$v_i = W_v x_i + b_v$$

\item The query, key and value vectors $q_i, k_i, v_i$ are split into $n_{heads}$ segments of equal size $d_{head} = \frac{d_{hidden}}{n_{heads}}$. $q_{i_s}, k_{i_s}, v_{i_s} \in \mathbb{R}^{d_{head}}$. The following steps are performed for each segment $s$ $(1 \leq s \leq n_{heads})$. This is called multi-head attention and was found to be beneficial to model performance comparing to just using a single head \citepage{4-5}{allyouneed} \cite{alammar-gpt2}.
Multi-head attention can also be described as running the Scaled Dot-product attention mechanism $n_{heads}$ times in parallel with each query/key/value vector having the size $d_{head} = \frac{d_{hidden}}{n_{heads}}$ with different learnable parameters for each head, then concatenating and projecting the outputs for all heads at each position \citepage{4-5}{allyouneed}.



\begin{samepage}
\begin{itemize}

\item The query vector $q_{i_s}$ is compared with the key vector of each position $j$  $(1 \leq j \leq i)$ using dot product to generate a score value $a_{ij_s}\in \mathbb{R}$.

$$a_{ij_s} = \frac {1} {\sqrt{d_{head}}} \ dot(q_{i_s}, k_{j_s})$$

The scaling factor $\frac {1} {\sqrt{d_{head}}}$ helps improve gradient stability \citepage{4}{allyouneed}.

Positions after $i$ $(j > i)$ are not considered because of the autoregressive nature of the model, i.e. the model is going to be used to predict the token at position $i+1$ based on the tokens up to position $i$. While the entire sequence is available during training, allowing for an efficient training process, during evaluation only the tokens up to the current position $i$ will be available so this is what should be learned \citepage{5}{allyouneed}.


\item The scores $a_{ij_s}$ are normalized using softmax so their sum is 1, this is required for the next step.
   $$ softmax(a_{ij_s}) = \frac {e^{a_{ij_s}}} {\sum_{1 \leq j' \leq i} {e^{a_{ij'_s}}}} $$


\item Using the scores from before as weights, the output vector $o_{i_s}$ is "blended together" from the value vectors $v_{j_s}$ $(1 \leq j \leq i)$ \cite{alammar-gpt2}:
    $$o_{i_s} = \sum _{1 \leq j \leq i} softmax(a_{ij_s}) \ v_{j_s}$$

\item This so-called "Scaled Dot-product attention mechanism" is further illustrated in \cref{diagrams/gpt2/attention_dot}.

\end{itemize}
\end{samepage}


\item The output vectors of the segments are concatenated:
   $$o_i = concat _{1 \leq s \leq n_{heads}} \ o_{i_s}$$



\item Finally, a linear transformation with bias (the original transformer paper mentions such a transform, but with no bias \citepage{5}{allyouneed}) is applied on the concatenated vector \cite{alammar-gpt2}. Its parameters are learnable.
   $$y_i = W_p o_i + b_p$$

\end{itemize}

\subsubsection{Multi-layer perceptron}

\includediagram{diagrams/gpt2/mlp}{Multi-layer perceptron stage of the GPT2 block. It is a simple feed-forward neural network with two linear steps with bias and an activation function in between.}

This is a simple feed-forward neural network with one hidden layer \citepage{5}{allyouneed}.
As opposed to the original transformer, GPT2 uses the Gaussian Error Linear Unit (GELU) activation function \citepage{5}{improvinglu}, which is defined as follows \cite{gelu}: $gelu(x) = x \ P(X<x)$ when $X$ follows a normal distrbution with mean 0 and variance 1.

$$x'_i = gelu(W_c x_i + b_c)$$
$$y_i = W_p x'_i + b_p$$

